\section{Выбор модели классификации временных рядов}

В данном разделе рассматривается задача построения сети глубокого обучения для классификации временных рядов, где
под временным рядом понимается упорядоченный по времени набор изменения некоторой случайной величины. Для решения этой задачи используются методы глубокого обучения. 
Решению прикладных задач методами глубокого обучения посвящено значительное число современных работ. Работы~\cite{ts1,ts2,ts3} посвящены классификации временных рядов с использованием методов глубокого обучения. В работе~\cite{ts2} используются рекуррентные нейронные сети. В работе~\cite{ts3} для классификации временных рядов рассматриваются различные комбинации ограниченной машины Больцмана, автокодировщика и двухслойной нейронной сети. Исследуется суперпозиция, состоящая из ограниченной машины Больцмана, автокодировщика и двухслойной нейронной сети~\cite{foundamentals}. В работах~\cite{stab1, stab2} рассматривается проблема неустойчивости сети. В работе~\cite{stab1} исследуется поведение модели глубокого обучения как липшицевой функции.  Работа~\cite{recrbm} посвящена рекуррентной модификации модели ограниченной машины Больцмана~\cite{rbm} для классификации временных рядов. Схожие идеи предлагаются в работе~\cite{rae}, посвященной построению рекурсивного автокодировщика.

В данном разделе решается прикладная задача классификации временных рядов. В качестве данных для вычислительного эксперимента используются данные с акселерометров мобильных телефонов~\cite{wisdm}. Для решения задачи оптимизации используется алгоритм обратного распространения ошибок с послойным предобучением сети и дальнейшей настройкой параметров всех слоев~\cite{finetuning}.

\textbf{Постановка задачи. }
Рассматривается задача классификации. 
Моделью классификации  $\mathbf{f}$ выступает суперпозиция подмоделей, аналогичная~\eqref{eq:softmax_example}:
\begin{equation}
\label{eq:wisdm_superposition}
 \mathbf{f}(\mathbf{w}, \mathbf{x}) = \mathbf{f}_1(\mathbf{f}_2(\dots \mathbf{f}_{|V|}(\mathbf{x}))): \mathbb{R}^n \to [0,1]^Z,
\end{equation}
где $\mathbf{f}_v, v \in \{1,\dots,{|V|}\},$ --- модели, параметрическое семейство вектор-функции; $\mathbf{w}$ --- вектор параметров моделей;
$r$-ю компоненту вектора $\mathbf{f}(\mathbf{x},\mathbf{w})$ будем интерпретировать как вероятность отнесения объекта $\mathbf{x}_i$ к классу с меткой $r$~\eqref{eq:proba_softmax}.

Требуется минимизировать функцию ошибки $L$ на обучающей выборке $\mathfrak{D}$,
где $L$ --- сумма отрицательных логарифмов правдоподобия по всем объектам выборки
\[
\mathbf{w}^{*} = \argmin_\mathbf{w} L(\mathbf{w}|\mathfrak{D}),
\]
где
\[
 L(\mathbf{w}|\mathfrak{D}) = -\sum_{(\mathbf{x},y) \in \mathfrak{D} } \sum_{r=1}^Z [y_i = r] \text{log} p(y=r|\mathbf{x},\mathbf{w}).
\]

\textbf{Структура сети глубокого обучения. }
Предлагается использовать в качестве алгоритма решения задачи суперпозицию, состоящую из трех основных компонент:
ограниченной машины Больцмана, автокодировщика и двухслойной нейросети с softmax-классификатором.

\textbf{Ограниченная машина Больцмана.}
Ограниченная машина Больцмана представляет собой двудольный граф, где первая доля соответствует переменной $\mathbf{x}$, а вторая доля --- бинарному вектору $\mathbf{h}$ длины $n'$.
% \begin{equation}\end{equation}
Рассмотрим случай, когда вектор $\mathbf{x}$ принимает бинарные значения. Определим энергию пары входного слоя $\mathbf{x}$ и скрытого слоя $\mathbf{h}$ следующим образом:
\[
 E(\mathbf{x},\mathbf{h}) = -\mathbf{x}^\text{T} \cdot \mathbf{b}_\text{vis} -\mathbf{h}^\text{T} \cdot \mathbf{b}_\text{hid} - \mathbf{h}^\text{T}\mathbf{W}_\text{RBM}\mathbf{x},
\]
где $\mathbf{b}_\text{vis}, \mathbf{b}_\text{hid}, \mathbf{W}_\text{RBM}$ --- параметры модели.

Пусть совместное распределение пары векторов $\mathbf{x}, \mathbf{h}$ задано следующим образом:
\[
	p(\mathbf{x}, \mathbf{h}) = \frac{1}{J} \text{exp}\bigl(-E(\mathbf{x},\mathbf{h})\bigr),
\]
где $J$ --- нормировочный коэффициент:
\[
 J = \sum_{\mathbf{x} \in \{0,1\}^n, \mathbf{h}\in \{0,1\}^{n'}} \text{exp}\bigl(-E(\mathbf{x},\mathbf{h})\bigr).
\]


Функция вероятностей вектора $\mathbf{x}$ есть сумма вероятностей по всем скрытым состояниям вектора $\mathbf{h}$:
\[
	p(\mathbf{x}) = \sum_{\mathbf{h}\in \{0,1\}^{n'}} p(\mathbf{x}, \mathbf{h}).
\]

Определим элемент суперпозиции~\eqref{eq:wisdm_superposition}: 
\begin{equation}
\label{eq:rbm_model}
\mathbf{f}_\text{RBM}(\mathbf{x}) = \mathsf{E}(\mathbf{h}|\mathbf{x}).
\end{equation}
Настройка параметров модели ~\eqref{eq:rbm_model} осуществляется решением задачи оптимизации
\begin{equation}
\label{eq:rbm}
{\mathbf{w}}^{*}_\text{RBM},\hat{\mathbf{b}}_\text{vis}, \hat{\mathbf{b}}_\text{hid} = \argmax_{{\mathbf{W}_\text{RBM}},{\mathbf{b}}_\text{vis}, {\mathbf{b}}_\text{hid} } p(\mathfrak{D}; {\mathbf{W}},{\mathbf{b}_\text{vis}},{\mathbf{b}_\text{hid}}) = \prod_{\mathbf{x} \in \mathfrak{D}} \sum_{\mathbf{h}\in \{0,1\}^{n'}} \frac{1}{J} \text{exp}\bigl(-E(\mathbf{\mathbf{x}},\mathbf{h)}\bigr).
\end{equation}
В данной работе используется модифицированная версия ограниченной машины Больцмана, позволяющая работать с небинарными входными данными~\cite{gbrbm}. В этой модификации энергия $E$ пары входного слоя $\mathbf{x}$ и скрытого слоя $\mathbf{h}$ выглядит следующим образом:
\[
E(\mathbf{x},\mathbf{h}) = \frac{(\mathbf{x} - \mathbf{b}_\text{vis})^2}{2\boldsymbol{\sigma}^2} -\mathbf{h}^\text{T} \cdot \mathbf{b}_\text{hid} - \frac{\mathbf{h}}{\boldsymbol{\sigma}}^\text{T}\mathbf{W}\mathbf{x},
\]
где $\boldsymbol{\sigma}$ --- оценка дисперсии объектов выборки $\mathfrak{D}$, деление производится покомпонентно.

Для решения задачи оптимизации~\eqref{eq:rbm} используется алгоритм, описанный в~\cite{hinton_rbm}.

\textbf{Автокодировщик.}
Автокодировщик предназначен для снижения размерности исходного пространства признаков.
Автокодировщик представляет собой суперпозицию кодирующего и декодирующего блока:
\[
 \mathbf{f}_\text{AE}' = \mathbf{f}_\text{enc}(\mathbf{f}_\text{dec}(\mathbf{x})),
\]
где $$ \mathbf{f}_\text{enc}(\mathbf{x}) = \boldsymbol{\sigma}(\mathbf{w}_\textbf{e}\mathbf{x}+\mathbf{b}_\textbf{e}) \text{ --- кодирующий блок,}$$
$$  \mathbf{f}_\text{dec}(\mathbf{x})) = \boldsymbol{\sigma}(\mathbf{w}_\textbf{d}\mathbf{g}(\mathbf{x})+\mathbf{b}_\textbf{d})\text{ --- декодирующий блок,}$$ $$\boldsymbol{\sigma}(x) = (1+\textbf{exp}({-\mathbf{x}}))^{-1} \text{ --- сигмоидная функция},$$ $\mathbf{w}_\textbf{e},\mathbf{w}_\textbf{d},\mathbf{b}_\textbf{e}, \mathbf{b}_\textbf{d}$ --- параметры модели.

Введем дополнительное ограничение на матрицы $\mathbf{w}_\textbf{e}, \mathbf{w}_\textbf{d}$:
\[
 \mathbf{w}_\textbf{e} = \mathbf{w}_\textbf{d}^{^\text{T}}.
\]

Оптимизацию параметров модели $\mathbf{w}_e$ будем проводить таким образом, чтобы по образу вектора $\mathbf{x}$, получаемому с помощью кодирующего блока, можно было получить вектор $\mathbf{f}_\text{AE}$, близкий к исходному входному $\mathbf{x}$, при помощи преобразования декодирующего блока:
\begin{equation}
\label{eq:ae}
 \mathbf{w}^{*}_\textbf{e},\mathbf{w}^{*}_\textbf{d},\mathbf{b}^{*}_\textbf{e}, \mathbf{b}^{*}_\textbf{d} = \argmin_{{\mathbf{w}}_\textbf{e},{\mathbf{w}}_\textbf{d},{\mathbf{b}}_\textbf{e}, {\mathbf{b}}_\textbf{d}} \frac{1}{|\mathfrak{D}|}\sum_{\mathbf{x} \in \mathfrak{D}} ||\mathbf{f}_\text{AE}(\mathbf{x})-\mathbf{x}||^2_2.
\end{equation}

Декодирующий блок $\mathbf{f}_{\text{dec}}$ требуется только для решения задачи оптимизации~\eqref{eq:ae} и не используется в суперпозиции ~\eqref{eq:wisdm_superposition}. Таким образом, элемент суперпозиции~\eqref{eq:wisdm_superposition} определен как
\[
	\mathbf{f}_\text{AE} = \mathbf{f}_{\text{enc}}(\mathbf{x}).
\]
\textbf{Двухслойная нейросеть.}
Двухслойная сеть представляет собой логистическую вектор-функцию:
\begin{equation}
\label{sm}
 \mathbf{f}_{\text{hidden}}(\mathbf{x}) = \mathbf{w}^\mathsf{T}_2 \textbf{tanh}(\mathbf{w}^\mathsf{T}_1 \mathbf{x}),
\end{equation}
\[
 \mathbf{f}_\text{SM}(\mathbf{x}) = \frac{\textbf{exp}\bigl(\mathbf{f}_{\text{hidden}}(\mathbf{x})\bigr)}{\sum_{j=1}^Z \text{exp}\bigl(\mathbf{f}_{\text{hidden}}^j(\mathbf{x})\bigr)},
\]
где $r$-я компонента вектора $\mathbf{f}_\text{SM}(\mathbf{x})$ интерпретируется как вероятность принадлежности объекта $\mathbf{x}$ классу $r$. Итоговая функция классификации~\eqref{eq:wisdm_superposition} ставит в соответствие  объекту $\mathbf{x}$ метку класса $y$, где $y$ --- класс, к которому принадлежит $\mathbf{x}$ с наибольшей вероятностью:
$$
 f(\mathbf{w},\mathbf{x})(r) = \begin{cases}
  1,\text{ если }r = \argmax_{r'} {f}_\text{SM}(\mathbf{f}_\text{AE}(\mathbf{f}_\text{RBM}(\mathbf{x}))(r'),\\
  0 \text{ иначе.}
	\end{cases}
$$
Здесь $\mathbf{f}_\text{AE}, \mathbf{f}_\text{RBM}$ --- автокодировщик~\eqref{eq:ae} и ограниченная машина Больцмана~\eqref{eq:rbm} соответственно, ${f}_\text{SM}(\mathbf{x})(r)$ --- $r$-я компонента вектора $ \mathbf{f}_\text{SM}$, $f(\mathbf{w},\mathbf{x})(r)$ --- $r$-я компонента вектор-функции $\mathbf{f}$.

Итоговая задача оптимизации выглядит следующим образом:
\[
 \boldsymbol{\theta}^{*} = \argmin\sum_{\mathbf{x},y \in \mathfrak{D}}\sum_{r = 1}^Z[y = r]\log(f^r_\text{SM}(\mathbf{f}_\text{AE}(\mathbf{f}_\text{RBM}(\mathbf{x}))),
\]
где $\boldsymbol{\theta}^{*} = [\hat{\mathbf{w}}_\text{RBM},\hat{\mathbf{b}}_\text{vis},\hat{\mathbf{b}_\text{hid}}, \hat{\mathbf{w}}_\textbf{e}, \hat{\mathbf{b}}_\textbf{e}, \hat{\mathbf{w}^\mathsf{T}_2}, \hat{\mathbf{w}^\mathsf{T}_1}]$ --- параметры ограниченной машины Больцмана~\eqref{eq:rbm}, автокодировщика~\eqref{eq:ae} и двухслойной сети~\eqref{sm}.


\textbf{Результаты вычислительного эксперимента. }
В качестве данных для проведения вычислительного эксперимента использовались данные WISDM~\cite{wisdm}, представляющие собой набор записей акселерометра мобильного телефона. Каждой записи соответствуют три координаты по осям акселерометра. Набор данных содержит записи движений для 6 классов переменной длины.
При проведении вычислительного эксперимента из каждой записи использовались первые 200 сегментов. Т. к. выборка не сбалансирована, в нее добавлялись повторы записей классов, содержащих количество записей, меньшее чем у большего класса.

Основные эксперименты --- исследование зависимости ошибки классификации от числа параметров и размера выборки --- были проведены как с использованием инструментария на базе библиотеки Theano, так и с использованием инструментария на языке Matlab.
Для оценки качества классификации была проведена процедура скользящего контроля~\cite{cv_ms} при соотношении числа объектов обучающей и контрольной выборки 3:1. Число нейронов на каждом слое задавалось из соотношения 10:6:3. При проведении процедуры скользящего контроля для каждого отсчета количества нейронов было произведено пять запусков. В эксперименте с использованием инструментария на базе Theano при обучении двухслойной нейронной сети проводился мультистарт~\cite{multi}, т.~е. одновременный запуск обучения сети с 8 разными стартовыми значениями параметров для предотвращения возможного застревания алгоритма обучения в локальном минимуме. При оценке качества классификации выбиралась модель с наилучшими результатами. График зависимости ошибки классификации от числа используемых нейронов изображен на рис.~\ref{fig:neurons}.


\begin{figure}[tb!]
 \centering
  \includegraphics[width=1.0\textwidth]{plots/popova/neurons.pdf}
 \caption{Зависимость ошибки от числа нейронов}
 \label{fig:neurons}
\end{figure}


Для оценки зависимости качества классификации от размера обучающей выборки была проведена кроссвалидация с фиксированным количеством объектов в обучающей выборке (25\% исходной выборки) и переменным размером обучающей выборки. Число нейронов было установлено как 364:224:112. При проведении процедуры скользящего контроля для каждого отсчета было произведено пять запусков. График зависимости ошибки классификации от размера обучающей выборки представлен на рис.~\ref{fig:samples}.


\begin{figure}[tb!]
 \centering
  \includegraphics[width=1.0\textwidth]{plots/popova/samples.pdf}
 \caption{Зависимость ошибки от размера обучающей выборки}
 \label{fig:samples}
\end{figure}


Для исследования скорости оптимизации нейросети в зависимости от конфигурации Theano был сделан следующий эксперимент:
проводилось обучение двухслойной нейросети на основе подсчитанных заранее параметров ограниченной машины Больцмана~\eqref{eq:rbm} и автокодировщика~\eqref{eq:ae}. Обучение проходило за 100 итераций. При обучении алгоритм запускался параллельно с $r$ разными стартовыми позициями, $r \in \{1,\dots,4\}.$ Число нейронов было установлено как 300:200:100.
Запуск осуществлялся со следующими конфигурациями Theano:
\begin{itemize}
\item вычисление на центральном процессоре, задействовано
одно ядро;
\item вычисление на центральном процессоре, задействовано четыре ядра;
\item вычисление на центральном процессоре, задействовано восемь ядер;
\item вычисление на графическом процессоре.
\end{itemize}

Результаты эксперимента приведены на рис.~\ref{fig:speed}. Как видно из графика, вычисление с использованием CUDA показывает значительное ускорение по сравнению с вычислением на центральном процессоре.

\begin{figure}[tb!]
 \centering
  \includegraphics[width=0.8\textwidth]{plots/popova/result.pdf}
 \caption{Результаты эксперимента по исследованию скорости процесса обучения}
 \label{fig:speed}
\end{figure}




\section{Модели парафраза (Смердов)}
Цель эксперимента~--- проверка работоспособности предложенного алгоритма и сравнение результатов с ранее полученными. В качестве данных использовалась выборка SemEval 2015, состоящая из 8331 пары схожих и несхожих предложений. Слова преобразовывались в векторы размерности 50 при помощи алгоритма GloVe~\cite{GloveURL}.
%\footnote{https://github.com/stanfordnlp/GloVe}
Для базовых алгоритмов тренировочная, валидационная и тестовая выборки составили 70\%, 15\% и 15\% соответственно.
Для рекуррентной нейронной сети, полученной вариационным методом, валидационная выборка отсутствовала, а тренировочная и тестовая выборки составили 85\% и 15\% соответственно.
Критерием качества была выбрана F1-мера.
В качестве базовых алгоритмов использовались линейная регрессия, метод ближайших соседей, решающее дерево и модификация метода опорных векторов SVC. Базовые алгоритмы взяты из библиотеки sklearn. 
%\cite{sklearn}.
%\footnote{http://scikit-learn.org/stable/}.
Дополнительно были построены рекуррентная нейросеть с одним скрытым слоем~\cite{Sanborn} и нейросеть с одним скрытым слоем и вариационной оптимизацией параметров~\cite{Graves, code}.
% \footnote{https://sourceforge.net/p/mlalgorithms/code/HEAD/tree/Group474/Smerdov2017Paraphrase/code/}.

% Как показал вычислительный эксперимент, вариационная нейросеть, полученная методом, описанным в~\cite{Graves}, позволяет значительно улучшить качество предсказаний.
\begin{figure}[!h]
	\centering
	\includegraphics[width=0.5\textwidth]{plots/smerdov/lambdas.pdf}
	\caption{Доля неудаленных параметров сети в зависимости от порогового значения $\lambda$ для скалярного~($I$) и диагонального~($D$) вида апостериорной матрицы ковариаций}
	\label{lambdas}
\end{figure}

 

На рис.~\ref{evidence_I_I} и~\ref{evidence_I_D} представлена зависимость оценки правдоподобия $L$ \eqref{loss} от параметра $\lambda$.
%Видно, что существует некоторое оптимальное значения $\lambda$, при котором оценка минимальна --- именно это значение $\lambda$ соответствует оптимальной модели.
Для обоих случаев существует оптимальное значение $\lambda$, минимизирующее $L$; модели с таким параметром будут оптимальными. На рис.~\ref{score_I_I},~\ref{score_I_D},~\ref{portion_I_I} и~\ref{portion_I_D} отображены зависимости качества модели от $\lambda$ и доли выброшенных параметров. Видно, что даже при удалении большинства параметров из сети качество предсказаний меняется несущественно, что говорит о слишком большом числе параметров исходной модели.

Из рис.~\ref{lambdas} видно, что при малых $\lambda$ из сети с диагональной апостериорной матрицей ковариаций удаляется больше весов, а при больших $\lambda$ --- меньше, что говорит о лучшем отборе параметров такой моделью.


\section{Прореживание модели (Грабовой)}
Для анализа свойств предложенного алгоритма и сравнения его с существующими был проведен вычислительный эксперимент в котором параметры нейросети удалялись методами,  которые были описаны в разделах 3.1---3.3 и методом Белсли.

В качестве данных использовались три выборки. Выборки Wine~\cite{Wine} и Boston~Housing~\cite{Boston}  --- это реальные данные. Синтетические данные сгенерированы таким образом чтобы параметры сети были мультиколинеарными. Генерация данных состояла из двух этапов. 
На первом этапе генерировался вектор параметров $\mathbf{w}_{\text{synthetic}}$:
$$\mathbf{w}_{\text{synthetic}}  \sim \mathcal{N}(\textbf{m}_{\text{synthetic}}, \textbf{A}_{\text{synthetic}}), \eqno(5.1)$$ 
где 
$\textbf{m}_{\text{synthetic}} = \begin{bmatrix}
1.0\\
0.0025\\
\cdots\\
0.0025
\end{bmatrix}$,
$\textbf{A}_{\text{synthetic}} = \begin{bmatrix}
1.0& 10^{-3}& \cdots& 10^{-3}& 10^{-3}\\
10^{-3}& 1.0& \cdots& 0.95& 0.95\\
\cdots&\cdots&\cdots&\cdots&\cdots\\
10^{-3}& 0.95& \cdots& 0.95& 1.0
\end{bmatrix}$.

На втором этапе генерировалась выборка $\mathfrak{D}_{\text{synthetic}}$:
$$\mathfrak{D}_{\text{synthetic}} = \{(\textbf{x}_i,y_i)| \textbf{x}_i \sim  \mathcal{N}(\textbf{1}, \textbf{I}), y_i = x_{i0}, i = 1 \cdots 10000\}. \eqno(5.2)$$
В приведенном выше векторе параметров $\mathbf{w}_{\text{synthetic}}$ для выборки $\mathfrak{D}_{\text{synthetic}}$, наиболее релевантным является первый параметр, а все остальные параметры являются нерелевантными. Матрица ковариации была выбрана таким образом, чтобы все нерелевантные параметры были зависимы и метод Белсли был максимально эффективен.



\begin{table}[h]

\begin{center}
\caption{Описание выборок}
\begin{tabular}{|c|c|c|c|}
\hline
	Выборка &Тип задачи& Размер выборки& Число признаков\\
	\hline
	
	\multicolumn{1}{|l|}{Wine}
	&
	\multicolumn{1}{|l|}{класификация}
	 & 178 & 13\\
	\hline
	
	\multicolumn{1}{|l|}{Boston Housing}
	&
	\multicolumn{1}{|l|}{регресия}
	& 506 & 13\\
	\hline
	
	\multicolumn{1}{|l|}{Synthetic data}
	&
	\multicolumn{1}{|l|}{регресия}
	& 10000 & 100\\
\hline

\end{tabular}
\end{center}
\end{table}



Для алгоритмов тренировочная и тестовая выборки составили~$80\%$ и~$20\%$ соответсвенно. Критерием качества прореживания служит процент параметров нейросети, удаление которого не влечет значимой потери качества прогноза. Также критерием качества служит устойчивость нейросети к зашумленности данных. 

Качеством прогноза $R_{\text{cl}}$ модели для задачи классификации является точность прогноза модели:
$$R_{\text{cl}} = \frac{\sum_{(\textbf{x},y)\in \mathfrak{D}} [f(\textbf{x}, \textbf{w}) = y]}{\left|\mathfrak{D}\right|}, \eqno(5.3)$$

Качеством прогноза $R_{\text{rg}} $ модели для задачи регрессии является среднеквадратическое отклонение результата модели от точного:

$$R_{\text{rg}} = \frac{\sum_{(\textbf{x},y)\in \mathfrak{D}} \left(f(\textbf{x}, \textbf{w}) - y\right)^2}{\left|\mathfrak{D}\right|}, \eqno(5.4)$$

\textbf{Wine.} Рассмотрим нейроную сеть с 13 нейронами на входе, 13 нейронами в скрытом слое и 3 нейронами на выходе.

\begin{figure}[h!t]\center
\includegraphics[width=0.8\textwidth]{plots/grabovoy/wine_all.pdf}\\
\caption{Качество прогноза при удаление параметров на выборке Wine}
\label{WineAll}
\end{figure}

\begin{figure}[ht]\center
%\begin{subfigure}[а]{0.33\textwidth}
\includegraphics[width=0.33\textwidth]{plots/grabovoy/wine_random_noise3d.pdf}
%\end{subfigure}
%\begin{subfigure}[б]{0.33\textwidth}
{\includegraphics[width=0.33\textwidth]{plots/grabovoy/obd_noise_3d.pdf}}
%\end{subfigure}
%\begin{subfigure}[в]{0.33\textwidth}
{\includegraphics[width=0.33\textwidth]{plots/grabovoy/var_noise_3d.pdf}}
%\end{subfigure}

%\subfigure[Оптимальное прореживание]{\includegraphics[width=0.5\textwidth]{plots/grabovoy/obd_noise_3d.pdf}}\\
%\subfigure[Вариационный метод]{\includegraphics[width=0.5\textwidth]{plots/grabovoy/var_noise_3d.pdf}}
\caption{Влияние шума в начальных данных на шум выхода нейросети на выборке Wine: a --- Произвольное удаление параметров, б --- Оптимальное прореживание, в --- Вариационный метод}
\label{WineNoise}
\end{figure}

На рис.~\ref{WineAll} показано как меняется точность прогноза $R_{\text{cl}}$ при удалении параметров указанными методами. Из графика видно, что метод оптимального прореживания, вариационный метод и метод Белсли позволяют удалить $\approx80\%$ параметров и качество всех этих методов падает при удалении $\approx90\%$ параметров нейросети. 

На рис.~\ref{WineNoise} показаны поверхности изменения уровня шума ответов нейросети при изменении процента удаленных параметров и уровня шума входных данных для разных методов прореживания. На графиках показано, что при удалении параметров нейросети методом Белсли шум меньше, чем при удалении параметров другими методами, на это указывает то что поверхность которая соответствует методу Белсли ниже других поверхностей.

\textbf{Boston Housing. } Рассмотрим нейроную сеть с 13 нейронами на входе, 39 нейронами в скрытом слое и одним нейроном на выходе.

\begin{figure}[h    !t]\center
\includegraphics[width=0.8\textwidth]{plots/grabovoy/boston_all.pdf}\\
\caption{Качество прогноза при удаление параметров на выборке Boston}
\label{BostonAll}
\end{figure}



\begin{figure}[ht]\center
%\begin{subfigure}[а]{0.33\textwidth}
\includegraphics[width=0.33\textwidth]{plots/grabovoy/boston_random.pdf}
%\end{subfigure}
%\begin{subfigure}[б]{0.33\textwidth}
{\includegraphics[width=0.33\textwidth]{plots/grabovoy/boston_obd.pdf}}
%\end{subfigure}
%\begin{subfigure}[в]{0.33\textwidth}
{\includegraphics[width=0.33\textwidth]{plots/grabovoy/boston_var.pdf}}
%\end{subfigure}

%\subfigure[Оптимальное прореживание]{\includegraphics[width=0.5\textwidth]{plots/grabovoy/obd_noise_3d.pdf}}\\
%\subfigure[Вариационный метод]{\includegraphics[width=0.5\textwidth]{plots/grabovoy/var_noise_3d.pdf}}
\caption{Влияние шума в начальных данных на шум выхода нейросети на выборке Boston: a --- Произвольное удаление параметров, б --- Оптимальное прореживание, в --- Вариационный метод}
\label{BostonNoise}
\end{figure}

На рис.~\ref{BostonAll} показано как меняется среднеквадратическое отклонение прогноза $\mathsf{R}_{\text{rg}}$ от точного ответа  при удалении параметров указанными методами. График показывает, что метод Белсли является более эффективным, чем другие методы, так-как позволяет удалить больше параметров нейросети без потери качества.

На рис.~\ref{BostonNoise} показаны поверхности изменения уровня шума ответов нейросети при изменении процента удаленных параметров и уровня шума входных данных для разных методов прореживания. График показывает, что уровень шума всех методов одинаковый, так-как поверхности всех методов находятся на одном уровне.


\textbf{Синтетические данные. } Рассмотрим нейроную сеть с 100 нейронами на входе и одним нейроном на выходе.

\begin{figure}[h!t]\center
\includegraphics[width=0.8\textwidth]{plots/grabovoy/synt_all.pdf}\\
\caption{Качество прогноза при удаление параметров на синтетической выборке}
\label{Data1All}
\end{figure}

\begin{figure}[ht]\center
%\begin{subfigure}[а]{0.33\textwidth}
\includegraphics[width=0.33\textwidth]{plots/grabovoy/synt_random.pdf}
%\end{subfigure}
%\begin{subfigure}[б]{0.33\textwidth}
{\includegraphics[width=0.33\textwidth]{plots/grabovoy/synt_obd.pdf}}
%\end{subfigure}
%\begin{subfigure}[в]{0.33\textwidth}
{\includegraphics[width=0.33\textwidth]{plots/grabovoy/synt_var.pdf}}
%\end{subfigure}

%\subfigure[Оптимальное прореживание]{\includegraphics[width=0.5\textwidth]{plots/grabovoy/obd_noise_3d.pdf}}\\
%\subfigure[Вариационный метод]{\includegraphics[width=0.5\textwidth]{plots/grabovoy/var_noise_3d.pdf}}
\caption{Влияние шума в начальных данных на шум выхода нейросети на выборке Boston: a --- Произвольное удаление параметров, б --- Оптимальное прореживание, в --- Вариационный метод}
\label{Data1Noise}
\end{figure}
На рис.~\ref{Data1All} показано как меняется среднеквадратическое отклонение прогноза от $\mathsf{R}_{\text{rg}}$ точного ответа при удалении параметров указанными методами. График показывает, что удаление параметров методом Белсли являеться более эффективным чем другие методы прореживания, так-как качество прогноза нейросети улучшается при удалении шумовых параметров.

На рис.~\ref{Data1Noise} показаны поверхности изменения уровня шума ответов нейросети при изменении процента удаленных параметров и уровня шума входных данных для разных методов прореживания. На графиках показано, что при удалении параметров нейросети методом Белсли шум меньше, чем при удалении параметров другими методами, так-как поверхность которая соответствует методу Белсли ниже других поверхностей.



