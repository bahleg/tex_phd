@inproceedings{greed_hyper,
  title = {Scalable Gradient-Based Tuning of Continuous Regularization Hyperparameters},
  author = {Jelena Luketina and Tapani Raiko and Mathias Berglund and Klaus Greff},
  year = {2016},
  doi = {http://jmlr.org/proceedings/papers/v48/luketina16.html},
  researchr = {http://researchr.org/publication/LuketinaRBG16},
  cites = {0},
  citedby = {0},
  pages = {2952-2960},
  booktitle = {Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016},
  editor = {Maria-Florina Balcan and Kilian Q. Weinberger},
  volume = {48},
  series = {JMLR Workshop and Conference Proceedings},
  publisher = {JMLR.org},
}

@article{reinf,
  title={Neural architecture search with reinforcement learning},
  author={Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1611.01578},
  year={2016}
}

@article{reinf_predict,
  title={Accelerating neural architecture search using performance prediction},
  author={Baker, Bowen and Gupta, Otkrist and Raskar, Ramesh and Naik, Nikhil},
  journal={CoRR, abs/1705.10823},
  year={2017}
}

@article{reinf_deep2net,
  title={Efficient Architecture Search by Network Transformation},
  author={Cai, Han and Chen, Tianyao and Zhang, Weinan and Yu, Yong and Wang, Jun},
  year={2018}
}

@article{reinf_transfer,
  title={Learning transferable architectures for scalable image recognition},
  author={Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V},
  journal={arXiv preprint arXiv:1707.07012},
  year={2017}
}

@article{optimal_racing,
  title={Toward Optimal Run Racing: Application to Deep Learning Calibration},
  author={Bousquet, Olivier and Gelly, Sylvain and Kurach, Karol and Schoenauer, Marc and Sebag, Michele and Teytaud, Olivier and Vincent, Damien},
  journal={arXiv preprint arXiv:1706.03199},
  year={2017}
}

@article{search_smbo,
  title={Progressive neural architecture search},
  author={Liu, Chenxi and Zoph, Barret and Shlens, Jonathon and Hua, Wei and Li, Li-Jia and Fei-Fei, Li and Yuille, Alan and Huang, Jonathan and Murphy, Kevin},
  journal={arXiv preprint arXiv:1712.00559},
  year={2017}
}

@inproceedings{l2l_by_gd_gd,
  title={Learning to learn by gradient descent by gradient descent},
  author={Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W and Pfau, David and Schaul, Tom and de Freitas, Nando},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3981--3989},
  year={2016}
}

@inproceedings{l2l,
  title={Learning to learn: Model regression networks for easy small sample learning},
  author={Wang, Yu-Xiong and Hebert, Martial},
  booktitle={European Conference on Computer Vision},
  pages={616--634},
  year={2016},
  organization={Springer}
}

@article{meta_sgd,
  title={Meta-SGD: Learning to Learn Quickly for Few Shot Learning},
  author={Li, Zhenguo and Zhou, Fengwei and Chen, Fei and Li, Hang},
  journal={arXiv preprint arXiv:1707.09835},
  year={2017}
}

@inproceedings{self_rnn,
  title={A neural network that embeds its own meta-levels},
  author={Schmidhuber, J{\"u}rgen},
  booktitle={Neural Networks, 1993., IEEE International Conference on},
  pages={407--412},
  year={1993},
  organization={IEEE}
}

@article{search_space,
  title={Deeparchitect: Automatically designing and training deep architectures},
  author={Negrinho, Renato and Gordon, Geoff},
  journal={arXiv preprint arXiv:1704.08792},
  year={2017}
}

@article{boost_res,
  title={Learning deep resnet blocks sequentially using boosting theory},
  author={Huang, Furong and Ash, Jordan and Langford, John and Schapire, Robert},
  journal={arXiv preprint arXiv:1706.04964},
  year={2017}
}

@article{layerwise_optimal,
  title={Layer-wise learning of deep generative models},
  author={Arnold, Ludovic and Ollivier, Yann},
  journal={arXiv preprint arXiv:1212.1524},
  year={2012}
}


@incollection{nvidia_prune, 
   title = {Learning both Weights and Connections for Efficient Neural Network}, 
   author = {Han, Song and Pool, Jeff and Tran, John and Dally, William}, 
   booktitle = {Advances in Neural Information Processing Systems 28}, 
   editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett}, 
   pages = {1135--1143}, 
   year = {2015}, 
   publisher = {Curran Associates, Inc.}, 
   url = {http://papers.nips.cc/paper/5784-learning-both-weights-and-connections-for-efficient-neural-network.pdf} 
   }

@inproceedings{cortes2017adanet,
  title={AdaNet: Adaptive Structural Learning of Artificial Neural Networks},
  author={Cortes, Corinna and Gonzalvo, Xavier and Kuznetsov, Vitaly and Mohri, Mehryar and Yang, Scott},
  booktitle={International Conference on Machine Learning},
  pages={874--883},
  year={2017}
}


@inproceedings{approx_hyper,
    author    = {Fabian Pedregosa},
    title     = {Hyperparameter optimization with approximate gradient},
    booktitle = {Proceedings of the 33nd International Conference on Machine Learning ({ICML})},
    year      = {2016},
    url       = {http://jmlr.org/proceedings/papers/v48/pedregosa16.html},
  }

@article{PopovaModel,
	title = {Выбор оптимальной модели классификации физической активности по измерениям акселерометра},
	journal = {Информатика и ее применения},
	volume = {9(1)},
	year = {2015},
	pages = {79-89},
	author = {Попова М.С., Стрижов В.В.},
	language = {russian},
	}

@inproceedings{vae_graph,
  title={Composing graphical models with neural networks for structured representations and fast inference},
  author={Johnson, Matthew and Duvenaud, David K and Wiltschko, Alex and Adams, Ryan P and Datta, Sandeep R},
  booktitle={Advances in neural information processing systems},
  pages={2946--2954},
  year={2016}
}

@article{metalearn,
  title={Simple principles of metalearning},
  author={Schmidhuber, Juergen and Zhao, Jieyu and Wiering, MA},
  journal={Technical report IDSIA},
  volume={69},
  pages={1--23},
  year={1996},
  publisher={IDSIA}
}


@ARTICLE{var_boost,
   author = {{Miller}, A.~C. and {Foti}, N. and {Adams}, R.~P.},
    title = "{Variational Boosting: Iteratively Refining Posterior Approximations}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1611.06585},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Learning, Statistics - Methodology},
     year = {2016},
    month = {nov},
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161106585M},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@incollection{Kingma,
title = {Semi-supervised Learning with Deep Generative Models},
author = {Kingma, Diederik P and Mohamed, Shakir and Jimenez Rezende, Danilo and Welling, Max},
booktitle = {Advances in Neural Information Processing Systems 27},
editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
pages = {3581--3589},
year = {2014},
publisher = {Curran Associates, Inc.},
language = {english},
url = {http://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models.pdf},
}


@inproceedings{SPCA,
 author = {Yu, Shipeng and Yu, Kai and Tresp, Volker and Kriegel, Hans-Peter and Wu, Mingrui},
 title = {Supervised Probabilistic Principal Component Analysis},
 booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 series = {KDD '06},
 year = {2006},
 isbn = {1-59593-339-5},
 location = {Philadelphia, PA, USA},
 pages = {464--473},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1150402.1150454},
 doi = {10.1145/1150402.1150454},
 acmid = {1150454},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dimensionality reduction, principal component analysis, semi-supervised projection, supervised projection},
language = {english},
} 

@inproceedings{vae,
 author = {D. Kingma, M. Welling},
 booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
 title = {Auto-Encoding Variational Bayes},
 year = {2014},
language = {english},
}

@INPROCEEDINGS{gendis_pictures, 
author={Yi Li and L. O. Shapiro and J. A. Bilmes}, 
booktitle={Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1}, 
title={A generative/discriminative learning algorithm for image classification}, 
year={2005}, 
volume={2}, 
pages={1605-1612 Vol. 2}, 
keywords={feature extraction;image classification;image colour analysis;image texture;learning (artificial intelligence);object recognition;discriminative learning;feature extraction;generative learning;image classification;image retrieval;object features;object recognition;salient region features;Classification algorithms;Computer science;Computer vision;Image classification;Image recognition;Image retrieval;Image segmentation;Layout;Object recognition;Videos}, 
doi={10.1109/ICCV.2005.7}, 
ISSN={1550-5499}, 
month={Oct},
language = {english},
}

@article{nn_incr,
  title={Incremental Training of Deep Convolutional Neural Networks},
  author={Istrate12, R and Malossi, ACI and Bekas, C and Nikolopoulos, D}
}

@inproceedings{branches,
  title={Branchynet: Fast inference via early exiting from deep neural networks},
  author={Teerapittayanon, Surat and McDanel, Bradley and Kung, HT},
  booktitle={Pattern Recognition (ICPR), 2016 23rd International Conference on},
  pages={2464--2469},
  year={2016},
  organization={IEEE}
}

@inproceedings{vae_mix,
  title={Infinite variational autoencoder for semi-supervised learning},
  author={Abbasnejad, M Ehsan and Dick, Anthony and van den Hengel, Anton},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={781--790},
  year={2017},
  organization={IEEE}
}

@article{layer_probe,
  title={Understanding intermediate layers using linear classifier probes},
  author={Alain, Guillaume and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1610.01644},
  year={2016}
}


@article{vae_stick,
  title={Deep Generative Models with Stick-Breaking Priors},
  author={Nalisnick, Eric and Smyth, Padhraic},
  journal={arXiv preprint arXiv:1605.06197},
  year={2016}
}

@article{gp_arc,
  title={Raiders of the lost architecture: Kernels for Bayesian optimization in conditional parameter spaces},
  author={Swersky, Kevin and Duvenaud, David and Snoek, Jasper and Hutter, Frank and Osborne, Michael A},
  journal={arXiv preprint arXiv:1409.4011},
  year={2014}
}

@inproceedings{jaakkola2010learning,
  title={Learning Bayesian network structure using LP relaxations},
  author={Jaakkola, Tommi and Sontag, David and Globerson, Amir and Meila, Marina},
  booktitle={Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages={358--365},
  year={2010}
}

@article{double_rnn,
  title={Tree-structured decoding with doubly-recurrent neural networks},
  author={Alvarez-Melis, David and Jaakkola, Tommi S},
  year={2016}
}

@inproceedings{gp_tree,
  title={Bayesian Optimization with Tree-structured Dependencies},
  author={Jenatton, Rodolphe and Archambeau, Cedric and Gonz{\'a}lez, Javier and Seeger, Matthias},
  booktitle={International Conference on Machine Learning},
  pages={1655--1664},
  year={2017}
}

@phdthesis{gendis_phd,
  author       = {Lasserre J.}, 
  title        = {Hybrid of generative and discriminative methods for machine learning},
  school       = {University of Cambridge},
  year         = 2008,
language = {english},
}


@article{gp_fusion,
  title={Structure Optimization for Deep Multimodal Fusion Networks using Graph-Induced Kernels},
  author={Ramachandram, Dhanesh and Lisicki, Michal and Shields, Timothy J and Amer, Mohamed R and Taylor, Graham W},
  journal={arXiv preprint arXiv:1707.00750},
  year={2017}
}

@book{MacKay,
 author = {MacKay, David J. C.},
 title = {Information Theory, Inference \& Learning Algorithms},
 year = {2002},
 isbn = {0521642981},
 publisher = {Cambridge University Press},
 address = {New York, NY, USA},
language = {english}
} 

@article{early,
  author    = {Dougal Maclaurin and
               David K. Duvenaud and
               Ryan P. Adams},
  title     = {Early Stopping is Nonparametric Variational Inference},
  journal   = {CoRR},
  volume    = {abs/1504.01344},
  year      = {2015},
  url       = {http://arxiv.org/abs/1504.01344},
  timestamp = {Sat, 02 May 2015 17:50:32 +0200},
  biburl    = {http://dblp.dagstuhl.de/rec/bib/journals/corr/MaclaurinDA15a},
  bibsource = {dblp computer science bibliography, http://dblp.org},
language = {english},
}

@InProceedings{langevin,
  author =    {Max Welling and Yee Whye Teh},
  title =     {Bayesian Learning via Stochastic Gradient Langevin Dynamics},
  booktitle = {Proceedings of the 28th International Conference on Machine Learning (ICML-11)},
  series =    {ICML '11},
  year =      {2011},
  editor =    {Lise Getoor and Tobias Scheffer},
  location =  {Bellevue, Washington, USA},
  isbn =      {978-1-4503-0619-5},
  month =     {June},
  publisher = {ACM},
  address =   {New York, NY, USA},
  pages=      {681--688},
language = {english},
}

@book{Bishop,
 author = {Bishop, Christopher M.},
 title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
 year = {2006},
 isbn = {0387310738},
 publisher = {Springer-Verlag New York, Inc.},
 address = {Secaucus, NJ, USA},
language = {english},
} 

@incollection{Bengio,
title = {Greedy Layer-Wise Training of Deep Networks},
author = {Bengio, Yoshua and Pascal Lamblin and Dan Popovici and Larochelle, Hugo},
booktitle = {Advances in Neural Information Processing Systems 19},
editor = {B. Sch\"{o}lkopf and J. C. Platt and T. Hoffman},
pages = {153--160},
year = {2007},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf},
language = {english},
}

@article{sgd_cont,
  title={Continuous-Time Limit of Stochastic Gradient Descent Revisited},
  author={Mandt, Stephan and Hoffman, Matthew D and Blei, David M},
language = {english},
}

@inproceedings{langevin_sato,
  title={Approximation analysis of stochastic gradient langevin dynamics by using fokker-planck equation and ito process},
  author={Sato, Issei and Nakagawa, Hiroshi},
  booktitle={Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
  pages={982--990},
  year={2014},
language = {english},
}

@article{sgd_conv,
  title={Gradient descent converges to minimizers},
  author={Lee, Jason D and Simchowitz, Max and Jordan, Michael I and Recht, Benjamin},
  journal={University of California, Berkeley},
  volume={1050},
  pages={16},
  year={2016},
language = {english},
}

@article{entropy,
  title={Information theoretic inequalities},
  author={Dembo, Amir and Cover, Thomas M and Thomas, Joy A},
  journal={Information Theory, IEEE Transactions on},
  volume={37},
  number={6},
  pages={1501--1518},
  year={1991},
  publisher={IEEE},
language = {english},
}

@article{pathnet,
  title={Pathnet: Evolution channels gradient descent in super neural networks},
  author={Fernando, Chrisantha and Banarse, Dylan and Blundell, Charles and Zwols, Yori and Ha, David and Rusu, Andrei A and Pritzel, Alexander and Wierstra, Daan},
  journal={arXiv preprint arXiv:1701.08734},
  year={2017}
}


@article{net2net,
  title={Net2net: Accelerating learning via knowledge transfer},
  author={Chen, Tianqi and Goodfellow, Ian and Shlens, Jonathon},
  journal={arXiv preprint arXiv:1511.05641},
  year={2015}
}

@article{partition,
  title={Reducing the Training Time of Neural Networks by Partitioning},
  author={Miranda, Conrado S and Von Zuben, Fernando J},
  journal={arXiv preprint arXiv:1511.02954},
  year={2015}
}

@article{mixed,
  title={Deep Learning as a Mixed Convex-Combinatorial Optimization Problem},
  author={Friesen, Abram L and Domingos, Pedro},
  journal={arXiv preprint arXiv:1710.11573},
  year={2017}
}

@article{supernet,
  title={Learning time-efficient deep architectures with budgeted super networks},
  author={Veniat, Tom and Denoyer, Ludovic},
  journal={arXiv preprint arXiv:1706.00046},
  year={2017}
}

@article{morph,
  title={Forward thinking: Building and training neural networks one layer at a time},
  author={Hettinger, Chris and Christensen, Tanner and Ehlert, Ben and Humpherys, Jeffrey and Jarvis, Tyler and Wade, Sean},
  journal={arXiv preprint arXiv:1706.02480},
  year={2017}
}

@article{energynet,
  title={EnergyNet: Energy-based Adaptive Structural Learning of Artificial Neural Network Architectures},
  author={Kristiansen, Gus and Gonzalvo, Xavi},
  journal={arXiv preprint arXiv:1711.03130},
  year={2017}
}

@article{ard,
  title={Automatic Relevance Determination For Deep Generative Models},
  author={Karaletsos, Theofanis and R{\"a}tsch, Gunnar},
  journal={arXiv preprint arXiv:1505.07765},
  year={2015}
}
@phdthesis {entropy2,
	title = {Information Loss in Deterministic Systems},
	journal = {Graz University of Technology},
	year = {2014},
	month = {June},
	address = {Graz},
	abstract = {<p>\&nbsp;A fundamental theorem in information theory \&ndash; the data processing inequality \&ndash; states that deterministic processing cannot increase the amount of information contained in a random variable\&nbsp;or a stochastic process. The task of signal processing is to operate on the physical representation\&nbsp;of information such that the intended user can access this information with little effort. In the\&nbsp;light of the data processing inequality, this can be viewed as the task of removing irrelevant\&nbsp;information, while preserving as much relevant information as possible.</p>
<div>This thesis defines information loss for memoryless systems processing random variables or\&nbsp;stochastic processes, both with and without a notion of relevance. These definitions are the\&nbsp;basis of an information-theoretic systems theory, which complements the currently prevailing\&nbsp;energy-centered approaches. The results thus developed are used to analyze various systems in\&nbsp;the signal processor\&rsquo;s toolbox: polynomials, quantizers, rectifiers, linear filters with and without\&nbsp;quantization effects, principal components analysis, multirate systems, etc. The analysis not only\&nbsp;focuses on the information processing capabilities of these systems: It also highlights differences\&nbsp;and similarities between design principles based on information-theoretic quantities and those</div>
<div>based on energetic measures, such as the mean-squared error. It is shown that, at least in some\&nbsp;cases, simple energetic design can be justified information-theoretically.</div>
<div>\&nbsp;</div>
<div>As a side result, this thesis presents two approaches to model complexity reduction for time-homogeneous, finite Markov chains. While one approach preserves full model information with\&nbsp;the cost of losing the (first-order) Markov property, the other approach yields a Markov chain on a\&nbsp;smaller state space with reduced model information. Finally, this thesis presents an information-theoretic characterization of strong lumpability, the case where the function of a Markov chain\&nbsp;is Markov (of some order).</div>},
	attachments = {https://www.spsc.tugraz.at/sites/default/files/Geiger_Thesis.pdf},
	author = {Bernhard Geiger},
language = {english},
}

@InProceedings{snoek_deep,
  title = 	 {Scalable Bayesian Optimization Using Deep Neural Networks},
  author = 	 {Jasper Snoek and Oren Rippel and Kevin Swersky and Ryan Kiros and Nadathur Satish and Narayanan Sundaram and Mostofa Patwary and Mr Prabhat and Ryan Adams},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {2171--2180},
  year = 	 {2015},
  editor = 	 {Francis Bach and David Blei},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/snoek15.pdf},
  url = 	 {http://proceedings.mlr.press/v37/snoek15.html},
  abstract = 	 {Bayesian optimization is an effective methodology for the global optimization of functions with expensive evaluations. It relies on querying a distribution over functions defined by a relatively cheap surrogate model. An accurate model for this distribution over functions is critical to the effectiveness of the approach, and is typically fit using Gaussian processes (GPs). However, since GPs scale cubically with the number of observations, it has been challenging to handle objectives whose optimization requires many evaluations, and as such, massively parallelizing the optimization. In this work, we explore the use of neural networks as an alternative to GPs to model distributions over functions. We show that performing adaptive basis function regression with a neural network as the parametric form performs competitively with state-of-the-art GP-based approaches, but scales linearly with the number of data rather than cubically. This allows us to achieve a previously intractable degree of parallelism, which we apply to large scale hyperparameter optimization, rapidly finding competitive models on benchmark object recognition tasks using convolutional networks, and image caption generation using neural language models.}
}

@article{feature_select,
  title={Challenges of feature selection for big data analytics},
  author={Li, Jundong and Liu, Huan},
  journal={IEEE Intelligent Systems},
  volume={32},
  number={2},
  pages={9--15},
  year={2017},
  publisher={IEEE}
}

@inproceedings{random_gaus,
  title={Bayesian Optimization in High Dimensions via Random Embeddings.},
  author={Wang, Ziyu and Zoghi, Masrour and Hutter, Frank and Matheson, David and De Freitas, Nando and others},
  booktitle={IJCAI},
  pages={1778--1784},
  year={2013}
}

@article{rbf_surrogate,
  title={Hyperparameter optimization of deep neural networks using non-probabilistic RBF surrogate model},
  author={Ilievski, Ilija and Akhtar, Taimoor and Feng, Jiashi and Shoemaker, Christine Annette},
  journal={arXiv preprint arXiv:1607.08316},
  year={2016}
}

@inproceedings{bo_gp,
  title={Practical bayesian optimization of machine learning algorithms},
  author={Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  booktitle={Advances in neural information processing systems},
  pages={2951--2959},
  year={2012}
}

@phdthesis{founds,
title={Foundations and Advances in Deep Learning},
author={Cho, Kyunghyun},
year={2014},
language={en},
pages={277},
keyword={deep learning; neural networks; multilayer perceptron; probabilistic model; restricted Boltzmann machine; deep Boltzmann machine; denoising autoencoder},
isbn={978-952-60-5575-6 (electronic); 978-952-60-5574-9 (printed)},
issn={1799-4942 (electronic); 1799-4934 (printed); 1799-4934 (ISSN-L)},
series={Aalto University publication series DOCTORAL DISSERTATIONS; 21/2014},
publisher={Aalto University; Aalto-yliopisto},
type={G5 Artikkeliväitöskirja},
url={http://urn.fi/URN:ISBN:978-952-60-5575-6},
language = {english},
}

@article{mnist,
  added-at = {2010-06-28T21:16:30.000+0200},
  author = {LeCun, Yann and Cortes, Corinna},
  biburl = {http://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
  groups = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash = {21b9d0558bd66279df9452562df6e6f3},
  intrahash = {935bad99fa1f65e03c25b315aa3c1032},
  keywords = {MSc _checked character_recognition mnist network neural},
  lastchecked = {2016-01-14 14:24:11},
  timestamp = {2016-01-14T15:24:40.000+0100},
  title = {{MNIST} handwritten digit database},
  url = {http://yann.lecun.com/exdb/mnist/},
  username = {mhwombat},
  year = 2010,
  language = {english},
}

@ARTICLE{multi, 
author={Yi Shang and B. W. Wah}, 
journal={Computer}, 
title={Global optimization for neural network training}, 
year={1996}, 
volume={29}, 
number={3}, 
pages={45-54}, 
keywords={learning (artificial intelligence);minimisation;neural nets;nonlinear programming;search problems;NOVEL;Nonlinear Optimization via External Lead;application problems;benchmark comparison;global minimization method;local minimum;local searches;neural network learning problems;neural network training;Feedforward neural networks;Feedforward systems;Heuristic algorithms;Minimization methods;Neural networks;Optimization methods;Search methods;Supervised learning;Testing;Topology}, 
doi={10.1109/2.485892}, 
ISSN={0018-9162}, 
month={Mar},}

@inproceedings{hinton_rbm, 
    Publisher = {Journal of Machine Learning Research - Proceedings Track}, 
    Author = {Ruslan Salakhutdinov and Geoffrey E. Hinton}, 
    Url = {http://jmlr.csail.mit.edu/proceedings/papers/v2/salakhutdinov07a/salakhutdinov07a.pdf}, 
    Booktitle = {Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics (AISTATS-07)}, 
    Title = {Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure}, 
    Volume = {2}, 
    Editor = {Marina Meila and Xiaotong Shen}, 
    Year = {2007}, 
    Pages = {412-419},
   }

@inproceedings{hinton_init,
    Publisher = {JMLR Workshop and Conference Proceedings},
    Author = {Ilya Sutskever and James Martens and George E. Dahl and Geoffrey E. Hinton},
    Url = {http://jmlr.org/proceedings/papers/v28/sutskever13.pdf},
    Booktitle = {Proceedings of the 30th International Conference on Machine Learning (ICML-13)},
    Title = {On the importance of initialization and momentum in deep learning},
    Number = {3},
    Month = may,
    Volume = {28},
    Editor = {Sanjoy Dasgupta and David Mcallester},
    Year = {2013},
    Pages = {1139-1147},
   } 

@article{greed,
author = "Barron, Andrew R. and Cohen, Albert and Dahmen, Wolfgang and DeVore, Ronald A.",
doi = "10.1214/009053607000000631",
fjournal = "The Annals of Statistics",
journal = "Ann. Statist.",
month = "02",
number = "1",
pages = "64--94",
publisher = "The Institute of Mathematical Statistics",
title = "Approximation and learning by greedy algorithms",
url = "http://dx.doi.org/10.1214/009053607000000631",
volume = "36",
year = "2008",
}

@Inbook{greed_mlp,
author="Tzikas, Dimitris
and Likas, Aristidis",
editor="Diamantaras, Konstantinos
and Duch, Wlodek
and Iliadis, Lazaros S.",
title="An Incremental Bayesian Approach for Training Multilayer Perceptrons",
bookTitle="Artificial Neural Networks -- ICANN 2010: 20th International Conference, Thessaloniki, Greece, September 15-18, 2010, Proceedings, Part I",
year="2010",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="87--96",
isbn="978-3-642-15819-3",
doi="10.1007/978-3-642-15819-3\_12",
url="http://dx.doi.org/10.1007/978-3-642-15819-3\_12",
}

@article{rvm,
 author = {Tipping, Michael E.},
 title = {Sparse Bayesian Learning and the Relevance Vector Machine},
 journal = {J. Mach. Learn. Res.},
 issue_date = {9/1/2001},
 volume = {1},
 month = sep,
 year = {2001},
 issn = {1532-4435},
 pages = {211--244},
 numpages = {34},
 url = {http://dx.doi.org/10.1162/15324430152748236},
 doi = {10.1162/15324430152748236},
 acmid = {944741},
 publisher = {JMLR.org},
} 

@article{hd,
 author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
 title = {A Fast Learning Algorithm for Deep Belief Nets},
 journal = {Neural Comput.},
 issue_date = {July 2006},
 volume = {18},
 number = {7},
 month = jul,
 year = {2006},
 issn = {0899-7667},
 pages = {1527--1554},
 numpages = {28},
 url = {http://dx.doi.org/10.1162/neco.2006.18.7.1527},
 doi = {10.1162/neco.2006.18.7.1527},
 acmid = {1161605},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@article{contractive,
  author    = {Guillaume Alain and
               Yoshua Bengio},
  title     = {What regularized auto-encoders learn from the data-generating distribution},
  journal   = {Journal of Machine Learning Research},
  volume    = {15},
  number    = {1},
  pages     = {3563--3593},
  year      = {2014},
  url       = {http://dl.acm.org/citation.cfm?id=2750359},
  timestamp = {Mon, 27 Apr 2015 15:45:37 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/jmlr/AlainB14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{score, 
    Publisher = {JMLR Workshop and Conference Proceedings}, 
    Title = {On autoencoder scoring}, 
    Url = {http://jmlr.org/proceedings/papers/v28/kamyshanska13.pdf}, 
    Booktitle = {Proceedings of the 30th International Conference on Machine Learning (ICML-13)}, 
    Author = {Hanna Kamyshanska and Roland Memisevic}, 
    Number = {3}, 
    Month = may, 
    Volume = {28}, 
    Editor = {Sanjoy Dasgupta and David Mcallester}, 
    Year = {2013}, 
    Pages = {720-728}, 
    Abstract = {Autoencoders are popular feature learning models because they are conceptually simple, easy to train and allow for efficient inference and training. Recent work has shown how certain autoencoders can assign an unnormalized ``score'' to data which measures how well the autoencoder can represent the data. Scores are commonly computed by using training criteria that relate the autoencoder to a probabilistic model, such as the Restricted Boltzmann Machine. In this paper we show how an autoencoder can assign meaningful scores to data independently of training procedure and without reference to any probabilistic model, by interpreting it as a dynamical system. We discuss how, and under which conditions, running the dynamical system can be viewed as performing gradient descent in an energy function, which in turn allows us to derive a score via integration. We also show how one can combine multiple, unnormalized scores into a generative classifier.} 
   }

@article{train_generative,
  added-at = {2016-03-01T00:00:00.000+0100},
  author = {Sønderby, Casper Kaae and Raiko, Tapani and Maaløe, Lars and Sønderby, Søren Kaae and Winther, Ole},
  biburl = {http://www.bibsonomy.org/bibtex/20bf986007faf1e1720395a35e7b1dcc0/dblp},
  ee = {http://arxiv.org/abs/1602.02282},
  interhash = {bc861e9e92e904635501498e3e3d7a98},
  intrahash = {0bf986007faf1e1720395a35e7b1dcc0},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2016-03-02T11:35:56.000+0100},
  title = {How to Train Deep Variational Autoencoders and Probabilistic Ladder Networks.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1602.html\#SonderbyRMSW16},
  volume = {abs/1602.02282},
  year = 2016
}

@article{ladder,
  added-at = {2015-08-02T00:00:00.000+0200},
  author = {Rasmus, Antti and Valpola, Harri and Honkala, Mikko and Berglund, Mathias and Raiko, Tapani},
  biburl = {http://www.bibsonomy.org/bibtex/289a04e0973f7d4fa56e5e0169e9f9e05/dblp},
  ee = {http://arxiv.org/abs/1507.02672},
  interhash = {7d90a3b658aa3a4b911e88ff18e3ce27},
  intrahash = {89a04e0973f7d4fa56e5e0169e9f9e05},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2015-08-04T11:39:21.000+0200},
  title = {Semi-Supervised Learning with Ladder Network.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1507.html\#RasmusVHBR15},
  volume = {abs/1507.02672},
  year = 2015
}

@article{tokmakova,
  author = {А. А. Токмакова and В. В. Стрижов},
  title = {Оценивание гиперпараметров линейных и регрессионных моделей при отборе шумовых и коррелирующих признаков},
  journal = {Информатика и её применения},
  year = {2012},
  volume = {6(4)},
  pages = {66-75},
  url = {http://strijov.com/papers/Tokmakova2011HyperParJournal\_Preprint.pdf},
  language={russian},
}

@article{zaitsev,
  author = {А. А. Зайцев and В. В. Стрижов and А. А. Токмакова},
  title = {Оценка гиперпараметров регрессионных моделей методом максимального правдоподобия},
  journal = {Информационные технологии},
  year = {2013},
  volume = {2},
  pages = {11-15},
  url = {http://strijov.com/papers/ZaytsevStrijovTokmakova2012Likelihood\_Preprint.pdf}
}

@TECHREPORT{strijov_webber,
       author = {Strijov, V. and Weber, Gerhard-Wilhelm},
     keywords = {Coherent Bayesian inference, Hyperparameters, Model generation, model selection, Regression},
        month = oct,
        title = {NONLINEAR REGRESSION MODEL GENERATION  USING HYPERPARAMETERS OPTIMIZATION},
         type = {Preprint},
       number = {2009-21},
         year = {2009},
  institution = {Institute of Applied Mathematics},
      address = {Middle East Technical University, 06800 Ankara, Turkey},
         note = {Preprint No. 149},
     abstract = {The problem of the non-linear regression analysis is 
considered. The algorithm of the inductive model 
generation is described. The regression model is a 
superposition of given smooth functions. To 
estimate the model parameters, two-level Bayesian 
Inference technique was used. It introduces 
hyperparameters, which describe the distribution 
function of the model parameters.}
}

@article{perekrestenko,
  author = {Д.О. Перекрестенко},
  title = {Анализ структурной и статистической сложности суперпозиции нейронных сетей},
  year = {2014},
  url = {http://sourceforge.net/p/mlalgorithms/code/HEAD/tree/Group074/Perekrestenko2014ComplexityAnalysis/doc/Perekrestenko2014ComplexityAnalysis.pdf}
}

@TECHREPORT{vladis,
title = {Model-based problem solving through symbolic regression via pareto genetic programming},
author = {Vladislavleva, E.},
year = {2008},
institution = {Tilburg University, School of Economics and Management},
type = {Other publications TiSEM},
abstract = {Pareto genetic programming methodology is extended by additional generic model selection and generation strategies that (1) drive the modeling engine to creation of models of reduced non-linearity and increased generalization capabilities, and (2) improve the effectiveness of the search for robust models by goal softening and adaptive fitness evaluations. In addition to the new strategies for model development and model selection, this dissertation presents a new approach for analysis, ranking, and compression of given multi-dimensional input-response data for the purpose of balancing the information content of undesigned data sets.},
url = {http://EconPapers.repec.org/RePEc:tiu:tiutis:65a72d10-6b09-443f-8cb9-88f3bb3bc31b}
}
@INPROCEEDINGS{mdl,
    author = {Peter Grünwald},
    title = {A Tutorial Introduction to the Minimum Description Length Principle},
    booktitle = {Advances in Minimum Description Length: Theory and Applications},
    year = {2005},
    publisher = {MIT Press},
}

@incollection{need_prune, 
    Title = {Predicting Parameters in Deep Learning}, 
    Url = {http://media.nips.cc/nipsbooks/nipspapers/paper\_files/nips26/1053.pdf}, 
    Booktitle = {Advances in Neural Information Processing Systems 26}, 
    Author = {Misha Denil and Babak Shakibi and Laurent Dinh and Marc'aurelio Ranzato and Nando D. Freitas}, 
    Editor = {C.j.c. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.q. Weinberger}, 
    Year = {2013}, 
    Pages = {2148--2156} 
   }

@Article{rob,
author="Xu, Huan
and Mannor, Shie",
title="Robustness and generalization",
journal="Machine Learning",
year="2012",
volume="86",
number="3",
pages="391--423",
abstract="We derive generalization bounds for learning algorithms based on their robustness: the property that if a testing sample is ``similar'' to a training sample, then the testing error is close to the training error. This provides a novel approach, different from complexity or stability arguments, to study generalization of learning algorithms. One advantage of the robustness approach, compared to previous methods, is the geometric intuition it conveys. Consequently, robustness-based analysis is easy to extend to learning in non-standard setups such as Markovian samples or quantile loss. We further show that a weak notion of robustness is both sufficient and necessary for generalizability, which implies that robustness is a fundamental property that is required for learning algorithms to work.",
issn="1573-0565",
doi="10.1007/s10994-011-5268-1",
url="http://dx.doi.org/10.1007/s10994-011-5268-1"
}

@article{intrig,
  added-at = {2014-01-06T00:00:00.000+0100},
  author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian J. and Fergus, Rob},
  biburl = {http://www.bibsonomy.org/bibtex/283a0e880777f36959760992d9d8eb1fb/dblp},
  ee = {http://arxiv.org/abs/1312.6199},
  interhash = {8be4106d034bcba43126e1ce32fd0b2a},
  intrahash = {83a0e880777f36959760992d9d8eb1fb},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2014-01-07T11:34:18.000+0100},
  title = {Intriguing properties of neural networks.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1312.html\#SzegedyZSBEGF13},
  volume = {abs/1312.6199},
  year = 2013
}

@INPROCEEDINGS{obd,
    author = {Yann Le Cun and John S. Denker and Sara A. Solla},
    title = {Optimal Brain Damage},
    booktitle = {Advances in Neural Information Processing Systems},
    year = {1990},
    pages = {598--605},
    publisher = {Morgan Kaufmann},
}

@inproceedings{obs,
  title={Optimal brain surgeon and general network pruning},
  author={Hassibi, Babak and Stork, David G and Wolff, Gregory J},
  booktitle={Neural Networks, 1993., IEEE International Conference on},
  pages={293--299},
  year={1993},
  organization={IEEE}
}

@article{popova,
  author = {М. С. Попова and В. В. Стрижов},
  title = {Выбор оптимальной модели классификации физической активности по измерениям акселерометра},
  journal = {Информатика и ее применения},
  year = {2015},
  volume = {9(1)},
  pages = {79-89},
  url = {http://strijov.com/papers/Popova2014OptimalModelSelection.pdf},
  language={russian},
}

@phdthesis{strijov_dsc,
  author = {Стрижов, В. В.},
  title = {Порождение и выбор моделей в задачах регрессии и классификации},
  school = {Вычислительный центр РАН},
  year = {2014},
  language={russian},
  url = {http://strijov.com/papers/Strijov2015ModelSelectionRu.pdf},
}

@article{hoffman,
 author = {Hoffman, Matthew D. and Blei, David M. and Wang, Chong and Paisley, John},
 title = {Stochastic Variational Inference},
 journal = {J. Mach. Learn. Res.},
 issue_date = {January 2013},
 volume = {14},
 number = {1},
 month = may,
 year = {2013},
 issn = {1532-4435},
 pages = {1303--1347},
 numpages = {45},
 url = {http://dl.acm.org/citation.cfm?id=2502581.2502622},
 acmid = {2502622},
 publisher = {JMLR.org},
 keywords = {Bayesian inference, Bayesian nonparametrics, stochastic optimization, topic models, variational inference},
} 


@article{weight_quantization,
  title={Incremental network quantization: Towards lossless cnns with low-precision weights},
  author={Zhou, Aojun and Yao, Anbang and Guo, Yiwen and Xu, Lin and Chen, Yurong},
  journal={arXiv preprint arXiv:1702.03044},
  year={2017}
}

@article{weight_quantization2,
  title={Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={arXiv preprint arXiv:1510.00149},
  year={2015}
}

@article{dropout,
  title={Dropout: A simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@incollection{nips,
title = {Practical Variational Inference for Neural Networks},
author = {Graves, Alex},
booktitle = {Advances in Neural Information Processing Systems 24},
editor = {J. Shawe-Taylor and R. S. Zemel and P. L. Bartlett and F. Pereira and K. Q. Weinberger},
pages = {2348--2356},
year = {2011},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks.pdf}
}

@inproceedings{bayes_compr,
  title={Bayesian compression for deep learning},
  author={Louizos, Christos and Ullrich, Karen and Welling, Max},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3290--3300},
  year={2017}
}

@inproceedings{varmc,
  added-at = {2015-07-05T00:00:00.000+0200},
  author = {Salimans, Tim and Kingma, Diederik P. and Welling, Max},
  biburl = {http://www.bibsonomy.org/bibtex/235fbdcbeb70cb01ad14472f9103bf546/dblp},
  booktitle = {ICML},
  crossref = {conf/icml/2015},
  editor = {Bach, Francis R. and Blei, David M.},
  ee = {http://jmlr.org/proceedings/papers/v37/salimans15.html},
  interhash = {ac857f47992f033774f0fe6f2612d89e},
  intrahash = {35fbdcbeb70cb01ad14472f9103bf546},
  keywords = {dblp},
  pages = {1218-1226},
  publisher = {JMLR.org},
  series = {JMLR Proceedings},
  timestamp = {2015-07-07T11:37:54.000+0200},
  title = {Markov Chain Monte Carlo and Variational Inference: Bridging the Gap.},
  url = {http://dblp.uni-trier.de/db/conf/icml/icml2015.html\#SalimansKW15},
  volume = 37,
  year = 2015
}

@inproceedings{sgld,
  author    = {Chunyuan Li and
               Changyou Chen and
               David E. Carlson and
               Lawrence Carin},
  title     = {Preconditioned Stochastic Gradient Langevin Dynamics for Deep Neural
               Networks},
  booktitle = {Proceedings of the Thirtieth {AAAI} Conference on Artificial Intelligence,
               February 12-17, 2016, Phoenix, Arizona, {USA.}},
  pages     = {1788--1794},
  year      = {2016},
  crossref  = {DBLP:conf/aaai/2016},
  url       = {http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11835},
  timestamp = {Thu, 21 Apr 2016 19:28:00 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/aaai/LiCCC16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{cv_ms,
author = "Arlot, Sylvain and Celisse, Alain",
doi = "10.1214/09-SS054",
fjournal = "Statistics Surveys",
journal = "Statist. Surv.",
pages = "40--79",
publisher = "The American Statistical Association, the Bernoulli Society, the Institute of Mathematical Statistics, and the Statistical Society of Canada",
title = "A survey of cross-validation procedures for model selection",
url = "http://dx.doi.org/10.1214/09-SS054",
volume = "4",
year = "2010"
}

@inproceedings{expensive,
  author    = {Abhinav Vishnu and
               Jeyanthi Narasimhan and
               Lawrence Holder and
               Darren J. Kerbyson and
               Adolfy Hoisie},
  title     = {Fast and Accurate Support Vector Machines on Large Scale Systems},
  booktitle = {2015 {IEEE} International Conference on Cluster Computing, {CLUSTER}
               2015, Chicago, IL, USA, September 8-11, 2015},
  pages     = {110--119},
  year      = {2015},
  crossref  = {DBLP:conf/cluster/2015},
  url       = {http://dx.doi.org/10.1109/CLUSTER.2015.26},
  doi       = {10.1109/CLUSTER.2015.26},
  timestamp = {Wed, 27 Apr 2016 17:41:15 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cluster/VishnuNHKH15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@Article{expensive2,
author="Krstajic, Damjan
and Buturovic, Ljubomir J.
and Leahy, David E.
and Thomas, Simon",
title="Cross-validation pitfalls when selecting and assessing regression and classification models",
journal="Journal of Cheminformatics",
year="2014",
volume="6",
number="1",
pages="1--15",
abstract="We address the problem of selecting and assessing classification and regression models using cross-validation. Current state-of-the-art methods can yield models with high variance, rendering them unsuitable for a number of practical applications including QSAR. In this paper we describe and evaluate best practices which improve reliability and increase confidence in selected models. A key operational component of the proposed methods is cloud computing which enables routine use of previously infeasible approaches.",
issn="1758-2946",
doi="10.1186/1758-2946-6-10",
url="http://dx.doi.org/10.1186/1758-2946-6-10"
}


@misc{bias,
           title = {Full versus incomplete cross-validation: measuring the impact of imperfect separation between training and test sets in prediction error estimation},
        abstract = {In practical applications of supervised statistical learning the separation of the training and test data is often violated through performing one or several analysis steps prior to estimating the prediction error by cross-validation (CV) procedures. We refer to such practices as incomplete CV. For the special case of preliminary variable selection in high-dimensional microarray data the corresponding error estimate is well known to be strongly downwardly biased, resulting in over-optimistic conclusions regarding prediction accuracy of the fitted models. However, while other data preparation steps may also be affected by these types of problems, their impact on error estimation is far less acknowledged in the literature. In this paper we shed light on these issues. We present a new measure quantifying the impact of incomplete CV that is based on the ratio between the errors estimated by incomplete CV and by a formally correct "full CV." The new measure is illustrated through applications to several low- and high-dimensional biomedical data sets  and various data preparation steps including preliminary variable selection, choice of tuning parameters, normalization of gene expression microarray data, and imputation of missing values. It may be used in biometrical applications to determine whether specific data preparation steps can be safely performed as preliminary steps before running the CV procedure, or if they should be repeatedly trained in each CV iteration.},
          series = {tech},
          author = {Roman Hornung and Christoph Bernau and Caroline Truntzer and Thomas Stadler and Anne-Laure Boulesteix},
         keyword = {Cross-validation, Over-optimism, Good practice, Error estimation, Practical guidelines, Supervised learning},
            year = {2014},
          volume = {159},
             url = {http://nbn-resolving.de/urn/resolver.pl?urn=nbn:de:bvb:19-epub-20682-6}
}

@article{bias2,
 author = {Bengio, Yoshua and Grandvalet, Yves},
 title = {No Unbiased Estimator of the Variance of K-Fold Cross-Validation},
 journal = {J. Mach. Learn. Res.},
 issue_date = {12/1/2004},
 volume = {5},
 month = dec,
 year = {2004},
 issn = {1532-4435},
 pages = {1089--1105},
 numpages = {17},
 url = {http://dl.acm.org/citation.cfm?id=1005332.1044695},
 acmid = {1044695},
 publisher = {JMLR.org},
} 


@inproceedings{cib_simple,
  title={Learning the structure of deep convolutional networks},
  author={Feng, Jiashi and Darrell, Trevor},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2749--2757},
  year={2015}
}

@article{shirakawa2018dynamic,
  title={Dynamic Optimization of Neural Network Structures Using Probabilistic Modeling},
  author={Shirakawa, Shinichi and Iwata, Yasushi and Akimoto, Youhei},
  journal={arXiv preprint arXiv:1801.07650},
  year={2018}
}

@inproceedings{cib,
  title={Learning the structure of deep sparse graphical models},
  author={Adams, Ryan and Wallach, Hanna and Ghahramani, Zoubin},
  booktitle={Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages={1--8},
  year={2010}
}

@inproceedings{hyper, 
   Publisher = {JMLR Workshop and Conference Proceedings}, 
   title="Gradient-based Hyperparameter Optimization through Reversible Learning", 
   Author={Maclaurin, Dougal and Duvenaud, David and Adams, Ryan}, 
   year="2015", 
   Booktitle="Proceedings of the 32nd International Conference on Machine Learning (ICML-15)", 
   Editor={David Blei and Francis Bach}, 
   pages="2113-2122", 
   url="http://jmlr.org/proceedings/papers/v37/maclaurin15.pdf" 
   }

@inproceedings{hyper2,
  added-at = {2013-11-25T00:00:00.000+0100},
  author = {Domke, Justin},
  biburl = {http://www.bibsonomy.org/bibtex/25fe6cc5e31520f0b4456df9ac449120c/dblp},
  booktitle = {AISTATS},
  crossref = {conf/aistats/2012},
  editor = {Lawrence, Neil D. and Girolami, Mark A.},
  ee = {http://jmlr.csail.mit.edu/proceedings/papers/v22/domke12.html},
  interhash = {e58681bab10cf3e76d20b51db2d0dc1c},
  intrahash = {5fe6cc5e31520f0b4456df9ac449120c},
  keywords = {dblp},
  pages = {318-326},
  publisher = {JMLR.org},
  series = {JMLR Proceedings},
  timestamp = {2016-04-09T11:41:26.000+0200},
  title = {Generic Methods for Optimization-Based Modeling.},
  url = {http://dblp.uni-trier.de/db/journals/jmlr/jmlrp22.html\#Domke12},
  volume = 22,
  year = 2012
}


@unpublished{var_grad1,
  author = {Nicholas, Altieri and Duvenaud D.},
  btitle = {Variational Inference with Gradient Flows},
  url = {http://approximateinference.org/accepted/AltieriDuvenaud2015.pdf},
  biburl = {123},
  note = {asd},
  year = 2015
}
@unpublished{var_grad,
  title = {Variational Inference with Gradient Flows},
  author = {Nicholas, Altieri and Duvenaud D.},
  year = {2016},
  url = {URL: http://approximateinference.org/accepted/AltieriDuvenaud2015.pdf},
  note = {дата обращения: 15.05.2016}
} 

