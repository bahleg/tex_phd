В данной главе рассматривается задача выбора структуры модели глубокого обучения. Предлагается ввести вероятностные предположения о распределении параметров и распределении структуры модели. 
Проводится градиентная оптимизация параметров и гиперпараметров модели на основе байесовского вариационного вывода.  В качестве оптимизируемой функции для гиперпараметров модели предлагается обобщенная функция обоснованности. Показано, что данная функция оптимизирует несколько критериев выбора структуры модели: метод максимального правдоподобия, последовательное увеличение и снижению сложности модели, полный перебор структуры модели, а также получение максимума вариационной оценки обоснованности модели. Решается двухуровневая задача оптимизации: на первом уровне проводится оптимизация нижней оценки обоснованности модели по вариационным параметрам модели. На втором уровне проводится оптимизация гиперпараметров модели.

\section{Вероятностная модель}
Определим априорные распределения параметров и структуры модели следующим образом.
Пусть для каждого ребра $(j,k) \in E$ и каждой базовой функции $\mathbf{g}^{j,k}_l$ параметры модели $\w^{j,k}_l$ распределены нормально с нулевым средним:
\[
    \w^{j,k}_l \sim \mathcal{N}\bigl(\mathbf{0}, \g^{j,k}_l(\A^{j,k}_l)^{-1}\bigr),
\]
где $ (\A^{j,k}_l)^{-1}$ --- диагональная матрица. Априорное распределение $p(\w|\G, \h)$ параметров $\w^{j,k}_l$ зависит не только от гиперпараметров $\A_k^{j,k}$, но и от структурного параметра $\g^{j,k}_l$.


В качестве априорного распределения для структуры $\G$ предлагается использовать произведение распределений Gumbel-Softmax ($\mathcal{GS}$)~\cite{gs}:
\[
    \priorG = \prod_{(j,k) \in E} p(\g^{j,k}|\s^{j,k}, \lamT),
\]
где для каждого структурного параметра $\g$ с количеством базовых функций $K$ вероятность $p(\g|\s, \lamT)$ определна следующим образом:
\[
    p(\g|\s, \lamT) = (K-1)!\lamT^{K-1}\prod_{l=1}^K s_l\g_l^{-\lamT -1} \left(\sum_{l=1}^K s_l\g_l^{-\lamT}\right)^{-K},
\]
где $\s \in (0,\infty)^K$ --- гиперпараметр, отвечающий за смещенность плотности распределения относительно точек симплекса на $K$ вершинах, $\lamT$ --- метапараметр температуры, отвечающий за концентрацию плотности вблизи вершин симплекса или в центре симплекса.

Перечислим свойства, которыми обладает распределение Gumbel-Softmax:
\begin{enumerate}
\item Реализация $\hat{\gamma}_l,$ т.е. $l$-й компоненты случайной величины $\g$ порождается следующим образом:
\[
    \hat{\gamma}_l = \frac{\text{exp}(\log s_l+\hat{g}_l)/\lamT}{\sum_{l'=1}^{K}\text{exp}(\log s_{l'}+\hat{g}_{l'})/\lamT},
\]
где $\hat{\g} \sim -\log \bigl(-\log\mathcal{U}(0,1)^K\bigr).$ 

\item Свойство округления: $p(\gamma_{l_1} > \gamma_{l_2}, l_1\neq l_2|\mathbf{s}, \lamT) = \frac{s_l}{\sum_{l'}s_{l'}}.$

\item При устремлении температуры к нулю реализация $\hat{\gamma}$ случайной величины концентрируется на вершинах симплекса:
\[
p(\lim_{\lamT \to 0}\hat{\gamma}_{l}= 1|\mathbf{s}, \lamT)=\frac{s_l}{\sum_{l'}s_{l'}}.
\]


\item При устремлении температуры к бесконечности плотность распределения концентрируется в центре симплекса:
\begin{equation}
\label{eq:theorem_gs}
    \lim_{\lamT \to \infty}  p(\g|\s, \lamT) = 
    \begin{cases}
    \infty, \g = \frac{1}{K}, l \in \{1,\dots,K\},\\
    0, \text{ иначе.}
    \end{cases}
\end{equation}
\end{enumerate}

Доказательства первых трех утверждений приведены в~\cite{gumbel}. Докажем утверждение 4.

\begin{proof} 
Формула плотности записывается следующим образом с точностью до множителя:
\begin{equation}
\label{eq:pdf_proof}
    p(\g|\s, \lamT) \propto    \frac{\lamT^{K-1}}{\left(\sum_{l=1}^K s_l\gamma_l^{-\frac{K-1}{K}\lamT}\prod_{l'=1}^K [l \neq l']\gamma_l^{\frac{1}{K}\lamT}\right)^{K}}.
\end{equation}

Заметим, что числитель $\lamT^{K-1}$ имеет меньшую скорость сходимости, чем знаменатель, поэтому для вычисления предела достаточно проанализировать только знаменатель. Знаменатель под степенью $(-K)$ представляется суммой слагаемых следующего вида: 
\begin{equation}
\label{eq:gs}
    \left(\frac{\prod_{l' \neq l} \gamma_{l'}^{\frac{1}{K}}}{\gamma_l^{\frac{K-1}{K}}}\right)^{\lamT}.
\end{equation}

Рассмотрим два случая: когда вектор $\g$  лежит не в центре симплекса, и когда  $\g$ лежит в центре симплекса. 
Пусть хотя бы для одной компоненты $l$ выполнено: $\gamma_l \neq \frac{1}{K}$. Пусть $l'$ соответствует индексу максимальной компоненты вектора $\g$.
Для $l=l'$ предел выражения~\eqref{eq:gs} при $\lamT$ стремится к бесконечности. Для $l\neq l'$ предел выражения~\eqref{eq:gs} при $\lamT$ стремится к нулю. Возводя сумму пределов в степень $(-K)$ получаем предел плотности, равный нулю.

Рассмотрим второй случай. Пусть ${\gamma}_l = \frac{1}{K}$ для всех $l$.
Тогда выражение~\eqref{eq:pdf_proof} с точностью до множителя упрощается до $\lamT^{K-1}$. Предел данного выражения стремится к бесконечности.
Таким образом, предел плотности Gumbel-Softmax равен выражению~\eqref{eq:theorem_gs}, что и требовалось доказать.

\end{proof}


Первое свойство Gumbel-Softmax распределения позволяет использовать репараметризацию при вычислении градиента в вариационном выводе (англ. reparametrization trick). 
\begin{defin} Репараметризацией случайной величины $\psi$, распределенную по распределению $q$ с параметрами $\teta_\psi$ назовем представление величины с помощью другой случайной величины, имеющей  распределение, не зависящее от параметров $\teta$:
\[
    \psi \sim q \iff \hat{\psi} \sim g(\boldsymbol{\varepsilon},\teta_\psi),
\]
где $\boldsymbol{\varepsilon}$ --- случайная величина, чье распределение не зависит от параметров $\teta_\psi$, $g$ --- некоторая детерминированная функция, $\hat{\psi}$ --- реализация случайной величины $\psi$.
\end{defin}

Идею репараметризации поясним на следующем примере.
\begin{example} Пусть структура $\G$ определена для модели $\model$ однозначно. Рассмотрим математическое ожидание логарифма правдоподобия выборки модели по некоторому непрерывному распределению $q$:
\[
    \E_q \log~\LL=  \int_{\w} \log~\LL \qw d\w.
\]
Продифференцируем данное выражение по параметрам $\tetaw$ вариационного распределения $\qw$:
\[
    \nabla_{\tetaw} \E_\q \log \LL = 
\int_{\w}  \log \LL \nabla_{\tetaw}\qw d\w.
\]
Выражение общем виде не имеет аналитического решения. Пусть распределение $q$ для параметров $\mathbf{w}$ подлежит репараметризации:
\[
    \w \sim \qw \iff \hat{\w} \sim g(\boldsymbol{\varepsilon},\tetaw).
\] 
Тогда справедливо следующее выражение:
\[
 \nabla_{\tetaw} \E_q \log \LL = \nabla_{\tetaw} \E_{\boldsymbol{\varepsilon}} \log p(\y|g(\boldsymbol{\varepsilon}, \tetaw), \X, \h, \lam) =
\]
\[= \int_{\boldsymbol{\varepsilon}}\nabla_{\tetaw} \log~p(\y|g(\varepsilon,\teta), \X, \h,\lam) p(\boldsymbol{\varepsilon}) d\boldsymbol{\varepsilon}=\E_{\boldsymbol{\varepsilon}} \nabla_{\teta} \log p(\y|g(\boldsymbol{\varepsilon}, \teta), \X, \h, \lam).\]
Таким образом, распределение, позволяющее произвести репараметризацию, является более удобным для вычисления интегральных оценок.
Кроме того, данный подход позволяет значительно повысить точность вычисления градиента от функций, зависящих от случайных величин~\cite{reparametrization}.
% отсюда: http://gregorygundersen.com/blog/2018/04/29/reparameterization/
\end{example}

Пример распределения Gumbel-Softmax при различных параметрах представлен на Рис.~\ref{fig:gs}. В качестве альтернативы для априорного распределения на структуре выступает  распределение Дирихле. В качестве предельного случая, когда все структуры равнозначны, выступает равномерное распределение. Выбор в качестве распределения на структуре произведения Gumbel-Softmax распределения обоснован выбором этого же распределения в качестве вариационного. 

\begin{figure}
 \begin{minipage}[t]{.2\textwidth}
        \centering
\begin{tikzpicture}[%
x={(1.7cm,0cm)},
y={(0cm,1.7cm)},
]

\coordinate (A) at (0,0); 
\coordinate (B) at (1,0) ;
\coordinate (C) at (0.5,0.86); 

%Ecken
\node[circle,scale=0.5,fill=black,draw=black](Ap) at (0,0){};
\node[circle,scale=0.5,fill=black,draw=black](Bp) at (1,0){};
\node[circle,scale=0.5,fill=black,draw=black](Cp) at (0.5,0.86){};

%Kanten
\draw[] (A)
-- (B)  node[midway, below]{}
-- (C)      node[midway, right]{}
-- (A)  node[midway, left]{};

\end{tikzpicture}
\subcaption{}
\end{minipage}
\hfill
 \begin{minipage}[t]{.2\textwidth}
   \includegraphics[width=\textwidth]{plots/notebooks/gs1.png}
\subcaption{}
\end{minipage}
\hfill
 \begin{minipage}[t]{.2\textwidth}
   \includegraphics[width=\textwidth]{plots/notebooks/gs5.png}
\subcaption{}
\end{minipage}
\hfill
 \begin{minipage}[t]{.2\textwidth}
   \includegraphics[width=\textwidth]{plots/notebooks/gs5_shift.png}
\subcaption{}
\end{minipage}

\caption{Пример распределения Gumbel-Softmax при различных значениях параметров: а)~$\lamT\to0$, б)~$\lamT=1, \s=[1,1,1]$, в)~$\lamT=5, \s=[1,1,1]$, г)~$\lamT=5, \mathbf{s}=[10,0.1,0.1].$}
\label{fig:gs}

\end{figure}


Заметим, что предлагаемое априорное распределение неоднозначно: одно и то же распределение  можно получить с различными значениями гиперпарамета $\mathbf{A}^{j,k}_l$ и структурного параметра $\gamma^{j,k}_l$. В качестве регуляризатора для матрицы $(\mathbf{A}^{j,k}_l)^{-1}$ предлагается использовать обратное гамма-распределение:
\[
    (\A^{j,k}_l)^{-1} \sim \text{inv-gamma}(\lambda_1,\lambda_2),
\]
где $\lambda_1,\lambda_2 \in \lam$ --- метапараметры оптимизации. 
Использование обратного гамма-распределения в качестве распределения гиперпараметров можно найти в~\cite{bishop,mackay}. В данной работе обратное распределение выступает как регуляризатор гиперпараметров.
Варьируя метапарамы $\lambda_1,\lambda_2$ получается  более сильная или более слабая регуляризация~\cite{rvm}. Пример распределений $\text{inv-gamma}(\lambda_1,\lambda_2)$ для разных значений метапараметров $\lambda_1,\lambda_2$ изображен на Рис.~\ref{fig:inv-gamma}. Оптимизации без регуляризации соответствует случай предельного распределения $\lim_{\lambda_1,\lambda_2\to 0}\text{inv-gamma}(\lambda_1, \lambda_2)$.

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{plots/notebooks/invgamma.png}
\caption{Графики обратных гамма распределений для различных значений метапараметров.}
\label{fig:inv-gamma}
\end{figure}


Таким образом, предлагаемая вероятностная модель содержит следующие компоненты:
\begin{enumerate}
\item Параметры $\w$ модели, распределенные нормально.
\item Структура модели $\G$, содержащая все структурные параметры $\{\g^{j,k}, (j,k) \in E\}$ распределены по распределению Gumbel-Softmax.
\item Гиперпараметры: $\h = [\text{diag}(\A), \s]$, где $\A$ --- конкатенация матриц $\A^{j,k}, (j,k) \in E,$ $\s$ --- конкатенация параметров Gumbel-Softmax распределений $\s^{j,k}, (j,k) \in E$, где $E$ --- множество ребер, соответствующих графу рассматриваемого параметрического семейства.
\item Метапараметры: $\lam = [\lambda_1, \lambda_2, \lamT].$ Эти параметры не подлежат оптимизации и задаются экспертно. 
\end{enumerate}

График вероятностной модели в формате плоских нотаций представлен на Рис.~\ref{fig:plate_prob}.
\begin{figure}
\centering
   \includegraphics[width=0.5\textwidth]{plots/notebooks/simple_plate.pdf}
\caption{График предлагаемой вероятностной модели в формате плоских нотаций. Переменные обозначены белыми и серыми кругами, константы обозначены обведенными черными кругами. Наблюдаемые переменные обозначены серыми кругами.}
\label{fig:plate_prob}
\end{figure}

\section{Вариационная оценка для обоснованности вероятностной модели}
В качестве критерия выбора структуры модели предлагается использовать апостериорную вероятность гиперпараметров:
\begin{equation}
\label{eq:optimal_hyper}
    \posth \propto \EV \priorh \to \max_{\h \in \Hb},
\end{equation}
где структура модели и параметры модели выбираются на основе полученных значений гиперпараметров:
\[
    \G^* = \argmax_{\G \in \Gb} p(\G|\y, \X, \h^*, \lam ),
\]
\[
    \w^* = \argmax_{\w \in \Wb} p(\w|\y, \X, \G^*, \g^*),
\]
где $\h^*$ --- решение задачи оптимизации~\eqref{eq:optimal_hyper}.

Для вычисления обоснованности $$\EV = \iint_{\G, \w}\LL \priorw \priorG d\G d\w$$ из~\eqref{eq:optimal_hyper} предлагается использовать вариационную оценку обоснованности.

\begin{theorem}
Пусть $\q = \qw \qG$ --- вариационное распределение c параметрами $\teta= [\tetaw, \tetaG ]$, аппроксимирующее апостериорное распределение структуры и параметров:
\[
    \q \approx \post,
\]
\[
    \qw  \approx \postw,
\]
\[
    \qG \approx \qG.
\]

Тогда справедлива следующая оценка:
\begin{equation}
\label{eq:full_elbo}
\log \EV \geq
\end{equation}
\[
 \E_{\G \sim \qG} \E_{\w \sim \qw} \log \LL - \KL{\qG}{\priorG} - 
\]
\[
 - \KL{\qw}{\priorw},
\]
где $\KL{\qw}{\priorw}$ вычисляется по формуле условной дивергенции~\cite{TODO}:
\[
\KL{\qw}{\priorw} = \E_{\G \sim \qG} \E_{\w \sim \qw} \log (\frac{\qw)}{\priorw}).
\]
\end{theorem}

\begin{proof}
Рассмотрим обоснованность:
\[
\log \EV  =  \log \iint_{\G,\w} \LL \priorw \priorG d\G d\w  =
\]
\[
   = \log\iint_{\G,\w} \LL \priorw \frac{\qw}{\qw}d\G d\w =
\]
\[
  =  \log \E_{\q} \frac{\EV}{\q}.
\]
Используя неравенство Йенсена получим 
\[
 \log \E_{\q} \frac{\EV}{\q} =
\]
\[
   \E_{\q} \log \LL - \KL{\q}{\prior}.
\]
Декомпозируем распределение $q$ по свойству условной дивергенции:
\[
\KL{\q}{\prior} = 
\]
\[
= \KL{\qG}{\priorG} + \E_{\G \sim \qG} \E_{\w \sim \qw} \log (\frac{\qw)}{\priorw}).    
\]
\end{proof}
В качестве вариационного распределения $\qw$ предлагается использовать нормальное распределение, не зависящее от структуры модели $\G$:
\[
    \qw  \sim \mathcal{N}(\boldsymbol{\mu}_q, \A_q), 
\]
где $\A_q$ --- диагональная матрица с диагональю $\boldsymbol{\alpha}_q$.

В качестве вариационного распределения $\qG$ предлагается использовать произведение распределений Gumbel-Softmax. Конкатенацию параметров концентрации распределений обозначим $\s_q$. Его температуру, общую для всех структурных параметров $\g \in \G$, обозначим $\theta_\text{temp}$.
Вариационными параметрами распределения $\q$ являются параметры распределений $\qw, \qG$:
\[\teta =[\boldsymbol{\mu}_q, \boldsymbol{\alpha}_q,\s_q, \theta_\text{temp}]. 
\]
График вероятностной вариационной модели в формате плоских нотаций представлен на Рис.~\ref{fig:plate_qprob}.
\begin{figure}
\centering
   \includegraphics[width=0.5\textwidth]{plots/notebooks/plate.pdf}
\caption{График предлагаемой вероятностной вариационной модели в формате плоских нотаций. Переменные обозначены белыми и серыми кругами, константы обозначены обведенными черными кругами. Вариационное распределение обозначено черным кругом. Наблюдаемые переменные обозначены серыми кругами.}
\label{fig:plate_qprob}
\end{figure}

Для анализа сложности полученной модели введем понятие \textit{параметрической сложности}. 
\begin{defin} 
Параметрической сложностью  $C_p(\teta|\Uh, \lam)$ модели с вариационными параметрами $\teta$ на компакте $\Uh \subset \Hb$ назовем минимальную дивергенцию между вариационным и априорным распределением:
\[
C_p(\teta|\Uh, \lam) = \min_{\h \in \Uh} \KL{\q}{\prior}.
\]
\end{defin}
Параметрическая сложность модели соответствует ожидаемой длине описания параметров модели при условии заданного параметрического априорного распределения~\cite{hinton_mdl}.

Одним из критериев удаления неинформативных параметров в вероятностных моделях является отношение вариационной плотности параметров в моде распределения к вариационной плотности параметра в нуле~\cite{nips}:
\[\frac{q_{\w}(w=\mu_q|\tetaw)}{\qw(w=0|\tetaw}= \text{exp}\left(-\frac{2\alpha_q^2}{\mu_q^2}\right),
\]
где $\qw(w|\tetaw) \sim \mathcal{N}(\mu_q, \alpha_q).$

Обобщим понятие относительной вариационной плотности на случай произвольных непрерывных распределений.
\begin{defin}
Относительной вариационной   плотностью параметра $w \in \w$  при условии структуры $\G$ и гиперпараметров $\h$ назовем отношение вариационной плотности в моде вариационного распределения параметра к вариационной плотности в моде априорного распределению параметра:
%\[\rho(w|\G,\tetaw, \h,\Lamb)=\frac{\q_\w\bigl(\text{mode}~\qw}{q_\w\bigl(\text{mode}~\priorw|\boldsymbol{\Gamma},\boldsymbol{\theta}_\mathbf{w}\bigr)}.\]
Относительной вариационной плотностью вектора параметров $\mathbf{w}$ назовем следующее выражение:
\[
    \boldsymbol{\rho}(\mathbf{w}|\boldsymbol{\Gamma}, \boldsymbol{\theta}_\mathbf{w}, \mathbf{h},\boldsymbol{\lambda}) = \prod_{w \in \mathbf{w}}\rho(w|\boldsymbol{\Gamma}, \boldsymbol{\theta}_\mathbf{w}, \mathbf{h},\boldsymbol{\lambda}).
\]

\end{defin}

Сформулируем и докажеми теорему о связи относительной плотности и параметрической сложности модели:

\begin{theorem}
Пусть
\begin{enumerate}
\item заданы компактные множества $U_\mathbf{h} \subset \mathbb{H}, U_{\boldsymbol{\theta}_\mathbf{w}} \times U_{\boldsymbol{\theta}_{\boldsymbol{\Gamma}}} \subset \amsmathbb{\Theta}$;


\item Мода априорного распределения $p(\mathbf{w},\boldsymbol{\Gamma}| \mathbf{h}, \boldsymbol{\lambda})$ не зависит от гиперпараметров $\mathbf{h}$  на $U_\mathbf{h}$ и структуры $\boldsymbol{\Gamma}$ на $U_{\boldsymbol{\theta}_{\boldsymbol{\Gamma}}}$:
\[\text{mode}~p(\mathbf{w}|\boldsymbol{\Gamma}_1, \mathbf{h}_1, \boldsymbol{\lambda})=\text{mode}~ p(\mathbf{w}|\boldsymbol{\Gamma}_2,\mathbf{h}_2, \boldsymbol{\lambda})=\mathbf{M}~\forall~\mathbf{h}_1, \mathbf{h}_2\in U_\mathbf{h},\boldsymbol{\Gamma}_1, \boldsymbol{\Gamma}_2\in U_{\boldsymbol{\theta}_{\boldsymbol{\Gamma}}}.
\]

\item вариационное распределение $q_\mathbf{w}$ и априорное распределение $p(\mathbf{w}|\boldsymbol{\Gamma}, \mathbf{h})$  являются абсолютно непрерывными и унимодальными на  $U_\mathbf{h}, U_{\boldsymbol{\theta}}$.

\item Решение задачи вида 
\begin{equation}
\label{eq:dkl_solve}
\mathbf{h} = \argmin_{\mathbf{h} \in U_\mathbf{h}} \text{D}_\text{KL}\left(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h})\right)
\end{equation} единственно для любого $\boldsymbol{\theta} \in U_{\boldsymbol{theta}}$.
 
\item Параметры модели $\mathbf{w}$ имеют конечные вторые моменты по маргинальным распределениям $q(\mathbf{w}|\boldsymbol{\theta}_{\mathbf{w}}), p(\mathbf{w}|\mathbf{h}, \boldsymbol{\lambda})$:
\[
    \mathsf{E}_q \mathbf{w}^2 = \mathsf{E}_{q_{\boldsymbol{\Gamma}}}\mathsf{E}_{q_\mathbf{w}}\mathbf{w}^2 < \infty;
\]
\[
    \mathsf{E}_{p(\mathbf{w},\boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda})}\mathbf{w}^2 = \mathsf{E}_{p(\boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda})} \mathsf{E}_{p(\mathbf{w}|\boldsymbol{\Gamma}, \mathbf{h}, \boldsymbol{\lambda})}\mathbf{w}^2 < \infty,
\]
где 
\[
    p(\mathbf{w}|\mathbf{h},\boldsymbol{\lambda}) = \int_{\boldsymbol{\Gamma}} p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda}), \quad
        q(\mathbf{w}|\boldsymbol{\theta}_\mathbf{w}) = \int_{\boldsymbol{\Gamma}} q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}).
\]

\item мода и матожидание вариационного распределения $q_\mathbf{w}$ и априорного распределения $p(\mathbf{w}|\boldsymbol{\Gamma},\mathbf{h}, \boldsymbol{\lambda})$  совпадают:
\[
    \text{mode}~p(\mathbf{w}|\boldsymbol{\Gamma},\mathbf{h}, \boldsymbol{\lambda}) = \mathsf{E}_{p(\mathbf{w}| \boldsymbol{\Gamma}, \mathbf{h}, \boldsymbol{\lambda})} \mathbf{w};
\]
\[
  \text{mode}~q(\mathbf{w}|\boldsymbol{\Gamma}, \boldsymbol{\theta}_\mathbf{w}) = \mathsf{E}_{q_\mathbf{w}(\mathbf{w}|\boldsymbol{\Gamma},  \boldsymbol{\theta}_\mathbf{w})} \mathbf{w};
\]

\item задана  бесконечная последовательность векторов вариационных параметров $\boldsymbol{\theta}_1,\boldsymbol{\theta}_2,\dots, \boldsymbol{\theta}_i \in U_{\boldsymbol{\theta}}$, такая что $\lim_{i \to \infty}C_p(\boldsymbol{\theta_i}|U_{\mathbf{h}}, \boldsymbol{\lambda}) = 0.$ 

\end{enumerate}
Тогда следующее выражение стремится к единице:
\[
   \mathsf{E}_{q_{\boldsymbol{\Gamma}}} \boldsymbol{\rho}(\mathbf{w}|\boldsymbol{\Gamma}, \boldsymbol{\theta}_\mathbf{w}, \mathbf{h},\boldsymbol{\lambda})^{-1} \to 1.
\]


\end{theorem}

\begin{proof}
Воспользуемся неравенством Пинскера:
\[
    ||F_q(\boldsymbol{\theta})-F_p(\mathbf{h})||_\text{TV},\leq\sqrt{2\text{D}_\text{KL}\left(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h})\right)},
\]
где $||\cdot||_\text{TV}$ --- расстояние по вариации, $F_q, F_p$ --- функции распределения   $q(\mathbf{w},\boldsymbol{\Gamma}|\boldsymbol{\theta})$ и $p(\mathbf{w},\boldsymbol{\Gamma}| \mathbf{h}, \boldsymbol{\lambda})$.
Т.к. дивергенция состоит из двух неотрицательных величин, то обе они стремятся к нулю.
Рассотрим вторую величину:
\[
   \lim 0 = \text{D}_\text{KL}(q_\mathbf{w}|p(\mathbf{w}) = |\int_{\boldsymbol{\Gamma}} \text{D}_\text{KL}(q_\mathbf{w}|p(\mathbf{w})q(\boldsymbol{\Gamma})| \geq \int_{q} ||F_q - F_p||_\text{TV}.  
\]
Отсюда $ \lim_{i \to \infty} ||F_q(\boldsymbol{\theta})-F_p(\mathbf{h})||_\text{TV} = 0.$
По теореме Шеффе данное выражение можно переписать как:
\[
    \lim \frac{1}{2}\iint_{\mathbf{w},\boldsymbol{\Gamma}} |p(\mathbf{w}| \mathbf{h}, \boldsymbol{\lambda}) -  q(\mathbf{w}|\boldsymbol{\theta})|q(\boldsymbol{\Gamma}) d\boldsymbol{\Gamma}d\mathbf{w} = 0.
\]

Для произвольного $\boldsymbol{\theta}_j$ рассмотрим выражение:
\[
    |\int_{\boldsymbol{\Gamma}} \frac{q_j(\mathsf{E}_q \mathbf{w})}{q(\mathbf{M})} -  \frac{q_j(\mathsf{E}_p \mathbf{w})}{{q(\mathbf{M})}}| \leq \int_{\boldsymbol{\Gamma}} |\frac{q_j(\mathsf{E}_q \mathbf{w})}{{q(\mathbf{M})}} -  \frac{q_j(\mathsf{E}_p \mathbf{w})}{{q(\mathbf{M})}}| \leq \frac{\max{L}}{\min q(\mathbf{M})}\int_{\boldsymbol{\Gamma}}  |\mathsf{E}_q \mathbf{w} - \mathsf{E}_p \mathbf{w}|  \leq
\]
\[
    \leq L\int_{\boldsymbol{\Gamma}} |\mathbf{w}| |q - p|.
\]

%https://math.stackexchange.com/questions/112786/convergence-in-law-and-uniformly-integrability
Т.к. вторые моменты $\mathsf{E}_{q(\mathbf{w}|\boldsymbol{\theta}_\mathbf{w})}\mathbf{w}^2,\mathsf{E}_{p(\mathbf{w}|\mathbf{h})}\mathbf{w}^2$ конечны, то случайная величина $\mathbf{w}$ равномерно интегрируема как при маргинальном распределении $q(\mathbf{w}|\boldsymbol{\theta}_\mathbf{w})$, так и при маргинальном распределении $p(\mathbf{w}|\mathbf{h})$.
Определим случайную величину $\boldsymbol{\nu}(t), t \geq 0$ следующим образом:
\[
    \boldsymbol{\nu}(t) = \boldsymbol{\max}(-t \cdot \mathbf{1}, \boldsymbol{\min}(t \cdot \mathbf{1}, \mathbf{w})).
\]
Данная величина совпадает с $\mathbf{w}$ при $|\mathbf{w}| < t$ и принимает значение $t$ или $-t$ при $|\mathbf{w}| \geq t$,

По определению равномерной интегрируемости для $\mathbf{w}$ для любого числа $\varepsilon$ существует число $t_0$, такое что для любого $t \geq t_0$ справедливо выражение:
\[
    \mathsf{E}|\mathbf{w} - \boldsymbol{\nu}(t)| \leq \varepsilon.
\]
где матожидание берется по  распределениям  $q(\mathbf{w}|\boldsymbol{\theta}_\mathbf{w})$ и $p(\mathbf{w}|\mathbf{h})$.
Тогда 
\[
    \int_{\boldsymbol{\Gamma}} |\mathsf{E}_{q(\mathbf{w}|\boldsymbol{\theta}_\mathbf{w})}\mathbf{w}-\mathsf{E}_{p(\mathbf{w}|\mathbf{h})}\mathbf{w}| \leq     \int_{\boldsymbol{\Gamma}}  \int_{\mathbf{w}} |\mathbf{w} -\boldsymbol{\nu}(t)||p(\mathbf{w}|\mathbf{h})-q(\mathbf{w}|\boldsymbol{\theta}_\mathbf{w})|d\mathbf{w} + 
\]
\[
 +    \int_{\boldsymbol{\Gamma}}  \int_{\mathbf{w}} |\boldsymbol{\nu}(t)||p(\mathbf{w}|\mathbf{h})-q(\mathbf{w}|\boldsymbol{\theta}_\mathbf{w})|d\mathbf{w} \leq
\]
\[
    \leq     \int_{\boldsymbol{\Gamma}}  \int_{\mathbf{w}} |\mathbf{w} -\boldsymbol{\nu}(t)|p(\mathbf{w}|\mathbf{h}) d\mathbf{w} +    \int_{\boldsymbol{\Gamma}}  \int_{\mathbf{w}} |\mathbf{w} -\boldsymbol{\nu}(t)|q(\mathbf{w}|\boldsymbol{\theta}_\mathbf{w})|d\mathbf{w}  +
\]
\[ +      \int_{\boldsymbol{\Gamma}}  \int_{\mathbf{w}} |\boldsymbol{\nu}(t)|p(\mathbf{w}|\mathbf{h}) - q(\mathbf{w}|\boldsymbol{\theta}_\mathbf{w})|d\mathbf{w}.
\]
Обозначим за $\mathbf{h}_i$ --- решение задачи~\eqref{eq:dkl_solve} для вектора $\boldsymbol{\theta}_i$.
Т.к. $|\boldsymbol{\nu}(t)|$ --- ограничена, то 
\[
      \int_{\boldsymbol{\Gamma}}  \int_{\mathbf{w}} |\boldsymbol{\nu}(t)|p(\mathbf{w}|\mathbf{h}_i) - q(\mathbf{w}|\boldsymbol{\theta}_\mathbf{w})|d\mathbf{w} \leq t     \int_{\boldsymbol{\Gamma}}  \int_{\mathbf{w}} |p(\mathbf{w}|\mathbf{h}) - q(\mathbf{w}|\boldsymbol{\theta}_\mathbf{w})|d\mathbf{w} =
\]
\[
= t    \int_{\boldsymbol{\Gamma}}  \int_{\mathbf{w}} |p(\mathbf{w},\boldsymbol{\Gamma}| \mathbf{h}_i, \boldsymbol{\lambda}) -  q(\mathbf{w},\boldsymbol{\Gamma}|\boldsymbol{\theta})|d\boldsymbol{\Gamma}d\mathbf{w} \to_{i \to \infty} 0.
\]
\[
   \lim_{i \to \infty}     \int_{\boldsymbol{\Gamma}}  |\mathsf{E}_{q(\mathbf{w}|\boldsymbol{\theta}_\mathbf{w})}\mathbf{w}-\mathsf{E}_{p(\mathbf{w}|\mathbf{h}_i)}\mathbf{w}| \leq      \int_{\boldsymbol{\Gamma}}  \int_{\mathbf{w}} |\mathbf{w} -\boldsymbol{\nu}(t)||p(\mathbf{w}|\mathbf{h}_i)-q(\mathbf{w}|\boldsymbol{\theta}_\mathbf{w})|d\mathbf{w} 
\]
для любого $t$. Устремляя $t$ к бесконечности, получим $ \lim_{i \to \infty}     \int_{\boldsymbol{\Gamma}}  |\mathsf{E}_{q(\mathbf{w}|\boldsymbol{\theta}_\mathbf{w})}\mathbf{w}-\mathsf{E}_{p(\mathbf{w}|\mathbf{h}_i)}\mathbf{w}|  = 0.$
Таким образом, мода $q(\mathbf{w}|\boldsymbol{\Gamma}, \boldsymbol{\theta}_\mathbf{w})$  стремится в среднем к моде априорного распределения $\mathbf{M}$.

\end{proof}

Теорема утверждает, что при устремлении параметрической сложности модели к нулю, все параметры модели подлежат удалению в среднем по всем возможным значениям  структуры $\boldsymbol{\Gamma}$ модели. Заметим, что теорема применима для случая, когда последовательность вариационных распределений $q$ не имеет предела. Так, в случае, если структура $\boldsymbol{\Gamma}$ определена однозначно, последовательность $q_i$ может являться последовательностью нормальных распределений, чье матожидание стремится к нулю:
\[
    q_i \sim \mathcal{N}((\boldsymbol{\mu}_q)_i, (\mathbf{A}^{-1}_q)_i), (\boldsymbol{\mu}_q)_i \to \mathbf{0}.
\]
Априорным распределением $p(\mathbf{w},\boldsymbol{\Gamma}|\mathbf{h},\boldsymbol{\lambda}) = p(\mathbf{w}|\mathbf{h},\boldsymbol{\lambda})$ при этом может являться семейство нормальных распределений с нулевым средним:
\[
    p(\mathbf{w}|\mathbf{h},\boldsymbol{\lambda}) = \mathcal{N}(\mathbf{0}, \mathbf{A}^{-1}).
\]
При этом сама последовательность распределений $q_i$ не обязана иметь предел.

\section{Обобщающая задача}
В данном разделе проводится анализ основных критериев выбора моделей, а также предлагается их обобщение на случай моделей, испольюзующих вариационное распределение $q$ для аппроксимации неизвестного апостериорного распределения параметров $p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{y}, \mathbf{X}, \mathbf{h}, \boldsymbol{\lambda})$.

Рассмотрим основные статистические критерии выбора вероятностных моделей. 
\begin{enumerate}
\item Критерий максимального правдоподобия:
\[\text{log}p(\mathbf{y}|\mathbf{X},\mathbf{w}, \boldsymbol{\Gamma})\to \max_{\mathbf{w}\in \mathbb{W}, \boldsymbol{\Gamma}\in \amsmathbb{\Gamma}}.\]
Для использования данного критерия в качестве задачи выбора модели предлагается следующее обобщение:
\begin{equation}
\label{eq:optim_ml}
    L =  \mathsf{E}_q \text{log}~p(\mathbf{y}|\mathbf{X}, \mathbf{w}, \boldsymbol{\Gamma}).
\end{equation}
Данное обобщение~\eqref{eq:optim_ml} эквивалентно  критерию правдоподобия при выборе в качестве $q$ эмпирического распределения парамтетров и структуры.
Метод не предполагает оптимизации гиперпараметров $\mathbf{h}$. Для формального соответствия данной задачи задаче выбора модели~\eqref{TOD}, т.е. двухуровневой задачи оптимизации, положим $L=Q:$
\[
    L =  \mathsf{E}_q \text{log}~p(\mathbf{y}|\mathbf{X}, \mathbf{w}, \boldsymbol{\Gamma}) \to \max_{\boldsymbol{\theta}},
\]
\[
    Q =  \mathsf{E}_q \text{log}~p(\mathbf{y}|\mathbf{X}, \mathbf{w}, \boldsymbol{\Gamma}).
\]



\item Метод максимальной апостериорной вероятности. 
\[\text{log}~p(\mathbf{y},\mathbf{w},\boldsymbol{\Gamma}|\mathbf{X}, \boldsymbol{\lambda} )\to \max_{\mathbf{w} \in \mathbb{W}, \boldsymbol{\Gamma}\in \amsmathbb{\Gamma}}.\]
Аналогично предыдущему методу сформулируем вариационное обобщение данной задачи:
\begin{equation}
\label{eq:optim_map}
L=Q =\mathsf{E}_q\bigl(\text{log}~p(\mathbf{y}|\mathbf{X}, \mathbf{w},\boldsymbol{\Gamma})+\text{log}p(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\lambda})\bigr).
\end{equation}
Т.к. в рамках данной задачи~\eqref{eq:optim_map} не предполагается оптимизации гиперпараметров $\mathbf{h}$, положим параметры распределения $p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda})$ фиксированными:
\[
    \boldsymbol{\lambda} = [\lambda_1, \lambda_2, \lambda_{\text{temp}}, \mathbf{s}, \text{diag}(\mathbf{A})].
\]

\item Перебор структуры:
\begin{equation}
\label{eq:optim_struct}
    L =  Q = \mathsf{E}_q\text{log}p(\mathbf{y}, \mathbf{w}|\mathbf{X})[q_{\boldsymbol{\Gamma}} = p']
\end{equation}
где $p'$ --- некоторое распределение на структуре $\boldsymbol{\Gamma}$, выступающее в качестве метапараметра.




\item Критерий Акаике:
\[
   \text{AIC} =  \text{log}p(\mathbf{y}|\mathbf{X}, \mathbf{w}, \boldsymbol{\Gamma})-|\mathbb{W}|.
\]
Т.к. все рассматриваемые модели принадлежат одному параметрическому семейству моделей $\mathfrak{F}$, то количество параметров у всех рассматриваемых моделей  совпадает . Тогда критерий Акаике совпадает с критерием максимального правдоподобия. Для использования критерия Акаике для сравнения моделей, принадлежащих одному параметрическому семейству~$\mathfrak{F}$ предлагается следующая переформулировка:
\begin{equation}
\label{eq:optim_aic}
    L = Q = \text{log}p(\mathbf{y}|\mathbf{X}, \mathbf{w}, \boldsymbol{\Gamma})
-|\{w: D_\text{KL}(\theta, \mathbf{h})<\lambda_{\text{prune}}\}|,
\end{equation}
где 
\begin{equation}\label{eq:aic_compl}\mathbf{h}=\argmin_{\mathbf{h}'\in U_\mathbf{h}}D_\text{KL}(q(\mathbf{w},\boldsymbol{\Gamma})| p(\mathbf{w},\boldsymbol{\Gamma}|\mathbf{h}',\boldsymbol{\lambda}),\end{equation} $\lambda_{\text{prune}}$ --- метапараметр алгоритма, $U_\mathbf{h}  \subset \mathbb{H}$ --- область определения задачи по гиперпараметрам. Предложенное обобщение~\eqref{eq:optim_aic} применимо только в случае, если выражение~\eqref{eq:aic_compl} определено однозначно, т.е. существует единственный вектор гиперпараметров на $U_\mathbf{h},$ доставляющий минимум дивергенции $D_\text{KL}(q(\mathbf{w},\boldsymbol{\Gamma})| p(\mathbf{w},\boldsymbol{\Gamma}|\mathbf{h},\boldsymbol{\lambda}).$

\item Информационный критерий Шварца:
\[
    \text{BIC} = \text{log}p(\mathbf{y}|\mathbf{X}, \mathbf{w})-0.5\text{log}(m)|\mathbb{W}|.
\]
Переформулируем данный критерий аналогично критерию AIC:
\begin{equation}
\label{eq:optim_bic}
    L = Q = BIC_{\lambda} = \text{log}p(\mathbf{y}|\mathbf{X}, \mathbf{w}) - \text{log}(m)|\{w: D_\text{KL}(\theta, \mathbf{h})<\lambda_{\text{prune}}\}|,
\end{equation}
метапараметр $\lambda_{\text{prune}}$ определен аналогично~\eqref{eq:aic_compl}.

\item Метод вариационной оценки обоснованности:
\begin{equation}
\label{eq:optim_elbo_method}   
    L = \mathsf{E}_q \text{log}p(\mathbf{y}|\mathbf{X}, \mathbf{w}, \boldsymbol{\Gamma})-D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\boldsymbol{\Gamma}, \mathbf{w}|\mathbf{h},\boldsymbol{\lambda}))  + \text{log}~p(\mathbf{h}|\boldsymbol{\lambda}) \to \max_{\boldsymbol{\theta}},
\end{equation}
\[
    Q = \mathsf{E}_q \text{log}p(\mathbf{y}|\mathbf{X}, \mathbf{w}, \boldsymbol{\Gamma})-D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\boldsymbol{\Gamma}, \mathbf{w}|\mathbf{h},\boldsymbol{\lambda}))  + \text{log}~p(\mathbf{h}|\boldsymbol{\lambda}) \to \max_{\mathbf{h}},
\]
В рамках данной задачи функции $L$ и $Q$ совпадают, все гиперпараметры $\mathbf{h}$ подлежат оптимизации.

\item Валидация на отложенной выборке:
\begin{equation}
\label{eq:optim_hold_out}
    L = \mathsf{E}_q \text{log}p(\mathbf{y}_\text{train}, \mathbf{w}, \boldsymbol{\Gamma}|\mathbf{X}_\text{train}, \mathbf{h}, \boldsymbol{\lambda}) \to \max_{\boldsymbol{\theta}},
\end{equation}
\[
    Q = \mathsf{E}_q \text{log}p(\mathbf{y}_\text{test}|\mathbf{X}_\text{test}, \mathbf{w}, \boldsymbol{\Gamma}) \to \max_{\mathbf{h}},
\]
где $(\mathbf{X}_\text{train}, \mathbf{y}_\text{train}), (\mathbf{X}_\text{test}, \mathbf{y}_\text{test})$ --- разбиение выборки на обучающую и контрольную подвыборку.
В рамках данной задачи, все гиперпараметры $\mathbf{h}$ подлежат оптимизации.

\end{enumerate}

Каждый из рассмотренных критерии удовлетворяет хотя бы одному из перечисленных свойств:
\begin{enumerate}[label={\arabic*)}]
\item модель, оптимизируемая согласно критерию, доставляет максимум правдоподобия выборки;
\item модель, оптимизируемая согласно критерию, доставляет максимум оценки обоснованности;
\item для моделей, доставляющих сопоставимые значения правдоподобия выборки, выбирается модель с меньшим количеством информативных параметров.
\item критерий позволяет производить перебор структур для отбора наилучших модели.
\end{enumerate}

Формализуем рассмотренные критерии. Оптимизационную задачу, которая удовлетворяет всем перечисленным свойствам при некоторых значинях метапараметров, будет называть \textit{обобщающей}.

\begin{defin}
Двухуровневую задачу оптимизации будем называть \textit{обобщающей} на компакте $U = U_{\boldsymbol{\theta}} \times U_{\mathbf{h}} \times U_{\boldsymbol{\lambda}} \subset \amsmathbb{\Theta} \times \mathbb{H} \times \amsmathbb{\Lambda}$, если она удовлетворяет следующим критериям.
\begin{enumerate}
\item Область определения каждого параметра $w \in \mathbf{w}$, гиперпараметра $h \in \mathbf{h}$ и метапараметра $\lambda \in \boldsymbol{\lambda}$ не  является пустым множеством и не является точкой.
\item Для каждого значения гиперпараметров $\mathbf{h}$ оптимальное решение нижней~\eqref{TODO} задачи оптимизации 
\[
\boldsymbol{\theta}^{*}(\mathbf{h}) = \argmax_{\boldsymbol{\theta} \in \amsmathbb{\Theta}} L(\boldsymbol{\theta}, \mathbf{h})
\]
определено однозначно при любых значениях метапараметров $\boldsymbol{\lambda} \in U_{\lambda}$.

\item Критерий максимизации правдоподобия выборки: существует $\boldsymbol{\lambda} \in U_{\lambda}$ и  $$K_1>0, \quad K_1 < \max_{\mathbf{h}_1, \mathbf{h}_2 \in U_\mathbf{h}} Q(\mathbf{h}_1) - Q(\mathbf{h}_2),$$ такие что для любых векторов гиперпараметров, удовлетворяющих неравенству $$\mathbf{h}_1, \mathbf{h}_2 \in U_{h}, Q(\mathbf{h}_1)-Q(\mathbf{h}_2) > K_1,$$ выполняется неравенство $$\mathsf{E}_q \text{log}~p(\mathbf{y}|\mathbf{X}, \boldsymbol{\theta}^{*}(\mathbf{h}_1), \boldsymbol{\lambda}, \mathbf{f})>\mathsf{E}_q \text{log}~p(\mathbf{y}|\mathbf{X}, \boldsymbol{\theta}^{*}(\mathbf{h}_2), \boldsymbol{\lambda}, \mathbf{f}).$$

\item Критерий минимизации параметрической сложности:  существует  $\boldsymbol{\lambda} \in U_{\lambda}$ и $$K_2>0, \quad K_2 < \max_{\mathbf{h}_1, \mathbf{h}_2 \in U_\mathbf{h}} Q(\mathbf{h}_1) - Q(\mathbf{h}_2),$$  такие что для любых векторов гиперпараметров $\mathbf{h}_1, \mathbf{h}_2 \in U_\mathbf{h}$, удовлетворяющих неравенству $$Q(\mathbf{h}_1)-Q(\mathbf{h}_2) > K_2,$$ параметрическая сложность первой модели меньше, чем второй: $$C_p(\boldsymbol{\theta}^{*}(\mathbf{h}_1)|U_\mathbf{h},\boldsymbol{\lambda})<C_p(\boldsymbol{\theta}^{*}(\mathbf{h}_2)|U_\mathbf{h},\boldsymbol{\lambda}).$$

\item Критерий приближения оценки обоснованности: существует значение гиперпараметров $\boldsymbol{\lambda}$, такое что значение функций потерь $L$ и валидации $Q$ пропорционален вариационной оценки обоснованности модели: $$Q \propto  L \propto 
\mathsf{E}_q \text{log}~p(\mathbf{y}|\mathbf{w}, \mathbf{X})-{D}_{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda}) + \text{log}~p(\mathbf{h}|\boldsymbol{\lambda})).
$$ для всех $\boldsymbol{\theta}\in U_{\boldsymbol{\theta}}, \mathbf{h} \in U_{\mathbf{h}}.$ 

\item Критерий перебора оптимальных структур: существует набор метапараметров $\boldsymbol{\lambda}$ и константа $$K_3>0, \quad K_3 < \max_{\mathbf{h}_1, \mathbf{h}_2} D_\text{KL}(p (\boldsymbol{\Gamma}|\mathbf{h}_1, \boldsymbol{\lambda}) | p (\boldsymbol{\Gamma}|\mathbf{h}_2, \boldsymbol{\lambda})),$$ такие что для локальных оптимумов задачи оптимизации $\mathbf{h}_{1}, \mathbf{h}_2$, полученных при метапараметрах $\boldsymbol{\lambda}$ и удовлетворяющих неравенствам $$D_\text{KL}(p (\boldsymbol{\Gamma}|\mathbf{h}_1, \boldsymbol{\lambda})| p (\boldsymbol{\Gamma}|\mathbf{h}_2, \boldsymbol{\lambda})) > K_3, p(\boldsymbol{\Gamma}|\mathbf{h}_2, \boldsymbol{\lambda})| p(\boldsymbol{\Gamma}|\mathbf{h}_1, \boldsymbol{\lambda})) > K_3,$$ $$Q(\mathbf{h}_1) > Q(\mathbf{h}_2),$$  существует значение метапараметров $\boldsymbol{\lambda}'$, такие что
\begin{enumerate}
\item Соответствие между вариационными параметрами $\boldsymbol{\theta}^{*}(\mathbf{h}_1), \boldsymbol{\theta}^{*}(\mathbf{h}_2)$ сохраняется при  $\boldsymbol{\lambda}'$.
\item  $Q(\mathbf{h}_1) < Q(\mathbf{h}_2)$ при $\boldsymbol{\lambda}'$.
\end{enumerate}


\item Критерий нерперывности: функции $L$ и $Q$ непрерывны по метапараметрам $\boldsymbol{\lambda} \in U_{\boldsymbol{\lambda}}$.
\end{enumerate}
\end{defin}
Первый критерий является техническим и используется для исключения из рассмотрения вырожденных задач оптимизации.  
Второй критерий говорит о том, что решение первого и второго уровня должны быть согласованы и определены однозначно.
Критерии 3-5 определяют возможные критерии оптимизации, которые должны приближаться обобщающей задачей.
Критерий 6 говорит о возможности перехода между различными структурами модели. Данный критерий говорит о том, что мы можем перейти от одного набора гиперпараметров $\mathbf{h}_1$ к другим $\mathbf{h}_2$, если они соответствуют локальным оптимумам задачи оптимизации, и дивергенция соответствующих априорных  распределений на структурах $p(\boldsymbol{\Gamma}|\mathbf{s}, \boldsymbol{\lambda})$ значимо высока. При этом соответствующие вариационные распределения $q(\boldsymbol{\Gamma}|\boldsymbol{\theta}_{\boldsymbol{\Gamma}})$ могут оказаться достаточно близки. Возможным дополнением этого критерия был бы критерий, позволяющий переходить от структуры к структуре, если соответствующие распределения $q(\boldsymbol{\Gamma}|\boldsymbol{\theta}_{\boldsymbol{\Gamma}})$ различаются значимо.
Последний критерий говорит о том, что обобщающая задача должна позволять производить переход между различными методами выбора  параметров и структуры модели непрерывно.

\begin{theorem}Рассмотренные задачи~\eqref{eq:optim_ml},\eqref{eq:optim_map},\eqref{eq:optim_struct},\eqref{eq:optim_aic},\eqref{eq:optim_bic},\eqref{eq:optim_hold_out} не являются обобщающими.
\end{theorem}
\begin{proof}
Задачи~\eqref{eq:optim_ml},\eqref{eq:optim_map},\eqref{eq:optim_struct},\eqref{eq:optim_aic},\eqref{eq:optim_bic} не имеют гиперпараметров $\mathbf{h}$, подлежащих оптимизации, поэтому не могут оптимизировать вариационную оценку.

При  использовании валидации на отложенной выборки~\eqref{eq:optim_hold_out} в функцию валидации $Q$ не входит ни один метапараметр, поэтому критерий перебора структур 6 для нее также не выполняется. 

\end{proof}

\begin{theorem}
Пусть $q_{\boldsymbol{\Gamma}}$ --- абсолютно непрерывное распределение с дифференцируемой плотностью, такой что:
\begin{enumerate}
\item градиент плотности $\nabla_{\boldsymbol{\theta}_{\boldsymbol{\Gamma}}} q(\boldsymbol{\Gamma}|\boldsymbol{\theta}_{\boldsymbol{\Gamma}})$ является нулевым не более чем счетное количество раз. 
\item выражение $\nabla_{\boldsymbol{\theta}_{\boldsymbol{\Gamma}}} q(\boldsymbol{\Gamma}|\boldsymbol{\theta}_{\boldsymbol{\Gamma}}) \text{log}p(\boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda})$ ограничено на $U_{\boldsymbol{\theta}}$ некоторой случайной величиной с конечным первым моментом.
\end{enumerate}
Тогда задача~\eqref{eq:optim_elbo_method} не является обобщающей.
\end{theorem}
\begin{proof}
Пусть выполнены условия критерия 6 о переборе структур, и $\mathbf{h}_1, \mathbf{h}_2$ --- локальные оптимумы функции $Q$ при метапараметрах $\boldsymbol{\lambda}$.
По условию критерия соответствтие $\boldsymbol{\theta}^{*}(\mathbf{h}_1)$ и $\boldsymbol{\theta}^{*}(\mathbf{h}_2)$ должны сохраняться, т.е. для некоторого $\boldsymbol{\lambda}'$ решение  нижней задачи оптимизации $\boldsymbol{\theta}^{*}(\mathbf{h}_1)$ должно совпадать с решением $\boldsymbol{\theta}^{*}(\mathbf{h}_1)$ при метапараметрах $\boldsymbol{\lambda}$. Тогда
\[
    \nabla_{\boldsymbol{\theta}} \mathsf{E}_{q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}_1)} \text{log}~p(\mathbf{y}|\mathbf{X}, \mathbf{w}, \boldsymbol{\Gamma}) -\nabla_{\boldsymbol{\theta}}  \text{D}_{\text{KL}}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}_1) | p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}_1, \boldsymbol{\lambda})) = 
\]
\[
= \nabla_{\boldsymbol{\theta}} \mathsf{E}_{q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}_1)} \text{log}~p(\mathbf{y}|\mathbf{X}, \mathbf{w}, \boldsymbol{\Gamma}) - \nabla_{\boldsymbol{\theta}}  \text{D}_{\text{KL}}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}_1) | p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}_1, \boldsymbol{\lambda}')).
\]
Сокращая равные слагаемые в равенстве получим:
\[
\nabla_{\boldsymbol{\theta}}  \text{D}_{\text{KL}}(q(\boldsymbol{\Gamma}|\boldsymbol{\theta}_2) | p(\boldsymbol{\Gamma}| \boldsymbol{\lambda})) = \nabla_{\boldsymbol{\theta}} \text{D}_{\text{KL}}(q(\boldsymbol{\Gamma}|\boldsymbol{\theta}_2) | p(\boldsymbol{\Gamma}| \boldsymbol{\lambda}')),
\] 
Из второго условия теоремы следует, что по теореме Лебега о мажорируемой сходимости, осуществим переход дифференцирования под знак интеграла:
\[
\int_{\boldsymbol{\Gamma} \in \amsmathbb{\Gamma}} \nabla_{\boldsymbol{\theta}_\Gamma} q(\boldsymbol{\Gamma}|\boldsymbol{\theta}_2) (\text{log}~p(\boldsymbol{\Gamma}| \boldsymbol{\lambda}) - \text{log}~p(\boldsymbol{\Gamma}| \boldsymbol{\lambda}')) d\boldsymbol{\Gamma} = 0.
\]
Т.к. выражение $ \nabla_{\boldsymbol{\theta}_\Gamma} q(\boldsymbol{\Gamma}|\boldsymbol{\theta}_2)$ принимает нулевое значение в счетном количестве точек, то выражение $\text{log}~p(\boldsymbol{\Gamma}| \boldsymbol{\lambda}) - \text{log}~p(\boldsymbol{\Gamma}| \boldsymbol{\lambda}')$ равно нулю почти всюду, что означает что метапараметр температуры $\lambda_\text{temp}$  равен:
\[
\lambda_\text{temp} = \lambda_\text{temp}',\quad \lambda_\text{temp} \in \boldsymbol{\lambda}, \lambda_\text{temp}' \in \boldsymbol{\lambda}'.
\]
Таким образом, метапараметры $\boldsymbol{\lambda},\boldsymbol{\lambda}'$ отличаются лишь на метапараметры  $\lambda_1, \lambda_2$ регуляризации ковариационной матрицы~$\mathbf{A}^{-1}$. 
Возьмем в качестве векторов гиперпараметров $\mathbf{h}_1,\mathbf{h}_2$ гиперпараметры, отличающиеся только параметрами распределения структуры:
\[
    \mathbf{h}_1 = [\mathbf{s}_1, \text{diag}(\mathbf{A}_1)], \mathbf{h}_2 = [\mathbf{s}_2, \text{diag}(\mathbf{A}_2)],\quad \mathbf{s}_1 \neq \mathbf{s}_2, \mathbf{A}_1 = \mathbf{A}_2.
\]
Метапараметры $\lambda_1, \lambda_2$ не влияют на значение функции $Q$ при гиперпараметрах, отличающихся только параметрами распределения структуры, поэтому значение функции $Q$ для них будет неизменно при любых значениях $\lambda_1, \lambda_2$. Приходим к противоречию: значение $Q$ не меняется при изменении метапараметров $\boldsymbol{\lambda}$.

\end{proof}

В качестве обобщающей задачи оптимизации предлагается оптимизационную задачу следующего вида:
\begin{equation}
\label{eq:qopt}
\mathbf{h}^{*} = \argmax_{\mathbf{h}} Q = 
\end{equation}
\[
= {\lambda_\text{likelihood}^\text{Q}\mathsf{E}_{{q}^{*}} \text{log}~{p(\mathbf{y} | \mathbf{X}, \mathbf{w},\boldsymbol{\Gamma}, \mathbf{h}, \lambda_\text{temp}, \mathbf{f})}}
 -\]
\vspace{-0.3cm}
\[- {\lambda^\text{prior}_\text{Q}\text{D}_{KL}\bigl( q^{*}(\mathbf{w}, \boldsymbol{\Gamma}) || p(\mathbf{w}, \boldsymbol{\Gamma} |\mathbf{h}, \lambda_{\text{temp}},\mathbf{f}) \bigr)}  -\]
\vspace{-0.3cm}
\[
-{\sum_{p' \in \mathbf{P}, \lambda \in \boldsymbol{\lambda}^\text{struct}_\text{Q}} \lambda\text{D}_{KL}(\boldsymbol{\Gamma} | p')+\text{log}p(\mathbf{h}|\mathbf{f})}, 
\]
\begin{equation}
\tag{$L^{*}$}
{q}^{*} = \argmax_{q} L = 
{\mathsf{E}_q \text{log}~{p(\mathbf{y} | \mathbf{X}, \mathbf{w}, \boldsymbol{\Gamma}, \mathbf{h}, \lambda_{\text{temp}}, \mathbf{f})}}
\end{equation}
\vspace{-0.3cm}
\[- {\lambda^\text{prior}_\text{L}\text{D}_{KL}\bigl( q^{*}(\mathbf{w}, \boldsymbol{\Gamma}) || p(\mathbf{w}, \boldsymbol{\Gamma} |\mathbf{h}, \lambda_{\text{temp}},\mathbf{f}) \bigr)},
\]
где $\mathbf{P}$ --- непустое множество распределений на структуре $\boldsymbol{\Gamma}$, $\lambda^\text{prior}_\text{Q}, \lambda_\text{likelihood}^\text{Q}, \boldsymbol{\lambda}^\text{struct}_\text{Q}$ --- некоторые числа. Множество распределений $\mathbf{P}$ отвечает за перебор структур $\boldsymbol{\Gamma}$ в процессе оптимизации модели. Подробное объяснение данного множества дано ниже. 


\begin{theorem}
Пусть:
\begin{enumerate}[label={\arabic*)}] 
\item задано непустое множество непрерывных по параметрам распределений на структуре $\mathbf{P}$, где хотя бы одно распределение принадлежит Gumbel-Softmax-распределению.

\item вариационное распределение $q = q_{\boldsymbol{\Gamma}}(\boldsymbol{\Gamma}|\boldsymbol{\theta}_{\boldsymbol{\Gamma}}) q_{\mathbf{w}}(\mathbf{w}| \boldsymbol{\Gamma},\boldsymbol{\theta}_{\boldsymbol{\Gamma}})$ является  абсолютно непрерывным, плотность которого непрерывна по метапараметрам $\boldsymbol{\lambda}$;

\item задан компакт  $U = U_{\boldsymbol{\theta}} \times U_{\mathbf{h}} \times U_{\boldsymbol{\lambda}} \subset \amsmathbb{\Theta} \times \mathbb{H} \times \amsmathbb{\Lambda}$, где параметры распределений $\mathbf{P} \in \amsmathbb{\Lambda}$, область $U_{\boldsymbol{\theta}}$ декомпозируется на две области $U_{\boldsymbol{\theta}} = U_{\boldsymbol{\theta}_{\mathbf{w}}} \times U_{\boldsymbol{\theta}_{\boldsymbol{\Gamma}}}$;

\item область определения каждого параметра $w \in \mathbf{w}$, гиперпараметра $h \in \mathbf{h}$ и метапараметра $\lambda \in \boldsymbol{\lambda}$ не является является пустым и не является точкой;

\item для каждого значения гиперпараметров $\mathbf{h}$ оптимальное решение нижней задачи оптимизации $\boldsymbol{\theta}^{*}$ определено однозначно при любых значениях метапараметров $\boldsymbol{\lambda} \in U_{\lambda}$;

\item область значений метапараметров $\lambda_\text{likelihood}^\text{Q}, \lambda^\text{prior}_\text{Q}, \boldsymbol{\lambda}^\text{struct}_\text{Q}, \lambda^\text{prior}_\text{L}$ включает отрезок от нуля до единицы;

\item существует значение метапараметров $\lambda_1, \lambda_2, \lambda_\text{likelihood}^\text{Q}$, такое что
\[
\max_{\mathbf{h}} \text{log} p (\mathbf{h}|\boldsymbol{\lambda})-\min_{\mathbf{h}} \text{log}~p(\mathbf{h}|\boldsymbol{\lambda}) < \max_{\mathbf{h}} Q(\mathbf{h}) -  \min_{\mathbf{h}} Q(\mathbf{h}).
\] 
при $\boldsymbol{\lambda}^\text{struct}_Q = \mathbf{0}, \lambda^\text{prior}_Q = 0$.

\item существует значение метапараметров $ \lambda^\text{prior}_\text{Q}, \lambda_1, \lambda_2, \lambda_\text{temp}$, такое что 
\[
    \max_{\mathbf{h}} \text{log}~p (\mathbf{h}|\boldsymbol{\lambda}) - \min_{\mathbf{h}} \text{log}~p (\mathbf{h}|\boldsymbol{\lambda})  + \max_{\mathbf{h}} \min_{\boldsymbol{\theta}} D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda})) -
\]
\[ \min_{\mathbf{h}, \boldsymbol{\theta}} D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda})) + \max_{\boldsymbol{\theta}}\frac{1}{\lambda^\text{prior}_\text{L}}\mathsf{E}_q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}) \text{log} p(\mathbf{y}|\mathbf{X}, \mathbf{w})  - 
\]
\[
 - \min_{\boldsymbol{\theta}}\frac{1}{\lambda^\text{prior}_\text{L}}\mathsf{E}_q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}) \text{log} p(\mathbf{y}|\mathbf{X}, \mathbf{w}) < \max_{\boldsymbol{\theta}, \mathbf{h}} D_{\text{KL}} - \min_{\boldsymbol{\theta}, \mathbf{h}} D_{\text{KL}}.
\]

\item существуют значения метапараметров $\lambda^\text{prior}_\text{Q},\lambda^\text{likelihood}_\text{Q}, \lambda_1, \lambda_2, \lambda_\text{temp},$ такие что 
\[
\max_{\mathbf{h}} D_{\text{KL}} - \min_{\mathbf{h}} D_{\text{KL}} < \frac{\max_{\mathbf{h}} Q - \min_{\mathbf{h}} Q }{\max_{\lambda_\text{struct}}} 
\]
при $\boldsymbol{\lambda}^\text{struct}_\text{Q} = 0.$

\end{enumerate}
Тогда задача~\eqref{eq:qopt} является обобщающей на $U$.
\end{theorem}
\begin{proof}
Для доказательста теоремы требуется доказать критерии 1-7 из определения обобщающей задачи.
Выполнение критериев 1 и 2 следует из условий задачи.

Докажем критерий 3. 
Пусть $\lambda^{\text{prior}}_\text{Q} = 0, \boldsymbol{\lambda}^\text{struct}_\text{Q} = \mathbf{0}$. 
Пусть $\lambda_1, \lambda_2, \lambda_\text{likelihood}^\text{Q}$ удовлетворяют седьмому условияю теоремы.
Возьмем в качестве $K_1$ следующее выражение:
\[
    K_1= \max_{\mathbf{h}} \text{log} p (\mathbf{h}|\boldsymbol{\lambda})-\min_{\mathbf{h}} \text{log}~p(\mathbf{h}|\boldsymbol{\lambda}).
\]
Пусть $\mathbf{h}_1, \mathbf{h}_2 \in U_\mathbf{h}, Q(\mathbf{h}_1)-Q(\mathbf{h}_2)>K_1$.
Тогда 
\[
Q(\mathbf{h}_1)-Q(\mathbf{h}_2) = \lambda^\text{likelihood}_\text{Q} \mathsf{E}_{q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}_1)} \text{log}~p(\mathbf{y}|\mathbf{X}, \mathbf{w}, \boldsymbol{\Gamma})-\]
\[
-\lambda^\text{likelihood}_\text{Q}  \mathsf{E}_{q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}_2)} \text{log}~p(\mathbf{y}|\mathbf{X}, \mathbf{w}, \boldsymbol{\Gamma})
+\text{log}~p(\mathbf{h}_2|\boldsymbol{\lambda})-\text{log}~p(\mathbf{h}_1|\boldsymbol{\lambda})>K_1.
\]
Отсюда следует  выполнение критерия 3:
\[
\lambda^\text{likelihood}_\text{Q} \mathsf{E}_{q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}_1)} \text{log}~p(\mathbf{y}|\mathbf{X}, \mathbf{w}, \boldsymbol{\Gamma}) - \lambda^\text{likelihood}_\text{Q} \mathsf{E}_{q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}_2)} \text{log}~p(\mathbf{y}|\mathbf{X}, \mathbf{w}, \boldsymbol{\Gamma}) > 0.
\]

Докажем критерий 4. 
Пусть $\boldsymbol{\lambda}$ удовлетворяют восьмому условию и $\lambda^{\text{likelihood}}_\text{Q} = 0, \boldsymbol{\lambda}^\text{struct}_\text{Q} = \mathbf{0}$.
Пусть 
\[
K_2 =  \max_{\mathbf{h}} \text{log}~p (\mathbf{h}|\boldsymbol{\lambda}) - \min_{\mathbf{h}} \text{log}~p (\mathbf{h}|\boldsymbol{\lambda})  + \max_{\mathbf{h}} \min_{\boldsymbol{\theta}} D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda})) -
\]
\[ \min_{\mathbf{h}, \boldsymbol{\theta}} D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda})) + \max_{\boldsymbol{\theta}}\frac{1}{\lambda^\text{prior}_\text{L}}\mathsf{E}_q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}) \text{log} p(\mathbf{y}|\mathbf{X}, \mathbf{w})  - 
\]
\[
 - \min_{\boldsymbol{\theta}}\frac{1}{\lambda^\text{prior}_\text{L}}\mathsf{E}_q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}) \text{log} p(\mathbf{y}|\mathbf{X}, \mathbf{w}).
\]
Пусть $\mathbf{h}_1, \mathbf{h}_2 \in U_\mathbf{h}, Q(\mathbf{h}_1)-Q(\mathbf{h}_2)>K_1$.
Рассмотрим разность параметрических сложностей двух векторов:
\[
C_p(\boldsymbol{\theta}_2)-C_p(\boldsymbol{\theta}_1) = \min_{\mathbf{h}} D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}_2)|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda})) - 
\]
\[
-\min_{\mathbf{h}} D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}_1)|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda})) \geq
\]
% добавляем D_kl вместо D_KL^*
\[
\geq
\min_{\mathbf{h}} D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}_2)|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda}))- D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}_1)|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}_1, \boldsymbol{\lambda})) +
\]
\[
+D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}_2)|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}_2, \boldsymbol{\lambda})) - D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}_2)|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}_2, \boldsymbol{\lambda})) =
\]
% Доводим до Q
\[
= Q(\mathbf{h}_1)-Q(\mathbf{h}_2)-\text{log}~p (\mathbf{h}_1|\boldsymbol{\lambda})+\text{log}~p (\mathbf{h}_2|\boldsymbol{\lambda})+
\]
\[+\min_{\mathbf{h}} D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}_2)|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda})) -  D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}_1)|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}_1, \boldsymbol{\lambda})) >
\]
% Доводим до K_2
\[
>K_2 -\text{log}~p (\mathbf{h}_1|\boldsymbol{\lambda})+\text{log}~p (\mathbf{h}_2|\boldsymbol{\lambda}) + \min_{\boldsymbol{\theta}, \mathbf{h}} D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda})) 
\]
\[
-  D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}_1)|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}_1, \boldsymbol{\lambda})).
\]
Рассмотрим разность:
\[\min_{\boldsymbol{\theta}, \mathbf{h}} D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda}))  - D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}_1)|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}_1, \boldsymbol{\lambda})) =
\]
\[
= \min_{\boldsymbol{\theta}, \mathbf{h}} D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda}))  - \frac{1}{\lambda^\text{prior}_\text{L}}\mathsf{E}_q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}_1) \text{log} p(\mathbf{y}|\mathbf{X}, \mathbf{w}) +
\]
\[
+ \max_{\boldsymbol{\theta}}  (\frac{1}{\lambda^\text{prior}_\text{L}}\mathsf{E}_{q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})} \text{log} p(\mathbf{y}|\mathbf{X}, \mathbf{w}) - D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}_1, \boldsymbol{\lambda}))) \geq
\]
% взяли максимум от правдоподобия
\[
    \geq \min_{\boldsymbol{\theta}, \mathbf{h}} D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda})) - \max_{\boldsymbol{\theta}}\frac{1}{\lambda^\text{prior}_\text{L}}\mathsf{E}_q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}) \text{log} p(\mathbf{y}|\mathbf{X}, \mathbf{w}) +
\]
\[ + \max_{\boldsymbol{\theta}}  (\min_{\boldsymbol{\theta}'} \frac{1}{\lambda^\text{prior}_\text{L}}\mathsf{E}_{q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta'})} \text{log} p(\mathbf{y}|\mathbf{X}, \mathbf{w}) - D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}_1, \boldsymbol{\lambda})))  \geq
\]
% добавили минимум под максимум
\[
 \geq
\min_{\boldsymbol{\theta}, \mathbf{h}} D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda})) - \max_{\boldsymbol{\theta}}\frac{1}{\lambda^\text{prior}_\text{L}}\mathsf{E}_q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}) \text{log} p(\mathbf{y}|\mathbf{X}, \mathbf{w}) +
\]
\[
+ \min_{\boldsymbol{\theta}} \frac{1}{\lambda^\text{prior}_\text{L}}\mathsf{E}_{q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})} \text{log} p(\mathbf{y}|\mathbf{X}, \mathbf{w}) 
- \min_{\boldsymbol{\theta}} D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}_1, \boldsymbol{\lambda}))) 
 \geq
\]
% Взяли оценку от всех выражений
\[
 \geq
\min_{\boldsymbol{\theta}, \mathbf{h}} D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda})) - \max_{\boldsymbol{\theta}}\frac{1}{\lambda^\text{prior}_\text{L}}\mathsf{E}_q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}) \text{log} p(\mathbf{y}|\mathbf{X}, \mathbf{w}) +
\]
\[ + \min_{\boldsymbol{\theta}} \frac{1}{\lambda^\text{prior}_\text{L}}\mathsf{E}_{q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})} \text{log} p(\mathbf{y}|\mathbf{X}, \mathbf{w}) 
- \max_{\mathbf{h}}\min_{\boldsymbol{\theta}} D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda}))). 
\]
Складывая полученную оценку с $K_2 -\text{log}~p (\mathbf{h}_2|\boldsymbol{\lambda})+\text{log}~p (\mathbf{h}_1|\boldsymbol{\lambda})$ получаем разность параметрических сложностей больше нуля.

Докажем критерий 5. Пусть $\lambda^\text{likelihood}_\text{Q} = \lambda^\text{prior}_\text{L} = \lambda^\text{prior}_\text{Q} > 0$, $\boldsymbol{\lambda}^\text{struct}_\text{Q} = \mathbf{0}$. Тогда функции $L$ и $Q$  можно записать как: $$L = Q \propto \left(\mathsf{E}_q p(\mathbf{y}|\mathbf{w}, \mathbf{X})-{D}_{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda}))\right),$$ что и требовалось доказать.

Докажем критерий 6. 
Пусть задан вектор метапараметров~$\boldsymbol{\lambda}$, удовлетворяющий девятому условию теоремы и $\boldsymbol{\lambda} = \mathbf{0}$. 
Возьмем в качестве $K_4$ следующее выражение:
\[
K_4 = \frac{\max_{\mathbf{h}} Q - \min_{\mathbf{h}} Q }{\max_{\lambda_\text{struct}}}.
\]



Пусть вектор метапараметров $\boldsymbol{\lambda}'$ отличается от $\boldsymbol{\lambda}$ лишь метапараметром $\boldsymbol{\lambda}_\text{struct}$. Для  обоих векторов метапараметров нижняя задача  оптимизации $L$ совпадает, поэтому выполняется первое условие критерия.

Без ограничения общности предположим, что $Q(\mathbf{h}_1)-Q(\mathbf{h}_2) > 0$ при $\boldsymbol{\lambda}.$ Также без ограничения общности будем полгаать, что множестве $\mathbf{P}$ состоит только из одного распределения на структуре $\boldsymbol{\Gamma}$, равного распределению на структуре $p(\boldsymbol{\Gamma}|\mathbf{h}_1, \boldsymbol{\lambda}).$
%Положим распределение из $\mathbf{P}$ имеющим то же распределение, что и априорное $\mathbf{P} = \{p'(\boldsymbol{\Gamma})\}, p(\boldsymbol{\Gamma}) \sim \text{GS}(\mathbf{s}', \lambda_\text{temp}')\}$. 

Положим для $\boldsymbol{\lambda}'$ параметр $\lambda_\text{struct}$ равным максимальному значению: $\lambda_\text{struct} = \max \lambda_\text{struct}'$.
Тогда при $\boldsymbol{\lambda}'$ неравенство 
\[\
Q(\mathbf{h}_1|\boldsymbol{\lambda}')-Q(\mathbf{h}_2|\boldsymbol{\lambda}')  = Q(\mathbf{h}_1|\boldsymbol{\lambda})-Q(\mathbf{h}_2|\boldsymbol{\lambda})  + \lambda'_\text{struct} D_\text{KL}(p(\boldsymbol{\Gamma}|\mathbf{h}_2,\boldsymbol{\lambda}')|p(\boldsymbol{\Gamma}|\mathbf{h}_1,\boldsymbol{\lambda}')) >
\]
\[
  > Q(\mathbf{h}_1|\boldsymbol{\lambda})-Q(\mathbf{h}_2|\boldsymbol{\lambda})  +  \lambda'_\text{struct}K_4   = Q(\mathbf{h}_1|\boldsymbol{\lambda})-Q(\mathbf{h}_2 + |\boldsymbol{\lambda})  + \max_{\mathbf{h}} Q - \min_{\mathbf{h}} Q   =0,
\]
что и требовалось доказать.

Докажем критерий 7. Достаточным условием непрерывности функций $L$, $Q$ является непрерывность входящих в нее слагаемых. 
Т.к. априорные распределения задаются нерперывными функциями плотности $p(\mathbf{w}|\boldsymbol{\Gamma},\mathbf{h}), p(\boldsymbol{\Gamma}|\mathbf{h},\boldsymbol{\lambda})$, и функция плотности $p(\boldsymbol{\Gamma}|\mathbf{h},\boldsymbol{\lambda})$ распределения структуры $\boldsymbol{\Gamma}$ ограничена на компакте, то дивергенция $D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h},\boldsymbol{\lambda})$ непрерывна по метапараметрам.
Т.к. остальные слагаемые функций оптимизаций $L,Q$ также непрерывны по метапараметрам, то непрерывна и сами функции оптимизации.
\end{proof}

Метапараметрами данной задачи~\eqref{eq:qopt} являются коэффициенты $\lambda^\text{prior}_\text{Q}, \lambda^\text{prior}_\text{L}$, отвечающие за регуляризацию верхней и нижней задачи оптимизации, коэффициент $\lambda_\text{likelihood}^\text{Q}$ отвечает за максимизацию правдоподобия, а также параметры распрделений $\mathbf{P}$ и вектор коэффициентов перед ними $\boldsymbol{\lambda}^\text{struct}_\text{Q}$. 

В предельном случае, когда температура $\lambda_\text{temp}$ близка к нулю, а множество $\mathbf{P}$ состоит из распределений, близких к дискретным,а соответствующим всем возможным структурам, калибровка $\boldsymbol{\lambda}^\text{struct}_\text{Q}$ порождает последовательность задач оптимизаций, схожую с перебором структур. Рассмотрим следующий пример. 

\begin{example} 
Рассмотрим вырожденный случай поведения функции $Q$, когда $\lambda_\text{likelihood}^\text{Q} = \lambda^\text{prior}_\text{Q} = 0$. Пусть модель использует один структурный параметр, в качестве априорного распределения на структуре задано распределение Gumbel-Softmax с $\lambda_\text{temp}=1.0$. Пусть в качестве множества распределений $\mathbf{P}$ используется два распределения Gumbel-Softmax, сконцентрированных близко к вершинам симплекса:
\[
    \mathbf{P} = [\mathcal{GS}([0.95, 0.05, 0.05]^\text{T}, 1.0) ,\mathcal{GS}([0.95, 0.05, 0.05]^\text{T}, 1.0)].
\]
Из определения распределения Gumbel-Softmax следует, что достаточно рассмотреть только значения параметра $\mathbf{s}$ ,находящиеся внутри симплекса.
На рис.~\ref{fig:gs_comb} изображены значения функции Q в зависимости от метапараметров $\boldsymbol{\lambda}^\text{struct}_\text{Q}$ и значений гиперпараметра $\mathbf{s}$ распределения на структуре. Видно, что варьируя  коэффициенты метапараметров получается последовательность оптимизаций, схожая с полным перебором структуры.
\end{example}


\begin{figure}
 \begin{minipage}[t]{.32\textwidth}
   \includegraphics[width=\textwidth]{plots/notebooks/struct_reg_1.png}
\subcaption{}
\end{minipage}
\hfill
 \begin{minipage}[t]{.32\textwidth}
   \includegraphics[width=\textwidth]{plots/notebooks/struct_reg_2.png}
\subcaption{}
\end{minipage}
\hfill
 \begin{minipage}[t]{.32\textwidth}
   \includegraphics[width=\textwidth]{plots/notebooks/struct_reg_3.png}
\subcaption{}
\end{minipage}

\caption{Пример зависимости функции $Q$ от гиперпараметра $\mathbf{s}$ при различных значениях метапараметров $\boldsymbol{\lambda}^\text{struct}_Q$. Темные точки на графике соответсвуют наименее предпочтительным значениям гиперпараметра. а)~$\boldsymbol{\lambda}^\text{struct}_Q = [0,0],$ б)~$~\boldsymbol{\lambda}^\text{struct}_Q = [1,0],$ в)~$~\boldsymbol{\lambda}^\text{struct}_Q = [1,1].$}
\label{fig:gs_comb}

\end{figure}

\section{Анализ обобщающей задачи}
В данном разделе рассматриваются свойства предложенной задачи при различных значениях метапараметров, а также характер ассимптотического поведения задач.


\begin{theorem}
Пусть $m \gg 0$, ${\lambda_\text{prior}^L} > 0, \frac{m}{\lambda_\text{prior}^L}   \in \mathbb{N}, \frac{m}{\lambda_\text{prior}^L}  \gg 0.$ Тогда оптимизация функции
\[
L = 
{\mathsf{E}_q \text{log}~{p(\mathbf{y} | \mathbf{X}, \mathbf{w}, \boldsymbol{\Gamma})}}-{\lambda_\text{prior}^L \text{D}_{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})||p(\mathbf{w}, \boldsymbol{\Gamma} |\mathbf{h}, \lambda_{\text{temp}})}
\] эквивалентна оптимизации вариационной оценки обоснованности  $\mathsf{E}_q \text{log}~{p(\hat{\mathbf{y}} | \hat{\mathbf{X}}, \mathbf{w}, \boldsymbol{\Gamma}, \mathbf{h}, \lambda_{\text{temp}},\mathbf{f})} - \text{D}_{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})||p(\mathbf{w}, \boldsymbol{\Gamma} |\mathbf{h}, \lambda_{\text{temp}})$
для произвольной случайной подвыборки $\hat{\mathbf{y}}, \hat{\mathbf{X}}$ мощности $\frac{m}{{\lambda_\text{prior}^L}}$ из генеральной совопкупности.
\end{theorem}
\begin{proof}
Рассмотрим величину  $\frac{1}{m}L$: \\
\begin{equation}
\label{eq:l_m}
    \frac{1}{m}L = \frac{1}{m}\mathsf{E}_q \text{log}p(\mathbf{y}|\mathbf{X},  \mathbf{w}, \boldsymbol{\Gamma}, \mathbf{h}, \boldsymbol{\lambda})-\frac{\lambda_\text{prior}^L}{m}D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w},\boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda})).
\end{equation}

При $m \gg 0$ по усиленному закону больших чисел данная функция эквивалентна:
\[
    \frac{1}{m}L \approx  \mathsf{E}_{y, \mathbf{x}} \mathsf{E}_{q} \text{log}p(y|\mathbf{x}, \mathbf{w}, \boldsymbol{\Gamma}, \mathbf{h}, \boldsymbol{\lambda}) -\frac{\lambda_\text{prior}^L}{m}D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w},\boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda})).
\]

Аналогично рассмотрим вариационную оценку обоснованности для произвольной выборки мощностью $m_0 = \frac{m}{\lambda_\text{prior}^L}$, усредненную на мощность выборки:
\begin{equation}
\label{eq:l_m0}
    \frac{1}{m_0}\mathsf{E}_q \text{log}p(\mathbf{y}|\mathbf{X},  \mathbf{w}, \boldsymbol{\Gamma}, \mathbf{h}, \boldsymbol{\lambda})-\frac{1}{m_0}D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w},\boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda})) \approx
\end{equation}
\[
\approx  \mathsf{E}_{y, \mathbf{x}} \mathsf{E}_{q} \text{log}p(y|\mathbf{x}, \mathbf{w}, \boldsymbol{\Gamma}, \mathbf{h}, \boldsymbol{\lambda}) -\frac{1}{m_0}D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w},\boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda})) = 
\]
\[
= \mathsf{E}_{y, \mathbf{x}} \mathsf{E}_{q} \text{log}p(y|\mathbf{x}, \mathbf{w}, \boldsymbol{\Gamma}, \mathbf{h}, \boldsymbol{\lambda}) -\frac{\lambda_\text{prior}^L}{m}D_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w},\boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda})).
\]
Таким образом, задачи оптимизации функций~\eqref{eq:l_m},\eqref{eq:l_m0} совпадают, что и требовалось доказать.
\end{proof}
Таким образом, для достаточно большого $m$ и $\lambda^{\text{prior}}_L>0, \lambda^{\text{prior}}_L \neq 1$ оптимизация параметров и гиперпараметров эквивалентна нахождению оценки обоснованности для выборки другой мощности: чем выше значение $ \lambda^{\text{prior}}_L$, тем выше мощность выборки, для которой проводится оптимизация.


Следующие теоремы говорят о соответствии предлагаемой обобщающей задачи вероятностной модели. В частности, задача оптимизации параметров и гиперпараметров соответствует двухуровневому байесовскому выводу.
\begin{theorem}
Пусть ${\lambda^Q_\text{likelihood}} = {\lambda^L_\text{prior}=\lambda^Q_\text{prior}}=1, {\boldsymbol{\lambda}^Q_{\text{struct}}}=\mathbf{0}$. Тогда:
\begin{enumerate}
\item Задача оптимизации~\eqref{eq:qopt} доставляет максимум апостериорной вероятности гиперпараметров с использованием вариационной оценки обоснованности:
\vspace{-0.3cm}
\[
    \text{log}\hat{p}(\mathbf{y}|\mathbf{X}, \mathbf{h}, \lambda_\text{temp}, \mathbf{f})+{\text{log}p(\mathbf{h}|\mathbf{f})} \to \max_{\mathbf{h}}.
\]
\item Вариационное распределение $q$ приближает апостериорное распределение $p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{y}, \mathbf{X}, \mathbf{h}, \boldsymbol{\lambda}, \mathbf{f})$ наилучшим образом:
\vspace{-0.3cm}
\[
    {D}_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{y}, \mathbf{X}, \mathbf{h}, \boldsymbol{\lambda})) \to \min_{\boldsymbol{\theta}}.
\]


\item Если существуют такие значения параметров $\boldsymbol{\theta}_\mathbf{w}, \boldsymbol{\theta}_{\boldsymbol{\Gamma}},$ что $p(\boldsymbol{\Gamma}|\mathbf{y}, \mathbf{X}, \mathbf{h}, \boldsymbol{\lambda}) = q(\boldsymbol{\Gamma}|\boldsymbol{\theta}_{\boldsymbol{\Gamma}})$, $p(\mathbf{w}| \mathbf{y}, \mathbf{X}, \boldsymbol{\Gamma}, \mathbf{h}, \boldsymbol{\lambda}) = q(\mathbf{w}|\boldsymbol{\Gamma}, \boldsymbol{\theta}_{\mathbf{w}})$,
то решение задачи оптимизации $L$ доставляет эти значения вариационных параметров.  
\end{enumerate}
\end{theorem}
\begin{proof}
При $\lambda^Q_\text{likelihood} = \lambda^L_\text{prior} = 1$ как верхняя, так и нижняя задачи оптимизации~\eqref{eq:qopt} эквивалентны оптимизации вариационной оценки обоснованности, поэтому первое утверждение выполняется.

Докажем второе утвреждение. Рассмотрим логарифм обоснованности модели: 
\[
\text{log} p(\mathbf{y}|\mathbf{X}, \mathbf{h}, \boldsymbol{\lambda}) = \mathsf{E}_q \text{log}\frac{p(\mathbf{y}|\mathbf{x}, \mathbf{w}, \boldsymbol{\Gamma})p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda})}{q(\mathbf{w}, \boldsymbol{\Gamma}| \boldsymbol{\theta})} +
\]
\[
+ {D}_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{y}, \mathbf{X}, \mathbf{h}, \boldsymbol{\lambda})) = 
\]
\[
 \mathsf{E}_q \text{log} p(\mathbf{y}|\mathbf{x}, \mathbf{w}, \boldsymbol{\Gamma}) -{D}_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda})) + {D}_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{y}, \mathbf{X}, \mathbf{h}, \boldsymbol{\lambda})).
\]
Из данного равенства следует:
\[
\text{log} p(\mathbf{y}|\mathbf{X}, \mathbf{h}, \boldsymbol{\lambda}) - {D}_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{y}, \mathbf{X}, \mathbf{h}, \boldsymbol{\lambda})) =
\]
\[
    \mathsf{E}_q \text{log} p(\mathbf{y}|\mathbf{x}, \mathbf{w}, \boldsymbol{\Gamma}) -{D}_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda})),
\]
где правая часть равенства соответствует вариационной оценки обоснованности. Выражение $\text{log} p(\mathbf{y}|\mathbf{X}, \mathbf{h}, \boldsymbol{\lambda})$ не зависит от вариационного распределения  $q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})$, поэтому максимизации вариационной оценки эквивалентна минимизации дивергенции ${D}_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{y}, \mathbf{X}, \mathbf{h}, \boldsymbol{\lambda}))$.

Докажем третье утверждение. Т.к. вариационное распределение $q$ декомпозируется на $q(\boldsymbol{\Gamma}|\boldsymbol{\theta}_{\boldsymbol{\Gamma}}),  q(\mathbf{w}|\boldsymbol{\Gamma}, \boldsymbol{\theta}_{\mathbf{w}})$, апостериорное распределение $p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{y}, \mathbf{X}, \mathbf{h}, \boldsymbol{\lambda}))$ декомпозируется на $p(\boldsymbol{\Gamma}|\mathbf{y}, \mathbf{X}, \mathbf{h}, \boldsymbol{\lambda}),$ $p(\mathbf{w}| \mathbf{y}, \mathbf{X}, \boldsymbol{\Gamma}, \mathbf{h}, \boldsymbol{\lambda})$, поэтому достижимо значение нулевое значение
дивергенции: ${D}_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{y}, \mathbf{X}, \mathbf{h}, \boldsymbol{\lambda})) = 0.$ Она представима в следующем виде:
\[
{D}_\text{KL}(q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta})|p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{y}, \mathbf{X}, \mathbf{h}, \boldsymbol{\lambda})) = 
{D}_\text{KL}(q(\boldsymbol{\Gamma}|\boldsymbol{\theta}_{\boldsymbol{\Gamma}})|p(\boldsymbol{\Gamma}|\mathbf{y}, \mathbf{X}, \mathbf{h}, \boldsymbol{\lambda})) +
\]
\[+ {D}_\text{KL}(q(\boldsymbol{\Gamma}|\boldsymbol{\theta}_{\boldsymbol{\Gamma}})|p(\boldsymbol{\Gamma}|\mathbf{y}, \mathbf{X}, \mathbf{h}, \boldsymbol{\lambda})).
\]
Отсюда следует что соотстветсвующие вариационные и апостериорные распределения совпадают.
\end{proof}


Таким образом, предлагаемая обобщающая задача позволяет производить оптимизацию вариационной оценки обоснвоанности, а также оптимизацию обоснованности для выбор с другим эффекитвным размером. Чем больше размер выборки, тем больше влияние априорного распределения, которое выступает в качестве регуляризатора. Можно регулировать сложность модели следующим образом:
\begin{enumerate}
\item Калибруя верхнюю оптимизацию;
\item Калибруя нижнюю оптимизацию;
\item Калибруя обе оптимизации.
\end{enumerate}
Последний вариант соответствует теореме о калибровке. 
Рассмотрим различие варианта 1 и 2 на примере.
\begin{example}
Пусть задана модель и выборка и мы хотим уменьшить вес априорного распределения.
В случае, если мы калибруем нижнюю оптимизацию (->0), на первом уровне задача совпадает с задачей поиска наиболее правдоподобных параметров, при этом на верхнем уровне мы ищем те параметры, которые отвечают наилучшим с точки зрения обоснованности.

Если мы калибруем верхнюю оптимизацию или обе оптимизации, то это приведет к поиску наиболее правдоподобных параметров и гиперпараметров. 

Таким образом, основная разница между калибровкой верхней и нижней оптимизации заключается в следующем:
при калибровке нижнего уровня мы получаем модель, соответствующую критерию максимального правдоподобия.
В случае калибровки верхнего уровня мы получаем модель с параметрами, полученными в соответствии с методом максимальной обоснованности, но при минимально возможной регуляризации априорным распределением.
\end{example}



\begin{theorem}
Пусть $\frac{{\lambda_{\text{prior}}^Q}}{{\lambda_{\text{likelihood}}^Q}} = {\lambda_{\text{prior}}^L}$. 
Тогда задачи оптимизации~\eqref{eq:qopt} представима в виде одноуровневой задачи оптимизации:
\[
= {\lambda_\text{likelihood}^\text{Q}\mathsf{E}_{{q(\mathbf{w}, \boldsymbol{\Gamma}| \boldsymbol{\theta})}} \text{log}~{p(\mathbf{y} | \mathbf{X}, \mathbf{w},\boldsymbol{\Gamma}, \mathbf{h}, \lambda_\text{temp}, \mathbf{f})}}
 -\]
\vspace{-0.3cm}
\[- {\lambda^\text{prior}_\text{Q}\text{D}_{KL}\bigl( q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}) || p(\mathbf{w}, \boldsymbol{\Gamma} |\mathbf{h}, \lambda_{\text{temp}},\mathbf{f}) \bigr)}  -\]
\vspace{-0.3cm}
\[
-{\sum_{p' \in \mathbf{P}, \lambda \in \boldsymbol{\lambda}^\text{struct}_\text{Q}} \lambda\text{D}_{KL}(\boldsymbol{\Gamma} | p')+\text{log}p(\mathbf{h}|\mathbf{f})} \to \max_{\mathbf{h}, \boldsymbol{\theta}}. 
\]  
\end{theorem}
\begin{proof}
Параметры вариационного распределения $q$ не зависят от слагаемых вида $\text{log}p(\mathbf{h}|\mathbf{f})$ и $\text{D}_{KL}(\boldsymbol{\Gamma} | p'), p' \in \mathbf{P},$ поэтому нижняя задача оптимизации:
\[
    \text{log}~{p(\mathbf{y} | \mathbf{X}, \mathbf{w},\boldsymbol{\Gamma}, \mathbf{h}, \lambda_\text{temp}, \mathbf{f})} -
\]
\[
    - {\lambda^\text{prior}_\text{L}\text{D}_{KL}\bigl( q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}) || p(\mathbf{w}, \boldsymbol{\Gamma} |\mathbf{h}, \lambda_{\text{temp}},\mathbf{f}) \bigr)} \to \max_{\boldsymbol{\theta}}
\]
эквивалентна следующей задаче:
\[
    \text{log}~{p(\mathbf{y} | \mathbf{X}, \mathbf{w},\boldsymbol{\Gamma}, \mathbf{h}, \lambda_\text{temp}, \mathbf{f})} -
\]
\[
    - {\lambda^\text{prior}_\text{L}\text{D}_{KL}\bigl( q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}) || p(\mathbf{w}, \boldsymbol{\Gamma} |\mathbf{h}, \lambda_{\text{temp}},\mathbf{f}) \bigr)} \to \max_{\boldsymbol{\theta}}
\]
\[
-{\sum_{p' \in \mathbf{P}, \lambda \in \boldsymbol{\lambda}^\text{struct}_\text{Q}} \lambda\text{D}_{KL}(\boldsymbol{\Gamma} | p')+\text{log}p(\mathbf{h}|\mathbf{f})} \to \max_{\boldsymbol{\theta}}
\] 
для любого вектора $\boldsymbol{\lambda}^\text{struct}_\text{Q}$. 
Т.к. выполнено равенство  $\frac{{\lambda_{\text{prior}}^Q}}{{\lambda_{\text{likelihood}}^Q}} = {\lambda_{\text{prior}}^L}$, то нижняя задача оптимизации экивалентна следующей задаче:
\[
= {\lambda_\text{likelihood}^\text{Q}\mathsf{E}_{{q(\mathbf{w}, \boldsymbol{\Gamma}| \boldsymbol{\theta})}} \text{log}~{p(\mathbf{y} | \mathbf{X}, \mathbf{w},\boldsymbol{\Gamma}, \mathbf{h}, \lambda_\text{temp}, \mathbf{f})}}
 -\]
\vspace{-0.3cm}
\[- {\lambda^\text{prior}_\text{Q}\text{D}_{KL}\bigl( q(\mathbf{w}, \boldsymbol{\Gamma}|\boldsymbol{\theta}) || p(\mathbf{w}, \boldsymbol{\Gamma} |\mathbf{h}, \lambda_{\text{temp}},\mathbf{f}) \bigr)}  -\]
\vspace{-0.3cm}
\[
-{\sum_{p' \in \mathbf{P}, \lambda \in \boldsymbol{\lambda}^\text{struct}_\text{Q}} \lambda\text{D}_{KL}(\boldsymbol{\Gamma} | p')+\text{log}p(\mathbf{h}|\mathbf{f})} \to \max_{\boldsymbol{\theta}},
\]  
а значит верхняя и нижняя задачи совпадают:
\[
    \mathbf{h} = \argmax_{\mathbf{h}'} Q(\mathbf{h}, \boldsymbol{\theta}^{*}(\mathbf{h}')),
\]
где 
\[
     \boldsymbol{\theta}^{*}(\mathbf{h}') = \argmax_{\boldsymbol{\theta}'} Q(\mathbf{h}', \boldsymbol{\theta}')).
\]
Из свойства 
\[
    \max_{\mathbf{h}}\max_{\boldsymbol{\theta}} Q(\boldsymbol{\theta}, \mathbf{h}) = \max_{\boldsymbol{\theta}, \mathbf{h}} Q(\boldsymbol{\theta}, \mathbf{h})
\]
следует доказательство теоремы.
\end{proof}



Для вычисления приближенного значения функций $Q$ и $L$ предлагается использовать приближение методом Монте-Карло с порождением $R$ реализаций величин $\mathbf{w}, \boldsymbol{\Gamma}$:

\[
   \mathsf{E}_q \text{log}~p(\mathbf{y}|\mathbf{X}, \boldsymbol{\theta}_1 \lambda_{\text{temp}}, \mathbf{f}) \approx   \sum_{r=1}^R \text{log}p(\mathbf{y}|\boldsymbol{\mu}+\boldsymbol{\alpha}_q \circ \hat{\epsilon}_r, \hat{\boldsymbol{\Gamma}}_r, \mathbf{X}),
\]
\[
D_\text{KL}\left(q_{\boldsymbol{\Gamma}}(\boldsymbol{\Gamma}|\boldsymbol{\theta}_{\boldsymbol{\Gamma}})|p(\boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda})\right)   \approx  \sum_{r=1}^R \left(\text{log}q_{\boldsymbol{\Gamma}}(\hat{\boldsymbol{\Gamma}}_r|\boldsymbol{\theta}_{\boldsymbol{\Gamma}}))-p(\hat{\boldsymbol{\Gamma}}|\mathbf{h},\boldsymbol{\lambda})\right),
\]
\[
D_\text{KL}\left(q_{\mathbf{w}}(\mathbf{w}|\boldsymbol{\theta}_\mathbf{w},\boldsymbol{\Gamma})|p(\mathbf{w}|\boldsymbol{\Gamma}, \mathbf{h})\right)  =  \sum_{(j,k) \in E}\sum_{l=1}^{K^{j,k}} D_\text{KL}\left(q_{\mathbf{w}}(\mathbf{w}^{j,k}_l|\boldsymbol{\theta}_\mathbf{w},\gamma^{j,k}_l)|p(\mathbf{w}^{j,k}_l|\gamma^{j,k}_l, \mathbf{h})\right)\approx
\]
\[ 
\approx-\sum_{(j,k) \in E}\sum_{l=1}^{K_{j,k}}\sum_{r=1}^R\frac{1}{2}\left( (\hat{\gamma}^{j,k}_r[l]\right)^{-1}\text{tr}((\mathbf{A}^{j,k}_l)_q(\mathbf{A}^{j,k}_l)^{-1})+(\boldsymbol{\mu}^{j,k}_l)^{\mathsf{T}}\hat{\gamma}^{j,k}_r[l]^{-1}(\mathbf{A}^{j,k}_l)^{-1}\boldsymbol{\mu^{j,k}_l} -
\]
\[
- |\mathbf{w}^{j,k}_l|+\text{log}\frac{|\hat{\gamma}^{j,k}[l]_r\mathbf{A}^{j,k}_l|}{|(\mathbf{A}^{j,k}_l)_q|}),
\]
где $R$ --- количество реализаций случайных величин, по котором вычисляется значения вариационной оценки обоснованности, $\hat{\epsilon}_r \sim \mathcal{N}(0,1),$
 $\hat{\boldsymbol{\Gamma}}_r = [\hat{\boldsymbol{\gamma}}^{j,k}_r, (j,k) \in E]$ --- реализация случайной величины, соответствующей структуре $\boldsymbol{\Gamma}$.

Для решения двухуровневой задачи предлагается использовать градиентные методы. 
\begin{theorem}
Пусть $T$ --- оператор градиентного спуска.
Пусть $Q,L$ --- локально выпуклы и непрерывны в некоторой области $U_{W} \times U_{\Gamma} \times U_H \times U_\lambda \subset \mathbb{W}\times\amsmathbb{\Gamma}\times\mathbb{H}\times\amsmathbb{\Lambda}$, при  этом $U_H \times U_\lambda$ --- компакт. 
Тогда решение задачи градиентной оптимизации 
\[
     \mathbf{h}^{*} = T^\eta\bigl(Q, \mathbf{h}, T^\eta(L, \boldsymbol{\theta}_0, \mathbf{h})\bigr)
\] 
стремится к локальному минимуму  $\mathbf{h}^{*} \in U$ исходной задачи оптимизации при $\eta \to \infty$,
$\mathbf{h}^{*}$ является непрерывной функцией по метапараметрам модели.
\end{theorem}

\begin{proof}
TODO
\end{proof}


Следующие теоремы посвящены ассимптотическим свойствам представленной обобщающей задачи.
\begin{theorem}
Пусть ${\lambda_{\text{likelihood}}^Q}= {\lambda_{\text{prior}}^L}>0, {\boldsymbol{\lambda}^Q_{\text{struct}} }= \bf{0}$.
Тогда предел оптимизации
\[
\lim_{{\lambda^Q_\text{prior}} \to \infty} \lim_{\eta \to \infty}   T^\eta\bigl(Q, \mathbf{h}, T^\eta(L, \boldsymbol{\theta}_0, \mathbf{h})\bigr)
\]  
доставляет минимум параметрической сложности. 
\end{theorem}
\begin{proof}
TODO
\end{proof}

\begin{theorem}
Пусть ${\lambda^L_{\text{likelihood}}} = 1 ,{\boldsymbol{\lambda}^Q_{\text{struct}}} = \bf 0$.
Пусть  $\mathbf{f}_1, \mathbf{f}_2$ --- результаты градиентной оптимизации при разных значениях гиперпараметров ${\lambda_{\text{prior}}^{Q,1},\lambda_{\text{prior}}^{Q,2}, \lambda_{\text{prior}}^{Q,1}<\lambda_{\text{prior}}^{Q,2}}$, полученных при начальном значении вариационных параметров $\boldsymbol{\theta}_0$ и гиперпараметров $\mathbf{h}_0$.
Пусть $\boldsymbol{\theta}_0, \mathbf{h}_0$ принадлежат области  $U$, в которой соответствующие функции $L$ и $Q$ являются локально-выпуклыми.
Тогда:
\[
    C_p(\mathbf{f}_1)-C_p(\mathbf{f}_2)  \geq {\lambda_\text{prior}^L(\lambda_\text{prior}^L-\lambda_\text{prior}^{Q,1})}\text{sup}_{\boldsymbol{\theta}, \mathbf{h} \in U}|\nabla^2_{\boldsymbol{\theta}, \mathbf{h}} {D_{KL}(q|p)} (\nabla^2_{\boldsymbol{\theta}} L)^{-1}   \nabla_{\boldsymbol{\theta}} {D_{KL}(q|p))}|.
\]
\end{theorem}
\begin{proof}
TODO
\end{proof}
TODO: выводы
\textbf{Эксперимент: пример 1}

\textbf{Эксперимент: пример 2}







