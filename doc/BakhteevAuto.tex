\documentclass{dissert}
\usepackage{cmap}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{tabularx}
\usepackage[noend]{algorithmic}
\usepackage{tikz}
\usepackage{tikz,fullpage}
\usepackage{lastpage}
\usepackage[shortlabels]{enumitem}
\usepackage{subcaption}
\usepackage{mathbbol}
\usepackage{amssymb}
\usepackage{xargs}             % AMS Math

\DeclareUnicodeCharacter{00A0}{ } % При наборе текста с планшета появляются невидимые символы. ЭТо костыль.
\DeclareSymbolFontAlphabet{\mathbb}{AMSb}%
\DeclareSymbolFontAlphabet{\amsmathbb}{bbold}%

%https://tex.stackexchange.com/questions/163451/total-number-of-citations
\usepackage{totcount}
\newtotcounter{citnum} %From the package documentation
\def\oldbibitem{} \let\oldbibitem=\bibitem
\def\bibitem{\stepcounter{citnum}\oldbibitem}


\usetikzlibrary{arrows,automata}
\usetikzlibrary{positioning}


\def\algorithmicrequire{\textbf{Вход:}}
\def\algorithmicensure{\textbf{Выход:}}
\def\algorithmicif{\textbf{если}}
\def\algorithmicthen{\textbf{то}}
\def\algorithmicelse{\textbf{иначе}}
\def\algorithmicelsif{\textbf{иначе если}}
\def\algorithmicfor{\textbf{для}}
\def\algorithmicforall{\textbf{для всех}}
\def\algorithmicdo{}
\def\algorithmicwhile{\textbf{пока}}
\def\algorithmicrepeat{\textbf{повторять}}
\def\algorithmicuntil{\textbf{пока}}
\def\algorithmicloop{\textbf{цикл}}
\def\algorithmicreturn{\textbf{вернуть}}
% переопределение стиля комментариев
\def\algorithmiccomment#1{\quad// {\sl #1}}


%\usepackage[cp1251]{inputenc}
\renewcommand{\rmdefault}{cmr}
\usepackage{dsfont} % for indicator function
\usepackage[russian]{babel}

\usepackage{color}
\usepackage{caption}

\usepackage{float}
%\usepackage{slashbox}
%пакеты и команды, необходимость в которых может возникнуть по ходу работы (Вы можете добавлять свои):
\usepackage{indentfirst}%для отступов
    \usepackage{subfig}

\usepackage{geometry}
\geometry{left=2.5cm}
\geometry{right=1.0cm}
\geometry{top=2.0cm}
\geometry{bottom=2.0cm}
\renewcommand{\baselinestretch}{1.0}

%\usepackage[backend=biber,style=alphabetic]{biblatex}    

%\usepackage{geometry}
%\emergencystretch=25pt%для борьбы с переполнениями за счет разреж. слов в абзаце
%\righthyphenmin=2% для разрешения переноса двух последних букв
%\arrayrulewidth=.75pt% регулируем толщину линий в табл.
%\usepackage[dvips]{graphicx}%для включения PS файлов
%\usepackage[final]{epsfig}
\usepackage{multicol}%для организации многоколоночного текста (предм. указатель)
%\usepackage{subcaption}

\usepackage{indentfirst}
\usepackage{amsmath}
%\usepackage{enumerate}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amscd}
\usepackage{hhline}
%\usepackage{multirow}
\usepackage{graphicx, epsfig}
%\usepackage{epic}
\usepackage{amscd}
%\usepackage{ecltree}
\usepackage{color}
\usepackage[dvips,all]{xy}
%\usepackage{setspace}
%\usepackage{makeidx}
%пакет для вкл символов напр номер
%\usepackage{textcomp}
%\usepackage{amsfonts}
%\usepackage{afterpage}% Полезно для полного заполнения страниц перед большими таблицами
%\usepackage{longtable}%Для таблиц на нескольких страницах
%\usepackage{cite}
%\usepackage{rawfonts}
%\usepackage{oldlfont}%доступ к шрифтам через устаревшие команды
\usepackage{array}
%\renewcommand{\bibname}{Список литературы}
\renewcommand{\contentsname}{Содержание}
\renewcommand{\contentsdesc}{Стр.}
\renewcommand{\chaptername}{Глава}


%%% Библиография %%%
\makeatletter
\bibliographystyle{utf8gost71u}     % Оформляем библиографию по ГОСТ 7.1 (ГОСТ Р 7.0.11-2011, 5.6.7)
%\renewcommand{\@biblabel}[1]{#1.}   % Заменяем библиографию с квадратных скобок на точку
\makeatother

%\def\BibUrl#1.{}
%\bibliographystyle{gost2008} 
\theoremstyle{definition}
\newtheorem{theorem}{Теорема}
\newtheorem{lemma}{Лемма}
\newtheorem{defin}{Определение}
\newtheorem{utv}{Утверждение}
\newtheorem{example}{Пример}


\def\bbljan{Январь}
\def\bblfeb{Февраль}
\def\bblmar{Март}
\def\bblapr{Апрель}
\def\bblmay{Май}
\def\bbljun{Июнь}
\def\bbljul{Июль}
\def\bblaug{Август}
\def\bblsep{Сентябрь}
\def\bbloct{Октябрь}
\def\bblnov{Ноябрь}
\def\bbldec{Декабрь}


\renewcommand{\thesubfigure}{\asbuk{subfigure}}

\graphicspath{{pic/}}
 
%smerdov

\newcommand{\RR}{\mathbb{R}}
\newcommand{\PP}{p}
\newcommand{\DD}{{\mathfrak{D}}}
\newcommand{\FF}{{\mathcal{F}}}
%\newcommand{\AAA}{{\mathcal{A}}}
\newcommand{\FFF}{{\mathfrak{F}}}
\newcommand{\NNN}{{\mathcal{N}}}
\newcommand{\WW}{{\mathbb{W}}}
\newcommand{\bw}{{\textbf{w}}}
\newcommand{\ba}{{\textbf{a}}}
\newcommand{\bb}{{\textbf{b}}}
\newcommand{\bx}{{\textbf{x}}}
\newcommand{\II}{{\textbf{I}}}
%\newcommand{\bmu}{{\boldsymbol{\mu}}}
\newcommand{\bbf}{{\textbf{f}}}
\newcommand{\by}{{\textbf{y}}}
\newcommand{\bm}{{\textbf{m}}}
\newcommand{\bW}{{\textbf{W}}}
\newcommand{\bWs}{{\textbf{W}_{\textbf{s}}}}
\newcommand{\bU}{{\textbf{U}}}
\newcommand{\bV}{{\textbf{V}}}
\newcommand{\bh}{{\textbf{h}}}
\newcommand{\bu}{{\textbf{u}}}
\newcommand{\bbW}{{\textbf{b}_{\textbf{W}}}}
\newcommand{\bbU}{{\textbf{b}_{\textbf{U}}}}
\newcommand{\bbV}{{\textbf{b}_{\textbf{V}}}}
\newcommand{\bs}{{\boldsymbol{\sigma}}}
\newcommand{\al}{{\alpha}}
\newcommand{\bal}{\boldsymbol{\alpha}}
\newcommand{\bbt}{\boldsymbol{\beta}}
%\newcommand{\bA}{\mathbf{A^\text{-1}}}
\newcommand{\bApr}{\mathbf{A^\text{-1}_{\text{pr}}}}
\newcommand{\bAps}{\mathbf{A^\text{-1}_{\text{ps}}}}
\newcommand{\bmupr}{\boldsymbol{\mu}}
\newcommand{\bmups}{\textbf{m}}
\newcommand{\DKL}{\mathit{D}_{\text{KL}}}


% opers
\DeclareMathOperator*{\indicator}{\mathds{1}}
\DeclareMathOperator*{\softmax}{softmax}
\DeclareMathOperator*{\idx}{idx}
\DeclareMathOperator*{\pos}{pos}
\DeclareMathOperator*{\AUCH}{AUCH}
\DeclareMathOperator*{\tf}{tf}
\DeclareMathOperator*{\ntf}{ntf}
\DeclareMathOperator*{\idf}{idf}
\DeclareMathOperator*{\ndf}{ndf}
\DeclareMathOperator*{\similarity}{sim}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\const}{const}
\DeclareMathOperator*{\dBeta}{Beta}
\DeclareMathOperator*{\dDir}{Dir}
\DeclareMathOperator*{\Tr}{Tr}
\DeclareMathOperator*{\dDP}{DP}
\DeclareMathOperator*{\dMult}{Mult}
\DeclareMathOperator*{\dBern}{Bern}
\DeclareMathOperator*{\dCRP}{CRP}
\DeclareMathOperator*{\dKL}{KL}
\DeclareMathOperator*{\diag}{diag}
\newcommand{\dd}[1]{\mathrm{d}{#1}}

% BOLD
\newcommand{\bmatr}{{\mathbf{B}}}
\newcommand{\cmatr}{{\mathbf{C}}}
\newcommand{\hmatr}{{\mathbf{H}}}
\newcommand{\fmatr}{{\mathbf{F}}}
\newcommand{\mmatr}{{\mathbf{M}}}
\newcommand{\xmatr}{{\mathbf{X}}}
\newcommand{\pmatr}{{\mathbf{P}}}
\newcommand{\xmatrt}{{\tilde{\mathbf{X}}}}
\newcommand{\imatr}{{\mathbf{I}}}
\newcommand{\vmatr}{{\mathbf{V}}}
\newcommand{\wmatr}{{\mathbf{W}}}
\newcommand{\umatr}{{\mathbf{U}}}
\newcommand{\zmatr}{{\mathbf{Z}}}
\newcommand{\zmatrt}{{\tilde{\mathbf{Z}}}}
\newcommand{\Tmatr}{\mathbf{T}}
\newcommand{\lambdamatr}{{\mathbf{\Lambda}}}
\newcommand{\phimatr}{\mathbf{\Phi}}
\newcommand{\sigmamatr}{\mathbf{\Sigma}}
\newcommand{\thetamatr}{\boldsymbol{\Theta}}

\newcommand{\zbt}{{\tilde{\mathbf{z}}}}
\newcommand{\mub}{{\boldsymbol{\mu}}}
\newcommand{\alphab}{{\boldsymbol{\alpha}}}
\newcommand{\thetab}{\boldsymbol{\theta}}
\newcommand{\iotab}{\boldsymbol{\iota}}
\newcommand{\zetab}{\boldsymbol{\zeta}}
\newcommand{\xib}{\boldsymbol{\xi}}
\newcommand{\xibt}{\tilde{\boldsymbol{\xi}}}
\newcommand{\xit}{\tilde{\xi}}
\newcommand{\betab}{{\boldsymbol{\beta}}}
\newcommand{\phib}{{\boldsymbol{\phi}}}
\newcommand{\psib}{{\boldsymbol{\psi}}}
\newcommand{\gammab}{{\boldsymbol{\gamma}}}
\newcommand{\lambdab}{{\boldsymbol{\lambda}}}
\newcommand{\varepsilonb}{{\boldsymbol{\varepsilon}}}
\newcommand{\pib}{{\boldsymbol{\pi}}}


\newcommand{\scl}{s_{\mathsf{c}}}
\newcommand{\shi}{s_{\mathsf{h}}}
\newcommand{\shib}{\mathbf{s}_{\mathsf{h}}}
\newcommand{\MOD}{M}
\newcommand{\entr}{\mathsf{H}}
\newcommand{\REG}{\Omega}
\newcommand{\Mquol}{V}
%\newcommand{\prob}{\mathsf{P}}
\newcommand{\prob}{p}
\newcommand{\expec}{\mathsf{E}}

% overline
\newcommand{\xo}{{\overline{x}}}
\newcommand{\yo}{{\overline{y}}}

% bold overline
\newcommand{\xbo}{{\overline{\mathbf{x}}}}





\newcommand{\Amc}{{\mathcal{A}}}
\newcommand{\Bmc}{{\mathcal{B}}}
\newcommand{\Cmc}{{\mathcal{C}}}
\newcommand{\Jmc}{{\mathcal{J}}}
\newcommand{\Imc}{{\mathcal{I}}}
\newcommand{\Kmc}{{\mathcal{K}}}
\newcommand{\Lmc}{{\mathcal{L}}}
\newcommand{\Mmc}{{\mathcal{M}}}
\newcommand{\Nmc}{{\mathcal{N}}}
\newcommand{\Pmc}{{\mathcal{P}}}
\newcommand{\Tmc}{{\mathcal{T}}}
\newcommand{\Vmc}{{\mathcal{V}}}
\newcommand{\Wmc}{{\mathcal{W}}}


\newcommand{\T}{\mathsf{T}}
\newcommand{\deist}{\mathbb{R}}
\newcommand{\ebb}{\mathbb{E}}

%ALEX
\newcommand{\Amatr}{\mathbf{A}}

\newcommand{\Umatr}{\mathbf{U}}
\newcommand{\zetavec}{\boldsymbol{\zeta}}


%\renewcommand{\thesubfigure}{\asbuk{subfigure}}



%different caption style: ``Fig.~N.~Caption text''
\makeatletter
\long\def\@makecaption#1#2{%
  \vskip\abovecaptionskip
  \sbox\@tempboxa{#1.~#2}%
  \ifdim \wd\@tempboxa >\hsize
    #1.~#2\par
  \else
    \global \@minipagefalse
    \hb@xt@\hsize{\hfil\box\@tempboxa\hfil}%
  \fi
  \vskip\belowcaptionskip}
\makeatother

\reversemarginpar

%\captionsetup[figure]{labelformat=gostfigure, justification=centering}
%\captionsetup[subfigure]{labelformat=gostfigure, justification=centering}
%\captionsetup[table]{labelformat=gostfigure, justification=centering}



%\renewcommand{\thesubfigure}{\asbuk{subfigure}}

\DeclareMathOperator*{\argmax}{arg\,max}



%different caption style: ``Fig.~N.~Caption text''
\makeatletter
\long\def\@makecaption#1#2{%
  \vskip\abovecaptionskip
  \sbox\@tempboxa{#1.~#2}%
  \ifdim \wd\@tempboxa >\hsize
    #1.~#2\par
  \else
    \global \@minipagefalse
    \hb@xt@\hsize{\hfil\box\@tempboxa\hfil}%
  \fi
  \vskip\belowcaptionskip}
\makeatother

\reversemarginpar

%\captionsetup[figure]{labelformat=gostfigure, justification=centering}
%\captionsetup[subfigure]{labelformat=gostfigure, justification=centering}
%\captionsetup[table]{labelformat=gostfigure, justification=centering}


\begin{document}
\input{defs}


\begin{titlepage}
%\begin{center}
%\textsc{МОСКОВСКИЙ ФИЗИКО-ТЕХНИЧЕСКИЙ ИНСТИТУТ (ГОСУДАРСТВЕННЫЙ УНИВЕРСИТЕТ)}\\
%\end{center}
%\vspace{1.5cm}
\begin{flushright}
{На правах рукописи}
\end{flushright}
\vspace{1.5cm}
\begin{center}
{Бахтеев Олег Юрьевич}
\par
\vspace{2cm}
\textsc{Байесовский выбор субоптимальной структуры
\\модели глубокого обучения }
\par
\vspace{2cm}
{05.13.17~--- Теоретические основы информатики}
\par
\vspace{2cm}
{АВТОРЕФЕРАТ\\
диссертации на соискание ученой степени\\
кандидата физико-математических наук}
\end{center}
\par
\vspace{3.5cm}
\begin{center}
{Москва~--- 2019}
\end{center}
\end{titlepage}

%\clearpage\maketitle
\setcounter{page}{2}
%\pretolerance=10000
%\thispagestyle{empty}
\noindent {Работа выполнена на кафедре интеллектуальных систем федерального государственного автономного образовательного учреждения высшего образования <<Московский физико-технический институт (национальный исследовательский институт)>>.

\vspace{0.2cm}

%\begin{sloppy}

\vskip1ex\noindent
\begin{tabularx}{\linewidth}{@{}lX@{}}
  Научный руководитель: & \textbf{Стрижов Вадим Викторович}\\
  & доктор физико-математических наук, федеральный исследовательский~центр <<Информатика и управление>> Российской академии наук, отдел интеллектуальных систем, ведущий научный сотрудник.
  \\[6pt]
  Официальные оппоненты: & \textbf{Чуличков Алексей Иванович}\\
  & доктор физико-математических наук, профессор, федеральное государственное бюджетное образовательное учреждение высшего образования <<Московский государственный университет имени М.В. Ломоносова>>, заведующий кафедрой математического моделирования и информатики физического факультета.\\[6pt]
  & \textbf{Зайцев Алексей Алексеевич}\\
  & кандидат физико-математических наук, автономная некоммерческая
образовательная организация высшего образования <<Сколковский институт науки и технологий>>, старший преподаватель \\[6pt]
  Ведущая организация: Федеральное государственное автономное образовательное учреждение высшего образования <<Санкт-Петербургский национальный исследовательский университет информационных технологий, механики и оптики>>
\end{tabularx}
\vskip2ex\noindent


\vspace{0.2cm}
%\noindent Защита состоится~``\underline{\phantom{332}}
%''\underline{\phantom{xxxxxxxxxxxxxxxxxxxx}} 2017~года~в~\underline{\phantom{332}}:\underline{\phantom{332}} на~заседании диссертационного совета Д 002.073.05 при Федеральном исследовательском центре <<Информатика и управление>> Российской академии наук (ФИЦ~ИУ~РАН) по адресу: 119333, г.\,Москва, ул.\,Вавилова, д.\,40.
\noindent TODO Защита состоится~<<25>> декабря 2019 года~в~13:00 на~заседании диссертационного совета Д 002.073.05 при Федеральном исследовательском центре <<Информатика и управление>> Российской академии наук (ФИЦ~ИУ~РАН) по адресу: 119333, г.\,Москва, ул.\,Вавилова, д.\,40.

\vspace{0.2cm}
\noindent С диссертацией можно ознакомиться в библиотеке Федерального государственного учреждения <<Федеральный исследовательский центр <<Информатика и управление>> Российской академии наук>> и на сайте http://www.frccsc.ru/

\vspace{0.2cm}
\noindent Автореферат разослан TODO <<\underline{\phantom{30}}>> \underline{\phantom{июня}} 2019 года.

\vspace{0.3cm}
\noindent Ученый секретарь\\
диссертационного совета Д 002.073.05\\
д.ф.-м.н., профессор
\hspace{9cm} В.В.Рязанов
}

\clearpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pretolerance=-1

%\setcounter{page}{0}

\section*{Общая характеристика работы}
\label{ch:Introduction}

\textbf{Актуальность темы.}
TODO: формат библиографии? 
В работе рассматривается задача автоматического построения моделей глубокого обучения оптимальной и субоптимальной сложности. 

Под сложностью модели понимается \emph{минимальная длина описания}~\cite{mdl}, т.е. минимальное количество информации, которое требуется для передачи информации о модели и о выборке. Вычисление минимальной длины описания модели является вычислительно сложной процедурой. В работе предлагается получение ее приближенной оценки, основанной на связи минимальной длины описания и \emph{обоснованности модели}~\cite{mdl}. Для получения оценки обоснованности используются вариационные методы получения оценки обоснованности~\cite{bishop}, основанные на аппроксимации неизвестного апостериорного распределения другим заданным распределением. Под субоптимальной сложностью понимается вариационная оценка обоснованности модели.

Одна из проблем построения моделей глубокого обучения --- большое количество параметров моделей~\cite{hinton_rbm, hinton_init}. Поэтому задача выбора моделей глубокого обучения включает в себя выбор стратегии построения модели, эффективной по вычислительным ресурсам. В работе~\cite{greed} приводятся теоретические оценки построения нейросетей с использованием жадных стратегий,  при которых построение модели производится итеративно последовательным увеличением числа нейронов в сети. В работе~\cite{greed_mlp} предлагается жадная стратегия выбора модели нейросети с использованием релевантных распределений, т.е. параметрических распределений, оптимизация параметров которых позволяет удалить часть параметров из модели. Данный метод был также применялся в задаче построения модели метода релевантных векторов~\cite{rvm}. Альтернативой данным алгоритмам построения моделей являются методы, основанные на прореживании сетей глубокого обучения~\cite{obd, popova, nvidia_prune}, т.е. на последовательном удалении параметров, не дающих существенного прироста качества модели. 
В работах~\cite{Bengio, hd} рассматривается послойное построение модели с отдельным критерием оптимизации для каждого слоя. В работах~\cite{Kingma, gendis_pictures, gendis_phd} предлагается декомпозиция модели на порождающую и разделяющую, оптимизируемые последовательно. В работе~\cite{adanet} предлагается метод автоматического построения сети, основанный на бустинге. В качестве оптимизируемого функционала предлагается линейная комбинация функции правдоподобия выборки и сложности модели по Радемахеру. 
В работах~\cite{reinf,reinf_predict,reinf_deep2net,reinf_transfer} предлагается метод автоматического построения сверточной сети с использованием обучения с подкреплением. В~\cite{darts} используется схожее представление сверточной сети, вместо обучения с подкреплением используется градиентная оптимизация параметров, задающих структуру нейронной сети.

%В качестве порождающих моделей в сетях глубокого обучения выступают ограниченные машины Больцмана~\cite{hinton_rbm} и автокодировщики~\cite{founds}. В работе~\cite{contractive} рассматриваются некоторые типы регуляризации автокодировщиков, позволяющие формально рассматривать данные модели как порождающие модели с использованием байесовского вывода. В работе~\cite{score} также рассматриваются регуляризованные автокодировщики и свойства оценок их правдоподобия. В работе~\cite{vae} предлагается обобщение автокодировщика с использованием вариационного байесовского вывода~\cite{bishop}. В работе~\cite{train_generative} рассматриваются модификации вариационного автокодировщика и ступенчатых сетей (англ. ladder network)~\cite{ladder} для случая построения многослойных порождающих моделей. 

В качестве критерия выбора модели в ряде работ~\cite{mackay,bishop,tokmakova,zaitsev,strijov_webber, strijov_dsc} выступает обоснованность модели. В работах~\cite{tokmakova,zaitsev,strijov_webber, strijov_dsc} рассматривается проблема выбора модели и оценки гиперпараметров в задачах регрессии. Альтернативным критерием выбора модели является минимальная длина описания~\cite{mdl}, являющаяся показателем статистической сложности модели и заданной выборки. 
В работе~\cite{perekrestenko} рассматривается перечень критериев сложности моделей глубокого обучения и их взаимосвязь. В работе~\cite{vladis} в качестве критерия сложности модели выступает показатель нелинейности, характеризуемый степенью полинома Чебышева, аппроксимирующего функцию. В работе~\cite{need_prune} анализируется показатель избыточности параметров сети. Утверждается, что по небольшому набору параметров в глубокой сети с большим количеством избыточных параметров можно спрогнозировать значения остальных. В работе~\cite{rob} рассматривается показатель робастности моделей, а также его взаимосвязь с топологией выборки и классами функций, в частности рассматривается влияние функции ошибки и ее липшицевой константы на робастность моделей. Схожие идеи были рассмотрены в работе~\cite{intrig}, в которой исследуется устойчивость классификации модели под действием шума. 

Одним из методов получения приближенного значения обоснованности является вариационный метод получения нижней оценки интеграла~\cite{bishop}. В работе~\cite{hoffman} рассматривается стохастическая версия вариационного метода. В работе~\cite{nips} рассматривается алгоритм получения вариационной нижней оценки обоснованности  для оптимизации гиперпараметров моделей глубокого обучения. В работе~\cite{varmc} рассматривается получение вариационной нижней оценки интеграла с использованием модификации методов Монте-Карло. В работе~\cite{early} рассматривается стохастический градиентный спуск в качестве оператора, порождающего распределение, аппроксимирующее апостериорное распределение параметров модели. Схожий подход рассматривается в работе~\cite{sgd_cont}, где также рассматривается стохастический градиентный спуск в качестве оператора, порождающего апостериорное распределение параметров. В работе~\cite{langevin} предлагается модификация стохастического градиентного спуска, аппроксимирующая апостериорное распределение. 

%Альтернативным методом выбора модели является выбор модели на основе скользящего контроля~\cite{cv_ms, tokmakova}. Проблемой такого подхода является возможная высокая вычислительная сложность~\cite{expensive, expensive2}. В работах~\cite{bias,bias2} рассматривается проблема смещения оценок качества модели при гиперпараметрах, получаемых с использованием $k$-fold метода скользящего контроля, при котором выборка делится на $k$ частей с обучением на $k-1$ части и валидацией результата на оставшейся части выборки. 

Задачей, связанной с проблемой выбора модели, является задача оптимизации гиперпараметров~\cite{mackay,bishop}. В работе~\cite{tokmakova} рассматривается оптимизация гиперпараметров с использованием метода скользящего контроля и методов оптимизации обоснованности моделей, отмечается низкая скорость сходимости гиперпараметров при использовании метода скользящего контроля. В ряде работ~\cite{hyper, hyper2} рассматриваются градиентные методы оптимизации гиперпараметров, позволяющие оптимизировать большое количество гиперпараметров одновременно. В работе~\cite{hyper} предлагается метод оптимизации гиперпараметров с использованием градиентного спуска с моментом, в качестве оптимизируемого функционала рассматривается ошибка на валидационной части выборки. В работе~\cite{approx_hyper} предлагается метод аппроксимации градиента функции потерь по гиперпараметрам, позволяющий использовать градиентные методы в задаче оптимизации гиперпараметров на больших выборках. В работе~\cite{greed_hyper} предлагается упрощенный метод оптимизации гиперпараметров с градиентным спуском: вместо всей истории обновлений параметров для оптимизации используется только последнее обновление. В работе~\cite{sgd_cont} рассматривается задача оптимизации параметров градиентного спуска с использованием нижней вариационной оценки обоснованности. 

\vspace{0.5cm}
\textbf{Цели работы.}
\vspace{0.2cm}
\begin{enumerate}
\item Исследовать методы построения моделей глубокого обучения оптимальной и субоптимальной сложности.
%\item Проанализировать различные подходы к решению задачи автоматического построения моделей глубокого обучения и оптимизации параметров модели.
\item Предложить критерии оптимальной и субоптимальной сложности модели глубокого обучения.
\item Предложить метод выбора субоптимальной структуры модели глубокого обучения.
\item Предложить алгоритм построения модели субоптимальной сложности и оптимизации ее параметров.
%\item Предложить алгоритм построения модели субоптимальной сложности и оптимизации параметров модели и
\end{enumerate}


\vspace{0.5cm}
\textbf{Методы исследования.} Для достижения поставленных целей используются методы вариационного байесовского вывода~\cite{mackay, bishop, early}. Рассматривается графовое представление нейронной сети~\cite{reinf,darts}. Для получения вариационных оценок обоснованности модели используется метод, основанный на градиентном спуске~\cite{sgd_cont, early}. В качестве метода получения модели субоптимальной сложности используется метод автоматического определения релевантности параметров~\cite{mackay,vae_ard} с использованием градиентных методов оптимизации гиперпараметров~\cite{hyper, hyper2, greed_hyper, approx_hyper}.

\vspace{0.5cm}
\textbf{Основные положения, выносимые на защиту.}
\vspace{0.3cm}
\begin{enumerate}
\item Предложен метод байесовского выбора оптимальной и субоптимальной структуры модели глубокого обучения с использованием автоматического определения
релевантности параметров.
\item Предложены критерии оптимальной и субоптимальной сложности модели глубокого обучения.
\item Предложен метод графового описания моделей глубокого обучения.
\item Предложено обобщение задачи оптимизации структуры модели, включающее ранее описанные методы выбора модели: оптимизация обоснованности модели, последовательное увеличение сложности модели, последовательное снижение сложности модели, полный перебор вариантов структуры модели.
\item Предложен метод оптимизации вариационной оценки обоснованности модели на основе метода мультистарта задачи оптимизации.
\item Предложен алгоритм оптимизации параметров, гиперпараметров и структурных параметров моделей глубокого обучения.
\item Исследованы свойства оптимизационной задачи при различных значениях метапараметров. Рассмотрены ее асимптотические свойства.
\end{enumerate}


\vspace{0.5cm}
\textbf{Научная новизна.} Разработан новый подход к построению моделей глубокого обучения. Предложены критерии субоптимальной и оптимальной сложности модели, а также исследована их связь. Предложен метод построения модели глубокого обучения субоптимальной сложности. Исследованы методы оптимизации гиперпараметров и параметров модели.  Предложена обобщенная задача выбора модели глубокого обучения.

\vspace{0.5cm}
\textbf{Теоретическая значимость.} В целом, данная диссертационная работа носит теоретический характер. В работе предлагаются критерии субоптимальной и оптимальной сложности, основанные на принципе минимальной длины описания. Исследуется взаимосвязь критериев оптимальной и субоптимальной сложности. Предлагаются градиентные методы для получения оценок сложности модели. Доказывается теорема об оценке энтропии эмпирического распределения параметров модели, полученных под действием оператора оптимизации.
Доказывается теорема об обобщенной задаче выбора модели глубокого обучения.


\vspace{0.5cm}
\textbf{Практическая значимость.} Предложенные в работе методы предназначены для построения моделей глубокого обучения в прикладных задачах регрессии и классификации; оптимизации гиперпараметров полученной модели; выбора модели из конечного множества заданных моделей; получения оценок переобучения модели.


\vspace{0.5cm}
\textbf{Степень достоверности и апробация работы.} Достоверность результатов подтверждена математическими доказательствами, экспериментальной проверкой полученных методов на реальных задачах выбора моделей глубокого обучения; публикациями результатов исследования в рецензируемых научных изданиях, в том числе рекомендованных ВАК. Результаты работы докладывались и обсуждались на следующих научных конференциях.
\begin{enumerate}
\item ``Восстановление панельной матрицы и ранжирующей модели в разнородных шкалах'', Всероссийская конференция <<57-я научная конференция МФТИ>>, 2014.
\item ``A monolingual approach to detection of text reuse in Russian-English collection'', Международная конференция <<Artificial Intelligence and Natural Language Conference>>, 2015~\cite{monolingual}.
\item ``Выбор модели глубокого обучения субоптимальной сложности с использованием вариационной оценки правдоподобия'', Международная конференция <<Интеллектуализация обработки информации>>, 2016~\cite{ioi16}.
\item ``Machine-Translated Text Detection in a Collection of Russian
Scientific Papers'', Международная конференция по компьютерной лингвистике и интеллектуальным технологиям <<Диалог-21>>, 2017~\cite{dialog}.
\item ``Author Masking using Sequence-to-Sequence Models'', Международная конференция <<Conference and Labs of the Evaluation Forum>>, 2017~\cite{pan_s2s}.
\item ``Градиентные методы оптимизации гиперпараметров моделей глубокого обучения'', Всероссийская конференция <<Математические методы распознавания образов ММРО>>, 2017~\cite{mmro17_hyper}.
\item ``Детектирование переводных заимствований в текстах научных статей из журналов, входящих в РИНЦ'', Всероссийская конференция <<Математические методы распознавания образов ММРО>>, 2017~\cite{mmro17_plag}.
\item ``ParaPlagDet: The system of paraphrased plagiarism detection'', Международная конференция <<Big Scholar at conference on knowledge discovery and data mining>>, 2018.
\item ``Байесовский выбор наиболее правдоподобной структуры модели глубокого обучения'', Международная конференция <<Интеллектуализация обработки информации>>, 2018~\cite{ioi18}.
\item ``Variational learning across domains with triplet
information'', Международная конференция <<Visually Grounded Interaction and Language workshop, Conference on Neural Information Processing Systems>>, 2018.
\end{enumerate}

%Работа поддержана грантами Российского фонда фундаментальных исследований.
%\begin{enumerate}
%\item 19-07-00875, Развитие методов автоматического построения и выбора вероятностных моделей субоптимальной сложности в задачах глубокого обучения.
%\item 16-37-00488, Разработка алгоритмов построения сетей глубокого обучения как суперпозиций универсальных моделей.
%\item 16-07-01158, Развитие теории построения суперпозиций универсальных моделей классификации сигналов.
%\item 14-07-3104,  Построение и анализ моделей классификации для выборок малой мощности.
%\end{enumerate}

\vspace{0.5cm}
\textbf{Публикации по теме диссертации.} Основные результаты по теме диссертации изложены в 11 печатных изданиях, 9 из которых изданы в журналах, рекомендованных ВАК.
%\end{enumerate}


\vspace{0.5cm}
\textbf{Личный вклад.} Все приведенные результаты, кроме отдельно оговоренных случаев, получены диссертантом лично при научном руководстве д.ф.-м.н. В. В. Стрижова.


\vspace{0.5cm}
\textbf{Структура и объем работы.} Диссертация состоит из оглавления, введения, четырех разделов, заключения, списка иллюстраций, списка таблиц, перечня основных обозначений и списка литературы из 162 наименований. Основной текст занимает 144 страницы.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Основное содержание работы}
Во \textbf{введении} обоснована актуальность диссертационной работы, сформулированы цели и методы исследования, поставлены основные задачи, обоснована научная новизна, теоретическая и практическая значимость полученных результатов.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
В \textbf{главе~1} приводится формальная постановка задачи выбора модели глубокого обучения. Вводятся основные определения и обозначения, функции качества модели глубокого обучения, описывается вероятностная интерпретация модели.

Проблема выбора структуры модели глубокого обучения формулируется следующим образом: решается задача классификации или регрессии на заданной или пополняемой выборке $\mathfrak{D}$. Требуется выбрать структуру нейронной сети, доставляющей минимум ошибки на этой функции и максимум качества на некотором внешнем критерии.
 Под моделью глубокого обучения понимается суперпозиция дифференцируемых по параметрам нелинейный функций. Под структурой модели понимается значения структурных параметров модели, т.е. величин, задающих вид итоговой суперпозиции. 

Формализуем описанную выше задачу.
\begin{defin}
\textit{Объектом} назовем пару $(\x, y), \x \in \Xb =  \mathbb{R}^n, y \in \yb$. В случае задачи классификации $\yb$ является распределением вероятностей принадлежности объекта $\mathbf{x} \in \mathbb{X}$ множеству классов $\{1, \dots, R\}$: $\yb \subset [0,1]^R$, где $R$ --- число классов. В случае задачи регрессии $\yb$ является некоторым подмножеством вещественных чисел ${y} \in \yb  \subseteq \mathbb{R}$.
Объект состоит из двух частей: $\mathbf{x}$  соответствует \textit{признаковому описанию объекта}, $y$ --- \textit{метке объекта}.
\end{defin}

Задана простая выборка \begin{equation}\label{eq:dataset}\mathfrak{D} = \{(\mathbf{x}_i,y_i)\}, i = 1,\dots,m,\end{equation} состоящая из множества объектов $$\mathbf{x}_i \in \mathbf{X} \subset  \mathbb{X}, \quad {y}_i \in \mathbf{y} \subset \yb.$$ 


\begin{defin}
\textit{Моделью} $\mathbf{f}(\mathbf{w}, \mathbf{x})$ назовем дифференцируемую по параметрам $\mathbf{w}$ функцию из множества признаковых описаний объекта во множество меток:
\[
    \mathbf{f}:  \mathbb{W}  \times \mathbb{X} \to \yb,
\] 
где $\mathbb{W}$ --- пространство параметров функции $\mathbf{f}$.
\end{defin}
Специфика задачи  выбора модели \textit{глубокого обучения} заключается в том, что модели глубокого обучения могут иметь значительное число параметров, что приводит к неприменимости ряда методов оптимизации и выбора модели. 
Перейдем к формальному описанию параметрического семейства моделей глубокого обучения. 
\begin{defin}
Пусть задан ациклический граф $(V,E)$, такой что 
\begin{enumerate}
\item для каждого ребра $(j,k) \in E$: вектор базовых дифференцируемых функций  $\mathbf{g}^{j,k} = [\mathbf{g}^{j,k}_0, \dots, \mathbf{g}^{j,k}_{K^{j,k}-1}]$  мощности $K^{j,k}$;
\item для каждой вершины $v \in V$: дифференцируемая функция агрегации $\textbf{agg}_v$.
\item Функция $\mathbf{f} = \mathbf{f}_{|V|-1}$, задаваемая по правилу 
\begin{equation}
\label{eq:modelfam}
    \mathbf{f}_{k}(\mathbf{w}, \mathbf{x}) = \textbf{agg}_{k}\left(\{ \langle \boldsymbol{\gamma}^{j,k}, \mathbf{g}^{j,k} \rangle \circ  \mathbf{f}_j(\mathbf{x})| j \in \text{Adj}(v_k)\}\right), 
\end{equation}
\[
k \in \{1,\dots,|V|-1\}, \quad \mathbf{f}_0(\mathbf{x}) = \mathbf{x}, \quad v_k \in V. 
\]
и являющаяся функцией из признакового пространства $\mathbb{X}$ в пространство меток $\yb$ при значениях векторов, $\boldsymbol{\gamma}^{j,k} \in [0,1]^{K^{j,k}}$.
\end{enumerate}

Граф $(V, E)$ со множеством векторов базовых функций $\{\mathbf{g}^{j,k}, (j,k) \in E\}$ и функций агрегаций $\{\textbf{agg}_k\},$ где $k \in \{0, \dots, |V|-1\},$ назовем \textit{параметрическим семейством моделей} $\mathfrak{F}$.
\end{defin}

Примером функций агрегации выступают функции суммы и конкатенации векторов.

\begin{defin}
Функции $\mathbf{f}_0, \dots, \mathbf{f}_{|V|-1}$ из~\eqref{eq:modelfam} назовем \textit{слоями или подмоделями} модели $\mathbf{f}$.
\end{defin}

\begin{utv}
Для любого значения $\boldsymbol{\gamma}^{j,k} \in [0,1]^{K^{j,k}}$ функция $\mathbf{f} \in \mathfrak{F}$ является моделью.
\end{utv}



\begin{defin}
\textit{Параметрами }модели $\mathbf{f}$ из параметрического семейства моделей $\mathfrak{F}$  назовем конкатенацию векторов параметров всех базовых функций $\{\mathbf{g}^{j,k}| {(j,k) \in E} \}, \mathbf{w} \in \mathbb{W}.$ Вектор параметров базовой функции $\mathbf{g}^{j,k}_l$ будем обозначать как $\mathbf{w}^{j,k}_l$.
\end{defin}



\begin{defin}
Структурой $\boldsymbol{\Gamma}$  модели $\mathbf{f}$ из параметрического семейства моделей $\mathfrak{F}$  назовем конкатенацию векторов $\boldsymbol{\gamma}^{j,k}$. Множество всех возможных значений структуры $\boldsymbol{\Gamma}$ будем обозначать как $\amsmathbb{\Gamma}$.
Векторы $\boldsymbol{\gamma}^{j,k}, (j,k) \in E$ назовем \textit{структурными параметрами модели.}
\end{defin}

\begin{defin}
\textit{Параметризацией }множества моделей $M$ назовем параметрическое семейство моделей $\mathfrak{F}$, такое что для каждой модели $\mathbf{f} \in M$ существуют значение структуры модели $\boldsymbol{\Gamma}$ при котором функция $\mathbf{f}$ совпадает с функцией~\eqref{eq:modelfam}.
\end{defin}

Рассмотрим варианты ограничений, которые накладываются на структурные параметры $\boldsymbol{\gamma}^{j,k}$ параметрического семейства моделей. Цель данных ограничений --- уточнение архитектуры модели глубокого обучения, которую требуется получить. 
\begin{enumerate}
\item Структурные параметры лежат на веришнах булевого куба: $\boldsymbol{\gamma}^{j,k} \in \{0,1\}^{K^{j,k}}$. Структурные параметры $\boldsymbol{\gamma}^{j,k}$ интерпретируются как параметр включения или выключения компонент вектора базовых функций $\mathbf{g}^{j,k}$ в итоговую модель.
\item Структурные параметры лежат внутри булевого куба: $\boldsymbol{\gamma} \in [0,1]^{K^{j,k}}$. Релаксированная версия предыдущих ограничений, позволяющая проводить градиентную оптимизацию для структурных параметров.
\item Структурные параметры лежат на веришнах симплекса: $\boldsymbol{\gamma}^{j,k} \in \bar{\Delta}^{K^{j,k}-1}$. Каждый вектор структурных параметров $\boldsymbol{\gamma}^{j,k}$ имеет только одну ненулевую компоненту, определяющую какая из базовых функций $\mathbf{g}^{j,k}$ войдет в итоговую модель. Примером параметрического семейства моделей, требующим такое ограничение является семейство полносвязанных нейронных сетей с одним скрытым слоем и двумя значениями количества нейронов на скрытом слое. Схема семейства представлена на Рис.~\ref{fig:scheme_mlp}. Данное семейство можно представить как семейство с двумя базовыми функциями вида $\mathbf{g} = \boldsymbol{\sigma}(\mathbf{w}^{\T}\mathbf{x}),$ где матрицы параметров каждой из функций  $\mathbf{g}^{1,1}, \mathbf{g}^{1,2}$ имеют фиксированное число нулевых столбцов. Количество этих столбцов определяет размерность итогового скрытого пространства или числа нейронов на скрытом слое.
\item  Структурные параметры лежат внутри симплекса: $\boldsymbol{\gamma}^{j,k} \in {\Delta}^{K^{j,k}-1}$. Релаксированная версия предыдущих ограничений, позволяющая проводить градиентную оптимизацию для структурных параметров. Значение стуктурных параметров $\boldsymbol{\gamma}^{j,k}$ интерпретируются как вклад каждой компоненты вектора базовых функций $\mathbf{g}^{j,k}$ в итоговую модель. 
\end{enumerate}

В данной работе рассматривается случай, когда на структурные параметры наложено ограничение 4.  Данные ограничения позволяют решать задачу выбора модели как для семейства моделей типа многослойных полносвязных нейронных сетей, так и для более сложных параметрических семейств~\cite{darts}. 

Для дальнейшей постановки задачи введем понятие вероятностной модели, и связанных с ним определений. Будем полагать, что для параметров модели $\mathbf{w}$ и структуры  $\boldsymbol{\Gamma}$ задано распределение $\prior$, соответствующее предположениям о распределении структуры и параметров. 

\begin{defin}
\textit{Гиперпараметрами} $\mathbf{h}\in \mathbb{H}$ модели  назовем параметры распределения $\prior$.
\end{defin}

\begin{defin}
\textit{Априорным распределением} параметров и структуры модели назовем вероятностное распределение, соответствующее предположениям о распределении параметров модели:
\[
    \prior: \mathbb{W} \times \Gb \to \mathbb{R}^{+}, 
\]
где $\mathbb{W}$ --- множество значений параметров модели, $\Gb$ --- множество значений структуры модели.% $\Hb$ --- множество значений гиперпараметров, $\Lamb$ -- множество значений метапараметров. Формальное определение последних будет дано далее.
Формальное определение метапараметров $\lam \in \Lamb$ будет дано далее.
\end{defin}


Одной из постановок задачи выбора структуры модели является \textit{двусвязный байесовский вывод.} 
На \textit{первом уровне} байесовского вывода  находится апостериорное распределение параметров.

\begin{defin}
\textit{Апостериорным распределением} назовем распределение вида
\begin{equation}
\label{eq:posterior}
    \post = \frac{\LL \prior}{\EV} \propto \LL \prior.% \mathbb{W} \times \Delta{\boldsymbol{\Gamma}} \times \mathbb{X}  \times \mathbb{H}. 
\end{equation}
\end{defin}

\begin{defin}
\textit{Вероятностной моделью глубокого обучения} назовем совместное распределение вида
\[
    p(\y, \mathbf{w},  \boldsymbol{\Gamma}|\X, \mathbf{h}, \lam) = p(\y|\X, \mathbf{w},  \boldsymbol{\Gamma}, \lam)\prior: {\yb}^m \times \mathbb{W}  \times \Gb  \to \mathbb{R}^{+}.
\]
\end{defin}

\begin{defin}
\textit{Функцией правдоподбия выборки } назовем величину
\[
    \LL : {\yb}^m \to \mathbb{R}^{+}.
\]
\end{defin}
%Для каждой модели определена функция обоснованности  $\EV$.
 
На \textit{втором уровне} байесовского вывода осуществляется выбор модели на основе обоснованности модели.
\begin{defin}
\textit{Обоснованностью модели }назовем величину
\begin{equation}
\label{eq:evidence}
\EV = \iint_{\mathbf{w}, \boldsymbol{\Gamma}} \LL \prior  d\mathbf{w}d\boldsymbol{\Gamma}.
\end{equation}
\end{defin}
Получение значений апостериорного распределения и обоснованности модели сетей глубокого обучения является вычислительно сложной процедурой. Для получения оценок на данные величины используют методы, такие как аппроксимация Лапласа~\cite{tokmakova} и вариационная нижняя оценка~\cite{nips}.  В данной работе в качестве метода получения оценок обоснованности модели выступает вариационная нижняя оценка.

\begin{defin}
\textit{Вариационным распределением} назовем параметрическое распределение $\q$, являющееся приближением  апостериорного распределения параметров и структуры $\post.$ 
\end{defin}

\begin{defin}
\textit{Вариационными параметрами} модели $\teta \in \Tetab$ назовем параметры вариационного распределения $\q$.
\end{defin} 

\begin{defin}
\label{def:l}
Пусть задано вариационное распределения $\q$.
\textit{Функцией потерь} $\Loss$ для модели $\model$ назовем дифференцируемую функцию, принимаемую за качество модели на обучающей выборки при параметрах модели, получаемых из  распределения $q$.
\end{defin}

В качестве функции $\Loss$ может выступать логарифм правдоподобия выборки $\log\LL$  и логарифм апостериорной вероятности $\log\post$ параметров и структуры модели на обучающей выборке.

\begin{defin}
\label{def:q}
Пусть задано вариационное распределения $\q$ и функция потерь $\Loss$. 
\textit{Функцией валидации} $\Val$ для модели $\model$ назовем дифференцируемую функцию, принимаемую за качество модели при векторе $\teta$, заданном неявно.
\end{defin}

 

В данной работе задача выбора структуры модели и параметров модели ставится как двухуровневая задача оптимизации:
\begin{equation}
\label{eq:optim_problem}
	\h^{*} = \argmax_{\h  \in \Hb} \Val[][][][\teta^{*}],
\end{equation}
где $\teta^{*}$ --- решение задачи оптимизации
\begin{equation}
\label{eq:optim_problem_in}
   \teta^{*} = \argmax_{\teta \in \Tetab} \Loss.
\end{equation}

\begin{defin}
\textit{Задачей выбора модели} $\model$ назовем   двухуровневую задачу оптимизации~\eqref{eq:optim_problem},\eqref{eq:optim_problem_in}.
\end{defin}

Методы, используемые для оптимизации гиперпараметров моделей глубокого обучения должны быть эффективными по вычислительным затратам в силу высокой вычислительной сложности оптимизации параметров модели. 
В~\cite{random1,random2} рассматривается задача оптимизации гиперпараметров стохастическими методами. В~\cite{random1} проводится сравнение случайного поиска значений гиперпараметров с переборным алгоритмом. В~\cite{random2} производится сравнение случайного поиска и алгоритмов, основанных на вероятностных моделях.

\textbf{Градиентные методы оптимизации гиперпараметров. } 
\begin{defin} Назовем \textit{оператором оптимизации} алгоритм $T$ выбора вектора параметров $\boldsymbol{\theta}'$  по параметрам предыдущего шага $\boldsymbol{\theta}$:
\begin{equation}
\label{eq:optim_operator}
	\teta' = \TLoss,
\end{equation}
где $\boldsymbol{\lambda}$ --- параметры оператора оптимизации или \textit{метапараметры}.
\end{defin}
Метапараметры соответствуют параметрам оптимизации, т.е. параметрам, которые не подлежат оптимизации в ходе задачи выбора модели. 

Пример схожего описания оптимизации модели с использованием оператора оптимизации можно найти в~\cite{early}.

Частным случаем оператора оптимизации является оператор стохастического спуска:
\begin{equation}
\label{eq:sgd_operator}
    \TLoss  = \boldsymbol{\theta} - \lambda_{\text{lr}} \nabla (-\Loss),
\end{equation}
где $\lambda_{\text{lr}}$ --- шаг градиентного спуска, $\hat{\mathbf{y}}, \hat{\mathbf{X}}$ --- случайная подвыборка заданной мощности выборки $\mathfrak{D}$.

В случае оптимизации гиперпараметров оператор оптимизации применяется не к вариационным параметрам $\boldsymbol{\theta}$, а к гиперпараметрам $\mathbf{h}$:
\begin{equation}
\label{eq:hyperoptim_operator}
    \h = \TVal.
\end{equation}

В случае, если для решения задачи~\eqref{eq:optim_problem_in} применяется несколько шагов оператора оптимизации~\eqref{eq:optim_operator},
$\boldsymbol{\theta}^{*}$ рассматривается как рекурсивная функция от начального приближения вариационных параметров $\boldsymbol{\theta}^{0}$ и вектора гиперпараметров $\mathbf{h}$:
\begin{equation}
\label{eq:param_trace}
    \boldsymbol{\theta}^{*} = T \circ \dots \circ \TLoss = \boldsymbol{\theta}^{*}(\boldsymbol{\theta}^{0}, \mathbf{h}).
\end{equation}

Решение задачи оптимизации~\eqref{eq:hyperoptim_operator} при~\eqref{eq:param_trace} является вычислительно сложным, поэтому применяются методы, аппроксимирующие применение градиентных методов при~\eqref{eq:param_trace}.


\vspace{0.5cm}
\textbf{В заключении} представлены основные результаты диссертационной работы.

\begin{enumerate}
\item Предложен метод байесовского выбора оптимальной и субоптимальной структуры модели глубокого обучения с использованием автоматического определения релевантности параметров.
\item Предложены критерии оптимальной и субоптимальной сложности модели глубокого обучения.
\item Предложен метод графового описания моделей глубокого обучения.
Предложено обобщение задачи оптимизации структуры модели, включающее ранее описанные методы выбора модели: оптимизация обоснованности модели, последовательное увеличение сложности модели, последовательное снижение сложности модели, полный перебор вариантов структуры модели.
\item Предложен метод оптимизации вариационной оценки обоснованности модели на основе метода мультистарта задачи оптимизации.
\item Предложен алгоритм оптимизации параметров, гиперпараметров и структурных параметров моделей глубокого обучения.
\item Исследованы свойства оптимизационной задачи при различных значениях метапараметров. Рассмотрены ее асимптотические свойства.

\end{enumerate}

\subsection*{Публикации соискателя по теме диссертации}
Публикации в журналах из списка ВАК.
\vspace{0.3cm}

\begin{enumerate}
\item Бахтеев О.Ю., Попова М.С., Стрижов В.В., “Системы и средства глубокого обучения в задачах классификации”, Системы и средства информатики, 26:2 (2016), 4–22~\cite{popova2}.
\item Bakhteev, O., Kuznetsova, R., Romanov, A. and Khritankov, A., 2015, November. A monolingual approach to detection of text reuse in Russian-English collection. In 2015 Artificial Intelligence and Natural Language and Information Extraction, Social Media and Web Search FRUCT Conference (AINL-ISMW FRUCT) (pp. 3-10). IEEE~\cite{monolingual}.
\item Romanov, A., Kuznetsova, R., Bakhteev, O. and Khritankov, A., 2016. Machine-Translated Text Detection in a Collection of Russian Scientific Papers. Computational Linguistics and Intellectual Technologies. 2016~\cite{dialog}. 
\item Bakhteev, O. and Khazov, A., 2017. Author Masking using Sequence-to-Sequence Models. In CLEF (Working Notes). 2017~\cite{pan_s2s}.
\item Бахтеев О.Ю., Стрижов В.В., “Выбор моделей глубокого обучения субоптимальной сложности”, Автоматика и телемеханика, 2018, № 8, 129–147; Automation Remote Control, 79:8 (2018), 1474–1488~\cite{var_ait}.
\item Огальцов А.В., Бахтеев О.Ю., “Автоматическое извлечение метаданных из научных PDF-документов”, Информатика и её применения, 12:2 (2018), 75–82~\cite{ogaltsov}.
\item Смердов А.Н., Бахтеев  О.Ю., Стрижов В.В., “Выбор оптимальной модели рекуррентной сети в задачах поиска парафраза”, Информатика и её применения, 12:4 (2018), 63–69~\cite{smerdov}.
\item Грабовой А.В., Бахтеев О.Ю., Стрижов В.В. “Определение релевантности параметров нейросети”, Информатика и её применения. 13:2 (2019), 62-71~\cite{grabovoy}.
\item Bakhteev, O.Y. and Strijov, V.V., 2019. Comprehensive analysis of gradient-based hyperparameter optimization algorithms. Annals of Operations Research, pp.1-15~\cite{hyper_bakhteev}.
\end{enumerate}

{Остальные публикации.}
\vspace{0.3cm}
\begin{enumerate}
\setcounter{enumi}{9}
\item Бахтеев О.Ю. Восстановление панельной матрицы и ранжирующей модели по метризованной выборке в разнородных данных. // Машинное обучение и анализ данных. 2016. № 7. С. 72-77~\cite{panel}.
\item Бахтеев О.Ю. Восстановление пропущенных значений в разнородных шкалах с большим числом пропусков. // Машинное обучение и анализ данных. 2015. № 11. С. 1-11~\cite{knn}.
\end{enumerate}

\end{document}
