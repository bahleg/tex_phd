\documentclass[11pt, a5paper]{dissert}

\usepackage{cmap}
\usepackage[T2A]{fontenc}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
\usepackage{tabularx}
\usepackage[noend]{algorithmic}
\usepackage{tikz}
\usepackage{tikz,fullpage}
\usepackage{lastpage}
\usepackage[shortlabels]{enumitem}
\usepackage{subcaption}
\usepackage{mathbbol}
\usepackage{amssymb}
\usepackage{xargs}   
\usepackage{microtype} % немного поджимает текст. Если не компилится - нужно поставить cm-super (ставится через apt-get )

\DeclareUnicodeCharacter{00A0}{ } % При наборе текста с планшета появляются невидимые символы. ЭТо костыль.
\DeclareSymbolFontAlphabet{\mathbb}{AMSb}%
\DeclareSymbolFontAlphabet{\amsmathbb}{bbold}%

%https://tex.stackexchange.com/questions/163451/total-number-of-citations
\usepackage{totcount}
\newtotcounter{citnum} %From the package documentation
\def\oldbibitem{} \let\oldbibitem=\bibitem
\def\bibitem{\stepcounter{citnum}\oldbibitem}


\usetikzlibrary{arrows,automata}
\usetikzlibrary{positioning}


\def\algorithmicrequire{\textbf{Вход:}}
\def\algorithmicensure{\textbf{Выход:}}
\def\algorithmicif{\textbf{если}}
\def\algorithmicthen{\textbf{то}}
\def\algorithmicelse{\textbf{иначе}}
\def\algorithmicelsif{\textbf{иначе если}}
\def\algorithmicfor{\textbf{для}}
\def\algorithmicforall{\textbf{для всех}}
\def\algorithmicdo{}
\def\algorithmicwhile{\textbf{пока}}
\def\algorithmicrepeat{\textbf{повторять}}
\def\algorithmicuntil{\textbf{пока}}
\def\algorithmicloop{\textbf{цикл}}
\def\algorithmicreturn{\textbf{вернуть}}
% переопределение стиля комментариев
\def\algorithmiccomment#1{\quad// {\sl #1}}


%\usepackage[cp1251]{inputenc}
\renewcommand{\rmdefault}{cmr}
\usepackage{dsfont} % for indicator function
\usepackage[russian]{babel}

\usepackage{color}
\usepackage{caption}

\usepackage{float}
%\usepackage{slashbox}
%пакеты и команды, необходимость в которых может возникнуть по ходу работы (Вы можете добавлять свои):
\usepackage{indentfirst}%для отступов
    \usepackage{subfig}

\usepackage{geometry}
\geometry{left=2.5cm}
\geometry{right=1.0cm}
\geometry{top=2.0cm}
\geometry{bottom=2.0cm}
\renewcommand{\baselinestretch}{1.0}

%\usepackage[backend=biber,style=alphabetic]{biblatex}    

%\usepackage{geometry}
%\emergencystretch=25pt%для борьбы с переполнениями за счет разреж. слов в абзаце
%\righthyphenmin=2% для разрешения переноса двух последних букв
%\arrayrulewidth=.75pt% регулируем толщину линий в табл.
%\usepackage[dvips]{graphicx}%для включения PS файлов
%\usepackage[final]{epsfig}
\usepackage{multicol}%для организации многоколоночного текста (предм. указатель)
%\usepackage{subcaption}

\usepackage{indentfirst}
\usepackage{amsmath}
%\usepackage{enumerate}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amscd}
\usepackage{hhline}
%\usepackage{multirow}
\usepackage{graphicx, epsfig}
%\usepackage{epic}
\usepackage{amscd}
%\usepackage{ecltree}
\usepackage{color}
\usepackage[dvips,all]{xy}
%\usepackage{setspace}
%\usepackage{makeidx}
%пакет для вкл символов напр номер
%\usepackage{textcomp}
%\usepackage{amsfonts}
%\usepackage{afterpage}% Полезно для полного заполнения страниц перед большими таблицами
%\usepackage{longtable}%Для таблиц на нескольких страницах
%\usepackage{cite}
%\usepackage{rawfonts}
%\usepackage{oldlfont}%доступ к шрифтам через устаревшие команды
\usepackage{array}
%\renewcommand{\bibname}{Список литературы}
\renewcommand{\contentsname}{Содержание}
\renewcommand{\contentsdesc}{Стр.}
\renewcommand{\chaptername}{Глава}


%%% Библиография %%%
\makeatletter
\bibliographystyle{utf8gost71u}     % Оформляем библиографию по ГОСТ 7.1 (ГОСТ Р 7.0.11-2011, 5.6.7)
%\renewcommand{\@biblabel}[1]{#1.}   % Заменяем библиографию с квадратных скобок на точку
\makeatother

%\def\BibUrl#1.{}
%\bibliographystyle{gost2008} 
\theoremstyle{definition}
\newtheorem{theorem}{Теорема}
\newtheorem{lemma}{Лемма}
\newtheorem{defin}{Определение}
\newtheorem{utv}{Утверждение}
\newtheorem{example}{Пример}


\def\bbljan{Январь}
\def\bblfeb{Февраль}
\def\bblmar{Март}
\def\bblapr{Апрель}
\def\bblmay{Май}
\def\bbljun{Июнь}
\def\bbljul{Июль}
\def\bblaug{Август}
\def\bblsep{Сентябрь}
\def\bbloct{Октябрь}
\def\bblnov{Ноябрь}
\def\bbldec{Декабрь}


\renewcommand{\thesubfigure}{\asbuk{subfigure}}

\graphicspath{{pic/}}
 
%smerdov

\newcommand{\RR}{\mathbb{R}}
\newcommand{\PP}{p}
\newcommand{\DD}{{\mathfrak{D}}}
\newcommand{\FF}{{\mathcal{F}}}
%\newcommand{\AAA}{{\mathcal{A}}}
\newcommand{\FFF}{{\mathfrak{F}}}
\newcommand{\NNN}{{\mathcal{N}}}
\newcommand{\WW}{{\mathbb{W}}}
\newcommand{\bw}{{\textbf{w}}}
\newcommand{\ba}{{\textbf{a}}}
\newcommand{\bb}{{\textbf{b}}}
\newcommand{\bx}{{\textbf{x}}}
\newcommand{\II}{{\textbf{I}}}
%\newcommand{\bmu}{{\boldsymbol{\mu}}}
\newcommand{\bbf}{{\textbf{f}}}
\newcommand{\by}{{\textbf{y}}}
\newcommand{\bm}{{\textbf{m}}}
\newcommand{\bW}{{\textbf{W}}}
\newcommand{\bWs}{{\textbf{W}_{\textbf{s}}}}
\newcommand{\bU}{{\textbf{U}}}
\newcommand{\bV}{{\textbf{V}}}
\newcommand{\bh}{{\textbf{h}}}
\newcommand{\bu}{{\textbf{u}}}
\newcommand{\bbW}{{\textbf{b}_{\textbf{W}}}}
\newcommand{\bbU}{{\textbf{b}_{\textbf{U}}}}
\newcommand{\bbV}{{\textbf{b}_{\textbf{V}}}}
\newcommand{\bs}{{\boldsymbol{\sigma}}}
\newcommand{\al}{{\alpha}}
\newcommand{\bal}{\boldsymbol{\alpha}}
\newcommand{\bbt}{\boldsymbol{\beta}}
%\newcommand{\bA}{\mathbf{A^\text{-1}}}
\newcommand{\bApr}{\mathbf{A^\text{-1}_{\text{pr}}}}
\newcommand{\bAps}{\mathbf{A^\text{-1}_{\text{ps}}}}
\newcommand{\bmupr}{\boldsymbol{\mu}}
\newcommand{\bmups}{\textbf{m}}
\newcommand{\DKL}{\mathit{D}_{\text{KL}}}


% opers
\DeclareMathOperator*{\indicator}{\mathds{1}}
\DeclareMathOperator*{\softmax}{softmax}
\DeclareMathOperator*{\idx}{idx}
\DeclareMathOperator*{\pos}{pos}
\DeclareMathOperator*{\AUCH}{AUCH}
\DeclareMathOperator*{\tf}{tf}
\DeclareMathOperator*{\ntf}{ntf}
\DeclareMathOperator*{\idf}{idf}
\DeclareMathOperator*{\ndf}{ndf}
\DeclareMathOperator*{\similarity}{sim}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\const}{const}
\DeclareMathOperator*{\dBeta}{Beta}
\DeclareMathOperator*{\dDir}{Dir}
\DeclareMathOperator*{\Tr}{Tr}
\DeclareMathOperator*{\dDP}{DP}
\DeclareMathOperator*{\dMult}{Mult}
\DeclareMathOperator*{\dBern}{Bern}
\DeclareMathOperator*{\dCRP}{CRP}
\DeclareMathOperator*{\dKL}{KL}
\DeclareMathOperator*{\diag}{diag}
\newcommand{\dd}[1]{\mathrm{d}{#1}}

% BOLD
\newcommand{\bmatr}{{\mathbf{B}}}
\newcommand{\cmatr}{{\mathbf{C}}}
\newcommand{\hmatr}{{\mathbf{H}}}
\newcommand{\fmatr}{{\mathbf{F}}}
\newcommand{\mmatr}{{\mathbf{M}}}
\newcommand{\xmatr}{{\mathbf{X}}}
\newcommand{\pmatr}{{\mathbf{P}}}
\newcommand{\xmatrt}{{\tilde{\mathbf{X}}}}
\newcommand{\imatr}{{\mathbf{I}}}
\newcommand{\vmatr}{{\mathbf{V}}}
\newcommand{\wmatr}{{\mathbf{W}}}
\newcommand{\umatr}{{\mathbf{U}}}
\newcommand{\zmatr}{{\mathbf{Z}}}
\newcommand{\zmatrt}{{\tilde{\mathbf{Z}}}}
\newcommand{\Tmatr}{\mathbf{T}}
\newcommand{\lambdamatr}{{\mathbf{\Lambda}}}
\newcommand{\phimatr}{\mathbf{\Phi}}
\newcommand{\sigmamatr}{\mathbf{\Sigma}}
\newcommand{\thetamatr}{\boldsymbol{\Theta}}

\newcommand{\zbt}{{\tilde{\mathbf{z}}}}
\newcommand{\mub}{{\boldsymbol{\mu}}}
\newcommand{\alphab}{{\boldsymbol{\alpha}}}
\newcommand{\thetab}{\boldsymbol{\theta}}
\newcommand{\iotab}{\boldsymbol{\iota}}
\newcommand{\zetab}{\boldsymbol{\zeta}}
\newcommand{\xib}{\boldsymbol{\xi}}
\newcommand{\xibt}{\tilde{\boldsymbol{\xi}}}
\newcommand{\xit}{\tilde{\xi}}
\newcommand{\betab}{{\boldsymbol{\beta}}}
\newcommand{\phib}{{\boldsymbol{\phi}}}
\newcommand{\psib}{{\boldsymbol{\psi}}}
\newcommand{\gammab}{{\boldsymbol{\gamma}}}
\newcommand{\lambdab}{{\boldsymbol{\lambda}}}
\newcommand{\varepsilonb}{{\boldsymbol{\varepsilon}}}
\newcommand{\pib}{{\boldsymbol{\pi}}}


\newcommand{\scl}{s_{\mathsf{c}}}
\newcommand{\shi}{s_{\mathsf{h}}}
\newcommand{\shib}{\mathbf{s}_{\mathsf{h}}}
\newcommand{\MOD}{M}
\newcommand{\entr}{\mathsf{H}}
\newcommand{\REG}{\Omega}
\newcommand{\Mquol}{V}
%\newcommand{\prob}{\mathsf{P}}
\newcommand{\prob}{p}
\newcommand{\expec}{\mathsf{E}}

% overline
\newcommand{\xo}{{\overline{x}}}
\newcommand{\yo}{{\overline{y}}}

% bold overline
\newcommand{\xbo}{{\overline{\mathbf{x}}}}





\newcommand{\Amc}{{\mathcal{A}}}
\newcommand{\Bmc}{{\mathcal{B}}}
\newcommand{\Cmc}{{\mathcal{C}}}
\newcommand{\Jmc}{{\mathcal{J}}}
\newcommand{\Imc}{{\mathcal{I}}}
\newcommand{\Kmc}{{\mathcal{K}}}
\newcommand{\Lmc}{{\mathcal{L}}}
\newcommand{\Mmc}{{\mathcal{M}}}
\newcommand{\Nmc}{{\mathcal{N}}}
\newcommand{\Pmc}{{\mathcal{P}}}
\newcommand{\Tmc}{{\mathcal{T}}}
\newcommand{\Vmc}{{\mathcal{V}}}
\newcommand{\Wmc}{{\mathcal{W}}}


\newcommand{\T}{\mathsf{T}}
\newcommand{\deist}{\mathbb{R}}
\newcommand{\ebb}{\mathbb{E}}

%ALEX
\newcommand{\Amatr}{\mathbf{A}}

\newcommand{\Umatr}{\mathbf{U}}
\newcommand{\zetavec}{\boldsymbol{\zeta}}


%\usepackage{natbib}
%\bibliographystyle{abbrvnat}
%\setcitestyle{authoryear,open={((},close={))}}
%\renewcommand{\thesubfigure}{\asbuk{subfigure}}



%different caption style: ``Fig.~N.~Caption text''
\makeatletter
\long\def\@makecaption#1#2{%
  \vskip\abovecaptionskip
  \sbox\@tempboxa{#1.~#2}%
  \ifdim \wd\@tempboxa >\hsize
    #1.~#2\par
  \else
    \global \@minipagefalse
    \hb@xt@\hsize{\hfil\box\@tempboxa\hfil}%
  \fi
  \vskip\belowcaptionskip}
\makeatother

\reversemarginpar

%\captionsetup[figure]{labelformat=gostfigure, justification=centering}
%\captionsetup[subfigure]{labelformat=gostfigure, justification=centering}
%\captionsetup[table]{labelformat=gostfigure, justification=centering}



%\renewcommand{\thesubfigure}{\asbuk{subfigure}}

\DeclareMathOperator*{\argmax}{arg\,max}



%different caption style: ``Fig.~N.~Caption text''
\makeatletter
\long\def\@makecaption#1#2{%
  \vskip\abovecaptionskip
  \sbox\@tempboxa{#1.~#2}%
  \ifdim \wd\@tempboxa >\hsize
    #1.~#2\par
  \else
    \global \@minipagefalse
    \hb@xt@\hsize{\hfil\box\@tempboxa\hfil}%
  \fi
  \vskip\belowcaptionskip}
\makeatother

\reversemarginpar

%\captionsetup[figure]{labelformat=gostfigure, justification=centering}
%\captionsetup[subfigure]{labelformat=gostfigure, justification=centering}
%\captionsetup[table]{labelformat=gostfigure, justification=centering}


\begin{document}
\input{defs}


\begin{titlepage}
%\begin{center}
%\textsc{МОСКОВСКИЙ ФИЗИКО-ТЕХНИЧЕСКИЙ ИНСТИТУТ (ГОСУДАРСТВЕННЫЙ УНИВЕРСИТЕТ)}\\
%\end{center}
%\vspace{1.5cm}
\begin{flushright}
{На правах рукописи}
\end{flushright}
\vspace{1.5cm}
\begin{center}
{Бахтеев Олег Юрьевич}
\par
\vspace{2cm}
\textsc{Байесовский выбор субоптимальной структуры
\\модели глубокого обучения }
\par
\vspace{2cm}
{05.13.17~--- Теоретические основы информатики}
\par
\vspace{2cm}
{АВТОРЕФЕРАТ\\
диссертации на соискание ученой степени\\
кандидата физико-математических наук}
\end{center}
\par
\vspace{3.5cm}
\begin{center}
{Москва~--- 2019}
\end{center}
\end{titlepage}

%\clearpage\maketitle
\setcounter{page}{2}
%\pretolerance=10000
%\thispagestyle{empty}
\noindent {Работа выполнена на Кафедре интеллектуальных систем Федерального государственного автономного образовательного учреждения высшего образования <<Московский физико-технический институт (национальный исследовательский институт)>>.

\vspace{0.1cm}

%\begin{sloppy}
%\fontdimen2\font=3pt

\vskip1ex\noindent
\begin{tabularx}{\linewidth}{@{}lX@{}}
  Научный руководитель: & \textbf{Стрижов Вадим Викторович}\\
  & доктор физико-математических наук, Федеральный исследовательский~центр <<Информатика и управление>> Российской академии наук, отдел интеллектуальных систем, ведущий научный сотрудник.
  \\[2pt]
  Официальные оппоненты: & \textbf{Чуличков Алексей Иванович}\\
  & доктор физико-математических наук, профессор, Федеральное государственное бюджетное образовательное учреждение высшего образования <<Московский государственный университет имени М. В.~Ломоносова>>, профессор кафедры математического моделирования и информатики физического факультета.\\[2pt]
  & \textbf{Зайцев Алексей Алексеевич}\\
  & кандидат физико-математических наук, Автономная некоммерческая
образовательная организация высшего образования <<Сколковский институт науки и технологий>>, руководитель лаборатории в Центре по научным и инженерным вычислительным технологиям для задач с большими массивами данных. \\[2pt]
  Ведущая организация: & Федеральное государственное автономное образовательное учреждение высшего образования <<Санкт-Петербургский национальный исследовательский университет информационных технологий, механики и оптики>>.
\end{tabularx}
\vskip2ex\noindent


\vspace{0.2cm}
%\noindent Защита состоится~``\underline{\phantom{332}}
%''\underline{\phantom{xxxxxxxxxxxxxxxxxxxx}} 2017~года~в~\underline{\phantom{332}}:\underline{\phantom{332}} на~заседании диссертационного совета Д 002.073.05 при Федеральном исследовательском центре <<Информатика и управление>> Российской академии наук (ФИЦ~ИУ~РАН) по адресу: 119333, г.\,Москва, ул.\,Вавилова, д.\,40.
\noindent Защита состоится~<<\quad>> января 2019 года~в~\quad\quad на~заседании диссертационного совета Д 002.073.05 при Федеральном исследовательском центре <<Информатика и управление>> Российской академии наук (ФИЦ~ИУ~РАН) по адресу: 119333, г.\,Москва, ул.\,Вавилова, д.\,40.

\vspace{0.2cm}
\noindent С диссертацией можно ознакомиться в библиотеке Федерального государственного учреждения Федеральный исследовательский центр <<Информатика и управление>> Российской академии наук и на сайте http://www.frccsc.ru/

\vspace{0.2cm}
\noindent Автореферат разослан  \quad \quad\quad\quad\quad\quad 2019 года.

\vspace{0.3cm}
\noindent Ученый секретарь\\
диссертационного совета Д 002.073.05\\
д.ф.-м.н., профессор
\hspace{9cm} В. В. Рязанов
}

\clearpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pretolerance=-1

%\setcounter{page}{0}

\section*{Общая характеристика работы}
\label{ch:Introduction}

\textbf{Актуальность темы.}
В работе исследуется проблема автоматического выбора моделей глубокого обучения оптимальной и субоптимальной сложности. 
Под сложностью модели понимается \emph{минимальная длина описания}~(Gr{\"u}nwald:~2005),  минимальное количество информации, которое требуется для передачи информации о модели и о выборке. Получение минимальной длины описания модели является вычислительно сложной процедурой. В работе предлагается получение ее приближенной оценки, основанной на связи минимальной длины описания и \emph{обоснованности модели}. Для получения оценки обоснованности используются вариационные методы получения оценки обоснованности~(Bishop:~2006), основанные на аппроксимации неизвестного апостериорного распределения другим заданным распределением. Под субоптимальной сложностью понимается нижняя вариационная оценка обоснованности модели.

Одна из проблем построения моделей глубокого обучения --- большое число параметров моделей~(Hinton:~2007, 2013). Поэтому задача выбора моделей глубокого обучения включает в себя выбор вычислительно эффективной стратегии построения модели. В работе~(Barron:~2008) приводятся теоретические оценки построения нейросетей с использованием жадных стратегий,  при которых построение модели производится итеративно последовательным увеличением числа нейронов в сети. %В работе~(Tzikas: 2010) предлагается жадная стратегия выбора модели нейросети с использованием релевантных распределений, оптимизация параметров которых позволяет снизить сложность модели. %Данный метод был также применялся в задаче построения модели метода релевантных векторов~\cite{rvm}. Альтернативой данным алгоритмам построения моделей являются методы, основанные на прореживании сетей глубокого обучения~\cite{obd, popova, nvidia_prune}, т.е. на последовательном удалении параметров, не дающих существенного прироста качества модели. 
%В работах~\cite{Bengio, hd} рассматривается послойное построение модели с отдельным критерием оптимизации для каждого слоя. В работах~\cite{Kingma, gendis_pictures, gendis_phd} предлагается декомпозиция модели на порождающую и разделяющую, оптимизируемые последовательно. В работе~\cite{adanet} предлагается метод автоматического построения сети, основанный на бустинге. В качестве оптимизируемого функционала предлагается линейная комбинация функции правдоподобия выборки и сложности модели по Радемахеру. 
В работах~(Zoph:~2016, Baker:~2017, Cai:~2018, Zoph:~2018) предлагаются методы автоматического построения моделей глубокого обучения, основанные на обучении с подкреплением. В~(Liu:~2018) предлагается градиентная оптимизация структуры модели.

%В качестве порождающих моделей в сетях глубокого обучения выступают ограниченные машины Больцмана~\cite{hinton_rbm} и автокодировщики~\cite{founds}. В работе~\cite{contractive} рассматриваются некоторые типы регуляризации автокодировщиков, позволяющие формально рассматривать данные модели как порождающие модели с использованием байесовского вывода. В работе~\cite{score} также рассматриваются регуляризованные автокодировщики и свойства оценок их правдоподобия. В работе~\cite{vae} предлагается обобщение автокодировщика с использованием вариационного байесовского вывода~\cite{bishop}. В работе~\cite{train_generative} рассматриваются модификации вариационного автокодировщика и ступенчатых сетей (англ. ladder network)~\cite{ladder} для случая построения многослойных порождающих моделей. 

В качестве критерия выбора модели в ряде работ~(MacKay:~2002, Bishop:~2006, Стрижов:~2010, 2014) выступает обоснованность модели. %В работах~(Токмакова: 2012, Зайцев: 2013, Strijov: 2010, Стрижов: 2014) рассматривается проблема выбора модели и оценки гиперпараметров в задачах регрессии. 
Альтернативными критериями выступают показатель нелинейности модели (Vladislavleva:~2008), робастность модели~(Xu:~2012) и эксплуатационные критерии качества модели. Важным свойством, предъявляемым к критериям качества модели, является устойчивость выбранных моделей под действием шума~(Szegedy:~2013). 
%В работе~\cite{perekrestenko} рассматривается перечень критериев сложности моделей глубокого обучения и их взаимосвязь. В работе~\cite{vladis} в качестве критерия сложности модели выступает показатель нелинейности, характеризуемый степенью полинома Чебышева, аппроксимирующего функцию. В работе~\cite{need_prune} анализируется показатель избыточности параметров сети. Утверждается, что по небольшому набору параметров в глубокой сети с большим количеством избыточных параметров можно спрогнозировать значения остальных. В работе~\cite{rob} рассматривается показатель робастности моделей, а также его взаимосвязь с топологией выборки и классами функций, в частности рассматривается влияние функции ошибки и ее липшицевой константы на робастность моделей. Схожие идеи были рассмотрены в работе~\cite{intrig}, в которой исследуется устойчивость классификации модели под действием шума. 

Одним из методов получения приближенного значения обоснованности является вариационный метод получения нижней оценки интеграла~(Bishop:~2006). Использование вариационной оценки в качестве приближения обоснованности позволяет аппроксимировать апостериорное распределение с использованием широкого семейства распределений.  В работе~(Graves:~2011) рассматривается алгоритм получения вариационной нижней оценки обоснованности  для оптимизации параметров и гиперпараметров моделей глубокого обучения. В работе~(Maclaurin:~2015) рассматривается стохастический градиентный спуск в качестве оператора, порождающего распределение, аппроксимирующее апостериорное распределение параметров модели. Схожий подход предлагается в работе~(Mandt:~2017), где также рассматривается стохастический градиентный спуск в качестве оператора, приближающего апостериорное распределение параметров. %В работе~(Welling: 2011) предлагается модификация стохастического градиентного спуска, аппроксимирующая апостериорное распределение. 

%Альтернативным методом выбора модели является выбор модели на основе скользящего контроля~\cite{cv_ms, tokmakova}. Проблемой такого подхода является возможная высокая вычислительная сложность~\cite{expensive, expensive2}. В работах~\cite{bias,bias2} рассматривается проблема смещения оценок качества модели при гиперпараметрах, получаемых с использованием $k$-fold метода скользящего контроля, при котором выборка делится на $k$ частей с обучением на $k-1$ части и валидацией результата на оставшейся части выборки. 

Задачей, связанной с проблемой выбора модели, является задача оптимизации гиперпараметров~(MacKay:~2002, Bishop:~2006). В работе~(Стрижов:~2012) рассматривается оптимизация гиперпараметров с использованием метода скользящего контроля и методов оптимизации обоснованности моделей, отмечается низкая скорость сходимости оптимизации гиперпараметров при использовании метода скользящего контроля. В ряде работ~(Maclaurin:~2015, Domke:~2012) рассматриваются градиентные методы оптимизации гиперпараметров, позволяющие оптимизировать большое количество гиперпараметров одновременно. В работе~(Maclaurin:~2015) предлагается метод оптимизации гиперпараметров с использованием градиентного спуска с моментом, в качестве оптимизируемой функции рассматривается ошибка на валидационной части выборки. В работах~(Pedregosa:~2016, Luketina:~2016) предлагается метод аппроксимации градиента функции потерь по гиперпараметрам, позволяющий использовать градиентные методы в задаче оптимизации гиперпараметров на больших выборках. %В работе~(Luketina: 2016) предлагается упрощенный метод оптимизации гиперпараметров с градиентным спуском: вместо всей истории обновлений параметров для оптимизации используется только последнее обновление. В работе~(Mandt: 2017) рассматривается задача оптимизации параметров градиентного спуска с использованием нижней вариационной оценки обоснованности. 

\vspace{0.5cm}
\textbf{Цели работы.}
\vspace{0.2cm}
\begin{enumerate}
\item Исследовать методы построения моделей глубокого обучения оптимальной и субоптимальной сложности.
%\item Проанализировать различные подходы к решению задачи автоматического построения моделей глубокого обучения и оптимизации параметров модели.
\item Предложить критерии оптимальной и субоптимальной сложности модели глубокого обучения.
\item Предложить метод выбора субоптимальной структуры модели глубокого обучения.
\item Предложить алгоритм построения модели субоптимальной сложности и оптимизации ее параметров.
%\item Предложить алгоритм построения модели субоптимальной сложности и оптимизации параметров модели и
\end{enumerate}


\vspace{0.5cm}
\textbf{Методы исследования.} Для достижения поставленных целей используются методы байесовского вывода. В качестве оценки обоснованности выступает вариационная нижняя оценка обоснованности модели. Рассматривается графовое представление нейронной сети. Для получения вариационных оценок обоснованности модели используется метод, основанный на градиентном спуске. В качестве метода получения модели субоптимальной сложности используется метод автоматического определения релевантности параметров с использованием градиентных методов оптимизации гиперпараметров.

\vspace{0.5cm}
\textbf{Основные положения, выносимые на защиту.}
\vspace{0.3cm}
\begin{enumerate}
\item Предложен метод байесовского выбора оптимальной и субоптимальной структуры модели глубокого обучения с использованием автоматического определения
релевантности параметров.
\item Предложены критерии оптимальной и субоптимальной сложности модели глубокого обучения.
\item Предложен метод графового описания моделей глубокого обучения.
\item Предложено обобщение задачи оптимизации структуры модели, включающее ранее описанные методы выбора модели: оптимизация обоснованности модели, последовательное увеличение сложности модели, последовательное снижение сложности модели, полный перебор вариантов структуры модели.
\item Предложен метод оптимизации вариационной оценки обоснованности модели на основе метода мультистарта задачи оптимизации.
\item Предложен алгоритм оптимизации параметров, гиперпараметров и структурных параметров моделей глубокого обучения.
\item Исследованы свойства оптимизационной задачи при различных значениях метапараметров. Рассмотрены ее асимптотические свойства.
\end{enumerate}


\vspace{0.5cm}
\textbf{Научная новизна.} Разработан новый подход к построению моделей глубокого обучения. Предложены критерии субоптимальной и оптимальной сложности модели, а также исследована их связь. Предложен метод построения модели глубокого обучения субоптимальной сложности. Исследованы методы оптимизации гиперпараметров и параметров модели.  Предложена обобщенная задача выбора модели глубокого обучения.

\vspace{0.5cm}
\textbf{Теоретическая значимость.} Диссертационная работа носит теоретический характер. В работе предлагаются критерии субоптимальной и оптимальной сложности, основанные на принципе минимальной длины описания. Исследуется взаимосвязь критериев оптимальной и субоптимальной сложности. Предлагаются градиентные методы для получения оценок сложности модели. Доказывается теорема об оценке энтропии эмпирического распределения параметров модели, полученных под действием оператора оптимизации.
Доказывается теорема об обобщенной задаче выбора модели глубокого обучения.


\vspace{0.5cm}
\textbf{Практическая значимость.} Предложенные в работе методы предназначены для построения моделей глубокого обучения в прикладных задачах регрессии и классификации; оптимизации гиперпараметров полученной модели; выбора модели из конечного множества заданных моделей; получения оценок переобучения модели.


\vspace{0.5cm}
\textbf{Степень достоверности и апробация работы.} Достоверность результатов подтверждена математическими доказательствами, экспериментальной проверкой полученных методов на реальных задачах выбора моделей глубокого обучения; публикациями результатов исследования в рецензируемых научных изданиях, в том числе рекомендованных ВАК. Результаты работы докладывались и обсуждались на следующих научных конференциях.
\begin{enumerate}
\item ``Восстановление панельной матрицы и ранжирующей модели в разнородных шкалах'', Всероссийская конференция <<57-я научная конференция МФТИ>>, 2014.
\item ``A monolingual approach to detection of text reuse in Russian-English collection'', Международная конференция <<Artificial Intelligence and Natural Language Conference>>, 2015.
\item ``Выбор модели глубокого обучения субоптимальной сложности с использованием вариационной оценки правдоподобия'', Международная конференция <<Интеллектуализация обработки информации>>, 2016.
\item ``Machine-Translated Text Detection in a Collection of Russian
Scientific Papers'', Международная конференция по компьютерной лингвистике и интеллектуальным технологиям <<Диалог-21>>, 2017.
\item ``Author Masking using Sequence-to-Sequence Models'', Международная конференция <<Conference and Labs of the Evaluation Forum>>, 2017.
\item ``Градиентные методы оптимизации гиперпараметров моделей глубокого обучения'', Всероссийская конференция <<Математические методы распознавания образов ММРО>>, 2017.
\item ``Детектирование переводных заимствований в текстах научных статей из журналов, входящих в РИНЦ'', Всероссийская конференция <<Математические методы распознавания образов ММРО>>, 2017.
\item ``ParaPlagDet: The system of paraphrased plagiarism detection'', Международная конференция <<Big Scholar at conference on knowledge discovery and data mining>>, 2018.
\item ``Байесовский выбор наиболее правдоподобной структуры модели глубокого обучения'', Международная конференция <<Интеллектуализация обработки информации>>, 2018.
\item ``Variational learning across domains with triplet
information'', Международная конференция <<Visually Grounded Interaction and Language workshop, Conference on Neural Information Processing Systems>>, 2018.
\end{enumerate}

%Работа поддержана грантами Российского фонда фундаментальных исследований.
%\begin{enumerate}
%\item 19-07-00875, Развитие методов автоматического построения и выбора вероятностных моделей субоптимальной сложности в задачах глубокого обучения.
%\item 16-37-00488, Разработка алгоритмов построения сетей глубокого обучения как суперпозиций универсальных моделей.
%\item 16-07-01158, Развитие теории построения суперпозиций универсальных моделей классификации сигналов.
%\item 14-07-3104,  Построение и анализ моделей классификации для выборок малой мощности.
%\end{enumerate}

\vspace{0.5cm}
\textbf{Публикации по теме диссертации.} Основные результаты по теме диссертации изложены в 11 печатных изданиях, 9 из которых изданы в журналах, рекомендованных ВАК.
%\end{enumerate}


\vspace{0.5cm}
\textbf{Личный вклад.} Все приведенные результаты, кроме отдельно оговоренных случаев, получены диссертантом лично при научном руководстве д.ф.-м.н. В. В. Стрижова.


\vspace{0.5cm}
\textbf{Структура и объем работы.} Диссертация состоит из оглавления, введения, четырех разделов, заключения, списка иллюстраций, списка таблиц, перечня основных обозначений и списка литературы из 162 наименований. Основной текст занимает 144 страницы.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Основное содержание работы}
Во \textbf{введении} обоснована актуальность диссертационной работы, сформулированы цели и методы исследования, поставлены основные задачи, обоснована научная новизна, теоретическая и практическая значимость полученных результатов.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
В \textbf{главе~1} приводится формальная постановка задачи выбора модели глубокого обучения. Вводятся основные определения и обозначения, функции качества модели глубокого обучения, описывается вероятностная интерпретация модели.

Проблема выбора структуры модели глубокого обучения формулируется следующим образом: решается задача классификации или регрессии на заданной или пополняемой выборке $\mathfrak{D}, (\x, y) \in \mathfrak{D}, \x \in \Xb =  \mathbb{R}^n, y \in \yb$. Требуется выбрать структуру нейронной сети, доставляющей минимум ошибки на этой функции и максимум качества на некотором внешнем критерии.
 Под моделью глубокого обучения понимается суперпозиция дифференцируемых по параметрам нелинейный функций. Под структурой модели понимается значение структурных параметров модели, т.е. величин, задающих вид итоговой суперпозиции. 

%Задана простая выборка \begin{equation}\label{eq:dataset}\mathfrak{D} = \{(\mathbf{x}_i,y_i)\}, i = 1,\dots,m,\end{equation} состоящая из множества объектов $$\mathbf{x}_i \in \mathbf{X} \subset  \mathbb{X}, \quad {y}_i \in \mathbf{y} \subset \yb.$$ 


\begin{defin}
\textit{Моделью} $\mathbf{f}(\mathbf{w}, \mathbf{x})$ назовем дифференцируемую по параметрам $\mathbf{w}$ функцию из множества признаковых описаний объекта во множество меток:
\[
    \mathbf{f}:  \mathbb{W}  \times \mathbb{X} \to \yb,
\] 
где $\mathbb{W}$ --- пространство параметров функции $\mathbf{f}$.
\end{defin}

\begin{defin}
Пусть задан ациклический граф $(V,E)$, такой, что 
1) для каждого ребра $(j,k) \in E$ задан вектор базовых дифференцируемых функций  $\mathbf{g}^{j,k} = [\mathbf{g}^{j,k}_0, \dots, \mathbf{g}^{j,k}_{K^{j,k}-1}]$  мощности $K^{j,k}$;
2) для каждой вершины $v \in V$ задана дифференцируемая функция агрегации $\textbf{agg}_v$;
3) задана функция $\mathbf{f} = \mathbf{f}_{|V|-1}$:
\begin{equation}
\label{eq:modelfam}
    \mathbf{f}_{k}(\mathbf{w}, \mathbf{x}) = \textbf{agg}_{k}\left(\{ \langle \boldsymbol{\gamma}^{j,k}, \mathbf{g}^{j,k} \rangle \circ  \mathbf{f}_j(\mathbf{x})| j \in \text{Adj}(v_k)\}\right), 
\end{equation}
\[
k \in \{1,\dots,|V|-1\}, \quad \mathbf{f}_0(\mathbf{x}) = \mathbf{x}, \quad v_k \in V,
\]
являющаяся функцией из признакового пространства $\mathbb{X}$ в пространство меток $\yb$ при значениях векторов, $\boldsymbol{\gamma}^{j,k} \in [0,1]^{K^{j,k}}$.

Граф $(V, E)$ со множеством векторов базовых функций $\{\mathbf{g}^{j,k}, (j,k) \in E\}$ и функций агрегаций $\{\textbf{agg}_k\},$ где $k \in \{0, \dots, |V|-1\},$ назовем \textit{параметрическим семейством моделей} $\mathfrak{F}$.
\end{defin}


%\begin{defin}
%Функции $\mathbf{f}_0, \dots, \mathbf{f}_{|V|-1}$ из~\eqref{eq:modelfam} назовем \textit{слоями или подмоделями} модели $\mathbf{f}$.
%\end{defin}

\begin{utv}
Для любого значения $\boldsymbol{\gamma}^{j,k} \in [0,1]^{K^{j,k}}$ функция $\mathbf{f} \in \mathfrak{F}$ является моделью.
\end{utv}



%\begin{defin}
%\textit{Параметрами }модели $\mathbf{f}$ из параметрического семейства моделей $\mathfrak{F}$  назовем конкатенацию векторов параметров всех базовых функций $\{\mathbf{g}^{j,k}| {(j,k) \in E} \}, \mathbf{w} \in \mathbb{W}.$ Вектор параметров базовой функции $\mathbf{g}^{j,k}_l$ будем обозначать как $\mathbf{w}^{j,k}_l$.
%\end{defin}



\begin{defin}
Структурой $\boldsymbol{\Gamma}$  модели $\mathbf{f}$ из параметрического семейства моделей $\mathfrak{F}$  назовем конкатенацию векторов $\boldsymbol{\gamma}^{j,k}$. Множество всех возможных значений структуры $\boldsymbol{\Gamma}$ будем обозначать как $\amsmathbb{\Gamma}$.
Векторы $\boldsymbol{\gamma}^{j,k}, (j,k) \in E$ назовем \textit{структурными параметрами модели.}
\end{defin}

%\begin{defin}
%\textit{Параметризацией }множества моделей $M$ назовем параметрическое семейство моделей $\mathfrak{F}$, такое, что для каждой модели $\mathbf{f} \in M$ существуют значение структуры модели $\boldsymbol{\Gamma}$ при котором функция $\mathbf{f}$ совпадает с функцией~\eqref{eq:modelfam}.
%\end{defin}

%Рассмотрим варианты ограничений, которые накладываются на структурные параметры $\boldsymbol{\gamma}^{j,k}$ параметрического семейства моделей. Цель данных ограничений --- уточнение архитектуры модели глубокого обучения, которую требуется получить. 
%\begin{enumerate}
%\item Структурные параметры лежат на веришнах булевого куба: $\boldsymbol{\gamma}^{j,k} \in \{0,1\}^{K^{j,k}}$. Структурные параметры $\boldsymbol{\gamma}^{j,k}$ %интерпретируются как параметр включения или выключения компонент вектора базовых функций $\mathbf{g}^{j,k}$ в итоговую модель.
%\item Структурные параметры лежат внутри булевого куба: $\boldsymbol{\gamma} \in [0,1]^{K^{j,k}}$. Релаксированная версия предыдущих ограничений, позволяющая проводить градиентную оптимизацию для структурных параметров.
%\item Структурные параметры лежат на веришнах симплекса: $\boldsymbol{\gamma}^{j,k} \in \bar{\Delta}^{K^{j,k}-1}$. Каждый вектор структурных параметров $\boldsymbol{\gamma}^{j,k}$ имеет только одну ненулевую компоненту, определяющую какая из базовых функций $\mathbf{g}^{j,k}$ войдет в итоговую модель. Примером параметрического семейства моделей, требующим такое ограничение является семейство полносвязанных нейронных сетей с одним скрытым слоем и двумя значениями количества нейронов на скрытом слое. Схема семейства представлена на Рис.~\ref{fig:scheme_mlp}. Данное семейство можно представить как семейство с двумя базовыми функциями вида $\mathbf{g} = \boldsymbol{\sigma}(\mathbf{w}^{\T}\mathbf{x}),$ где матрицы параметров каждой из функций  $\mathbf{g}^{1,1}, \mathbf{g}^{1,2}$ имеют фиксированное число нулевых столбцов. Количество этих столбцов определяет размерность итогового скрытого пространства или числа нейронов на скрытом слое.
%\item  Структурные параметры лежат внутри симплекса: $\boldsymbol{\gamma}^{j,k} \in {\Delta}^{K^{j,k}-1}$. Релаксированная версия предыдущих ограничений, позволяющая проводить градиентную оптимизацию для структурных параметров. Значение стуктурных параметров $\boldsymbol{\gamma}^{j,k}$ интерпретируются как вклад каждой компоненты вектора базовых функций $\mathbf{g}^{j,k}$ в итоговую модель. 
%\end{enumerate}

В работе рассматривается случай, когда структурные параметры лежат внутри симплекса: $\boldsymbol{\gamma}^{j,k} \in {\Delta}^{K^{j,k}-1}$.%  Данные ограничения позволяют решать задачу выбора модели как для семейства моделей типа многослойных полносвязных нейронных сетей, так и для более сложных параметрических семейств~\cite{darts}. 

%Для дальнейшей постановки задачи введем понятие вероятностной модели, и связанных с ним определений. Будем полагать, что для параметров модели $\mathbf{w}$ и структуры  $\boldsymbol{\Gamma}$ задано распределение $\prior$, соответствующее предположениям о распределении структуры и параметров. 

\begin{defin}
\textit{Гиперпараметрами} $\mathbf{h}\in \mathbb{H}$ модели  назовем параметры распределения $\prior$.
\end{defin}

\begin{defin}
\textit{Априорным распределением} параметров и структуры модели назовем вероятностное распределение, соответствующее предположениям о распределении параметров модели:
$
    \prior: \mathbb{W} \times \Gb \to \mathbb{R}^{+}, 
$
где $\mathbb{W}$ --- множество значений параметров модели, $\Gb$ --- множество значений структуры модели.% $\Hb$ --- множество значений гиперпараметров, $\Lamb$ -- множество значений метапараметров. Формальное определение последних будет дано далее.
%ормальное определение метапараметров $\lam \in \Lamb$ будет дано далее.
\end{defin}


%Одной из постановок задачи выбора структуры модели является \textit{двусвязный байесовский вывод.} 
%На \textit{первом уровне} байесовского вывода  находится апостериорное распределение параметров.

\begin{defin}
\textit{Апостериорным распределением} назовем распределение вида
\begin{equation}
\label{eq:posterior}
    \post = \frac{\LL \prior}{\EV} \propto \LL \prior.% \mathbb{W} \times \Delta{\boldsymbol{\Gamma}} \times \mathbb{X}  \times \mathbb{H}. 
\end{equation}
\end{defin}

%\begin{defin}
%\textit{Вероятностной моделью глубокого обучения} назовем совместное распределение вида
%\[
%    p(\y, \mathbf{w},  \boldsymbol{\Gamma}|\X, \mathbf{h}, \lam) = p(\y|\X, \mathbf{w},  \boldsymbol{\Gamma}, \lam)\prior: {\yb}^m \times \mathbb{W}  \times \Gb  \to \mathbb{R}^{+}.
%\]
%\end{defin}

%\begin{defin}
%\textit{Функцией правдоподбия выборки } назовем величину
%\[
%    \LL : {\yb}^m \to \mathbb{R}^{+}.
%\]
%\end{defin}
%Для каждой модели определена функция обоснованности  $\EV$.
 
%На \textit{втором уровне} байесовского вывода осуществляется выбор модели на основе обоснованности модели.
\begin{defin}
\textit{Обоснованностью модели }назовем величину
\begin{equation}
\label{eq:evidence}
\EV = \iint_{\mathbf{w}, \boldsymbol{\Gamma}} \LL \prior  d\mathbf{w}d\boldsymbol{\Gamma}.
\end{equation}
\end{defin}


\begin{defin}
\textit{Вариационным распределением} назовем параметрическое распределение $\q$ c параметрами $\teta \in \Tetab$, являющееся приближением  апостериорного распределения параметров и структуры $\post.$ 
\end{defin}



\begin{defin}
\label{def:l}
Пусть задано вариационное распределения $\q$.
\textit{Функцией потерь} $\Loss$ для модели $\model$ назовем дифференцируемую функцию, принимаемую за качество модели на обучающей выборки при параметрах модели, получаемых из  распределения $q$.
\end{defin}

\begin{defin}
\label{def:q}
Пусть задано вариационное распределения $\q$ и функция потерь $\Loss$. 
\textit{Функцией валидации} $\Val$ для модели $\model$ назовем дифференцируемую функцию, принимаемую за качество модели при векторе $\teta$, заданном неявно.
\end{defin}

 

Задача выбора структуры модели и параметров модели ставится как двухуровневая задача оптимизации:
\begin{equation}
\label{eq:optim_problem}
	\h^{*} = \argmax_{\h  \in \Hb} \Val[][][][\teta^{*}], \quad   \teta^{*} = \argmax_{\teta \in \Tetab} \Loss.
\end{equation}

\begin{defin}
\textit{Задачей выбора модели} $\model$ назовем   двухуровневую задачу оптимизации~\eqref{eq:optim_problem}.
\end{defin}

Метапараметры $\lam$ соответствуют параметрам оптимизации, т.е. параметрам, которые не подлежат оптимизации в ходе задачи выбора модели. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
В \textbf{главе 2} рассматривается задача выбора моделей глубокого обучения субоптимальной сложности. Вводятся вероятностные предположения о распределении параметров. В качестве сложности модели выступает \textit{обоснованность модели}~\eqref{eq:evidence}. Для получения оценки обоснованности применяются вариационные методы с использованием градиентных алгоритмов оптимизации. 
Предполагается, что структура  $\boldsymbol{\Gamma}$  модели глубокого обучения $\mathbf{f}$  и метапараметры $\lam$ определены однозначно:
\begin{equation}
\label{eq:workaround}
    \prior = p(\w,\G|\h), \quad \priorw = p(\w|\h),\quad    \LL = p(\y|\X,\w).
\end{equation}


Пусть априорное распределение параметров имеет вид
\begin{equation}
\label{eq:prior}
	\mathbf{w} \sim \mathcal{N}(\mathbf{0}, \mathbf{A}^{-1}),
\end{equation}
где $\mathbf{A}^{-1} = \text{diag}[\alpha_1, \dots, \alpha_u]^{-1}$ --- матрица ковариаций диагонального вида, где $u$ --- число параметров $\w$ модели $\model$. 



\begin{defin} Сложностью модели $\mathbf{f}$ назовем обоснованность модели:
\begin{equation}
\label{eq:model_evidence}
	p(\mathbf{y}|\mathbf{X},\mathbf{h}) = \int_{\mathbf{w} \in \mathbb{W}} p(\y|\X,\w)p(\mathbf{w}|\mathbf{h})d\mathbf{w}.
\end{equation}
\end{defin}


\begin{defin}Модель  $\mathbf{f}$ назовем оптимальной среди моделей множества $M$, если достигается максимум интеграла~\eqref{eq:model_evidence}.
\end{defin}

Требуется найти оптимальную модель $\mathbf{f}$ из заданного множества моделей $M$, а также значения ее параметров $\mathbf{w}$, доставляющие максимум апостериорной вероятности
\begin{equation}
\label{eq:var_inf_posterior}
	p(\mathbf{w}|\mathbf{y},\mathbf{X},\mathbf{h}) = \frac{p(\y|\X,\w)p(\mathbf{w}|\mathbf{h})}{p(\mathbf{y}|\mathbf{X}, \mathbf{h})}.
\end{equation}

%Интеграл обоснованности~\eqref{eq:model_evidence} модели является трудновычислимым для данного семейства моделей. Одним из методов вычисления приближенного значения обоснованности является получение вариационной оценки обоснованности.  

%{
%\begin{defin} Вариационной оценкой логарифма обоснованности модели~\eqref{eq:model_evidence} $\textnormal{log}~p(\mathbf{y}|\mathbf{X},\mathbf{h})$ называется оценка $\textnormal{log}~\hat{p}(\mathbf{y}|\mathbf{X},\mathbf{h})$, полученная аппроксимацией неизвестного апостериорного распределения $p(\mathbf{w}| \mathbf{y}, \mathbf{X}, \mathbf{h})$ заданным распределением $q(\mathbf{w})$.
%\end{defin}
%}


{В качестве функции, приближающей логарифм интеграла~\eqref{eq:model_evidence}, будем рассматривать его вариационную нижнюю оценку, полученную при помощи неравенства Йенсена:
\begin{equation} 
\label{eq:var_elbo}
\textnormal{log}~p(\mathbf{y}|\mathbf{X},\mathbf{h})   \geq	\int_{\mathbf{w}} q(\mathbf{w})\textnormal{log}~\frac{p(\y,\w|\X,\h)}{q(\mathbf{w})}d\mathbf{w} =
\end{equation} 
$$
= -\textnormal{D}_\textnormal{KL} \bigl(q(\mathbf{w})||p(\mathbf{w}|\mathbf{h})\bigr) + \int_{\mathbf{w}} q(\mathbf{w})\textnormal{log}~{p(\y|\X,\w)} d \mathbf{w},
$$
где $\textnormal{D}_\textnormal{KL}\bigl(q(\mathbf{w})||p(\mathbf{w} |\mathbf{h})\bigr)$ --- расстояние Кульбака--Лейблера между двумя распределениями.


\begin{defin} Пусть задано множество распределений $\mathfrak{Q}$. Модель $\mathbf{f}$ назовем субоптимальной на множестве моделей $M$, если модель доставляет максимум нижней вариационной оценке интеграла~\eqref{eq:var_elbo}.
\end{defin}

В качестве множества $\mathfrak{Q}$ рассматривается два семейства распределений.
Первое семейство --- семейство нормальных распределений с диагональными матрицами ковариаций:
\begin{equation}
\label{eq:diag}
	q \sim \mathcal{N}(\boldsymbol{\mu}_q, \mathbf{A}^{-1}_q),\quad\boldsymbol{\theta}=[\boldsymbol{\mu}_q, \textbf{diag}(\mathbf{A}^{-1}_q)]
\end{equation}
где $\mathbf{A}_q$ --- диагональная матрица ковариаций, $\boldsymbol{\mu}_q$ --- вектор средних компонент.

В качестве второго семейства распределений $\mathfrak{Q}$, рассматриваются распределения параметров, полученные в ходе оптимизации модели.

Представим интеграл~\eqref{eq:var_elbo} в виде:
\begin{equation}
\label{eq:var_elbo_entropy}
 \mathsf{E}_{q(\mathbf{w)}} \textnormal{log~}p (\mathbf{y}, \mathbf{w}|\mathbf{X}, \mathbf{h}) - \mathsf{S}\bigl({q(\mathbf{w)}}\bigr),
\end{equation}
где $\mathsf{S}$ --- энтропия распределения:
\[
\mathsf{S}\bigl({q(\mathbf{w)}}\bigr) = - \int_{\mathbf{w}} q(\mathbf{w})\textnormal{log}~q(\mathbf{w})d\mathbf{w}, \quad 
p (\mathbf{y}, \mathbf{w}|\mathbf{X}, \mathbf{h}) = p (\mathbf{w}| \mathbf{h}) p (\mathbf{y}|\mathbf{X}, \mathbf{w}).
\]


Оценка распределений производится при оптимизации параметров. Оптимизация выполняется в режиме мультистарта, т.е. при запуске оптимизации параметров модели из нескольких разных начальных приближений. Основная проблема такого подхода~---~вычисление энтропии $\mathsf{S}$ распределений $q(\mathbf{w}) \in \mathfrak{Q}$. Ниже представлен метод получения оценок энтропии~\eqref{eq:entropy} ~$\mathsf{S}$ и оценок обоснованности~\eqref{eq:var_elbo_entropy}.

%Запустим $r$ процедур оптимизаций модели $\mathbf{f}$ из разных начальных приближений:
%%
%	L( \boldsymbol{\theta}| \mathbf{h}, \mathbf{X}, \mathbf{y}) = \sum_{l=1}^r \text{log}p(\mathbf{y}, \mathbf{w}^l|\mathbf{X}, \mathbf{h})  \to \max, \quad \boldsymbol{\theta} = [\mathbf{w}^1, \dots, \mathbf{w}^r],
%$ 
%где $r$ --- число оптимизаций,
%\begin{equation}
%\label{eq:loss_func}
%\text{log}p(\mathbf{y}, \mathbf{w}^l|\mathbf{X}, \mathbf{h}) =  \textnormal{log}~p(\mathbf{w}^l|\mathbf{h}) + \sum_{i=1}^m \textnormal{log}p({y}_i |\mathbf{x}_i, \mathbf{w}^l, \mathbf{h}).
%\end{equation}

Пусть начальные приближения параметров $\mathbf{w}^1, \dots, \mathbf{w}^r$ порождены из некоторого начального распределения:
$
	\mathbf{w}^1, \dots, \mathbf{w}^r \sim q^0(\mathbf{w}). 
$

Для удобства будем использовать $L(\w)$ как эквивалентную форму записи $\Loss$ для $\teta = [\w]^\text{T}$. 
\begin{defin}
Оператором градиентного спуска назовем оператор оптимизации вида
$	T(\w)  = \w - \lambda_{\text{lr}} \nabla (-L(\w)), $
где  $\lambda_{\text{lr}}$ --- длина шага градиентного спуска.
\end{defin}

\begin{theorem}~Пусть $T$ --- оператор градиентного спуска,
 $L$ --- функция потерь, градиент $\nabla L$ которой имеет константу Липшица $C_L$.  Пусть $\teta = [\mathbf{w}^1,\dots,\mathbf{w}^r]^\text{T}$ ---  начальные приближения оптимизации модели, где $r$ --- число начальных приближений. Пусть $\lambda_{\text{lr}}$ --- длина шага градиентного спуска, такая, что
$
\lambda_{\text{lr}}<\frac{1}{C_L}, \quad \lambda_{\text{lr}} < \bigl(\max_{l \in \{1,\dots,r\}}\lambda_\textnormal{max} (\mathbf{H}(\mathbf{w}^l))\bigr)^{-1}, 
$
где $\lambda_\textnormal{max}$ --- наибольшее по модулю собственное значение гессиана  $\mathbf{H}$ минус функции потерь $(-L)$.

Тогда разность энтропий распределений $q'(\mathbf{w}), q(\mathbf{w})$ на смежных шагах почти наверное сходится к следующему выражению: 
\begin{equation}
\label{eq:entropy}
	\mathsf{S}\bigl(q'(\mathbf{w})) -  \mathsf{S}\bigl(q(\mathbf{w}))  \approx  \frac{1}{r}\sum_{l=1}^r \bigl(-\lambda_{\text{lr}} \textnormal{Tr}[\mathbf{H}(\mathbf{w}'^l)] - \lambda_{\text{lr}} \textnormal{Tr}[\mathbf{H}(\mathbf{w}'^l)\mathbf{H}(\mathbf{w}'^l)]  \bigr) + o_{\lambda_{\text{lr}}^2 \to 0}(1).
\end{equation}
\end{theorem}


\begin{theorem}\label{st:st2}
Оценка~\eqref{eq:var_elbo_entropy} на шаге оптимизации $\tau$ представима в виде
\begin{equation}
\label{eq:ev_grad_full}
 \frac{1}{r} \sum_{g = 1}^r L(\mathbf{w}^l_\tau| \mathbf{X}, \mathbf{y})  + \mathsf{S}\big(q^0(\mathbf{w})\bigr) + \frac{1}{r}\sum_{b=1}^\tau\sum_{l=1}^r \bigl(-\lambda_{\text{lr}} \textnormal{Tr}[\mathbf{H}(\mathbf{w}_b^l)] - \lambda_{\text{lr}}^2 \textnormal{Tr}[\mathbf{H}(\mathbf{w}_b^l)\mathbf{H}(\mathbf{w}_b^l)]  \bigr) 
\end{equation}
с точностью до слагаемых вида $o_{\lambda_{\text{lr}}^2 \to 0}(1)$,
где $\mathbf{w}_b^l$ --- $l$-я реализация параметров модели на шаге оптимизации $b$, $q^0(\mathbf{w})$ --- начальное распределение.
\end{theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

В \textbf{главе 3} рассматривается задача оптимизации гиперпараметров модели глубокого обучения. Для оптимизации гиперпараметров модели предлагаются алгоритмы, основанные на градиентном спуске. Так как сложность рассматриваемых алгоритмах сопоставима со сложностью оптимизации параметров модели, предлагается оптимизировать параметры и гиперпараметры в единой процедуре. Предполагается, что структура модели $\boldsymbol{\Gamma}$ для вероятностной модели глубокого обучения $\mathbf{f}$ и метапараметры $\lam$ определены однозначно~\eqref{eq:workaround}.

Пусть априорное распределение параметров имеет вид~\eqref{eq:prior}. Требуется найти параметры ${\boldsymbol{\theta}}^{*}$ и гиперпараметры $\mathbf{h}^{*}$ модели, доставляющие максимум следующей функции:
\begin{equation}
\label{eq:main}
    \mathbf{h}^{*} = \argmax_{\mathbf{h} \in \mathbb{H}} \Val[][][][{\teta}^{*}], \quad \boldsymbol{\theta}^{*}(\mathbf{h}) =  \argmax_{\boldsymbol{\theta} \in \Tetab} \Loss,
\end{equation}
где $L,Q$ --- функции потерь и валидации.

Перечислим алгоритмы оптимизации гиперпараметров, исследованные в этой главе:
\begin{enumerate}
\item Случайный поиск, стохастический метод. Неэффективен при большом количестве гиперпараметров в силу проклятия размерности.
\item Жадный алгоритм~(Luketina: 2016), градиентный метод. Доставляет локально-оптимальное решение задачи оптимизации. Позволяет производить одновременную оптимизацию параметров и гиперпараметров.
\item HOAG~(Pedregosa: 2016), градиентный метод. Алгоритм основан на приближении аналитического решения двухуровневой задачи оптимизации~\eqref{eq:main}.
\item DrMAD~(Fu: 2017), градиентный метод. Алгоритм работает в строгих предположениях о линейности траектории оптимизации гиперпараметров. Алгоритм позволяет также производить оптимизацию параметров оператора оптимизации.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
~\\
В \textbf{главе 4}  рассматривается задача выбора структуры модели глубокого обучения. Предлагается ввести вероятностные предположения о распределении параметров и распределении структуры модели. В качестве оптимизируемой функции для гиперпараметров модели предлагается обобщенная функция ее обоснованности. Показано, что данная функция оптимизирует ряд критериев выбора структуры модели: метод максимального правдоподобия, последовательное увеличение и снижению сложности модели, полный перебор структуры модели, а также получение максимума вариационной оценки обоснованности модели.

Определим априорные распределения параметров и структуры модели следующим образом.
Пусть для каждого ребра $(j,k) \in E$ и каждой базовой функции $\mathbf{g}^{j,k}_l$ параметры модели $\w^{j,k}_l$ распределены нормально с нулевым средним:
$
    \w^{j,k}_l \sim \mathcal{N}\bigl(\mathbf{0}, (\gamma^{j,k}_l)^2(\A^{j,k}_l)^{-1}\bigr),
$
где $ (\A^{j,k}_l)^{-1}$ --- диагональная матрица, $l \in \{0,\dots,K^{j,k}-1\}$, где $K^{j,k}$ --- количество базовых функций для ребра $K^{j,k}$. Априорное распределение $p(\w|\G, \h)$ параметров $\w^{j,k}_l$ зависит не только от гиперпараметров $\A_k^{j,k}$, но и от структурного параметра $\gamma^{j,k}_l \in (0,1)$.


В качестве априорного распределения для структуры $\G$ предлагается использовать произведение распределений Gumbel-Softmax ($\mathcal{GS}$):
$
    \priorG = \prod_{(j,k) \in E} p(\g^{j,k}|\s^{j,k}, \lamT), \quad 
$
где для каждого структурного параметра $\g^{j,k}$ с количеством базовых функций $K^{j,k}$ вероятность $p(\g^{j,k}|\s^{j,k}, \lamT)$ определена следующим образом:
\begin{multline}
\label{eq:gs_pdf}
    (K^{j,k}-1)!(\lamT)^{K^{h,j}-1}\prod_{l=0}^{K^{j,k}-1} s^{j,k}_l(\g^{j,k}_l)^{-\lamT -1}  \left(\sum_{l=0}^{K^{j,k}-1} s^{j,k}_l(\g^{j,k}_l)^{-\lamT}\right)^{-K^{j,k}},
\end{multline}
где $\s^{j,k} \in (0,\infty)^K{^{j,k}}$ --- гиперпараметр, отвечающий за смещенность плотности распределения относительно точек симплекса на $K^{j,k}$ вершинах, $\lamT > 0$ --- метапараметр температуры, отвечающий за концентрацию плотности вблизи вершин симплекса или в центре симплекса.


В качестве регуляризатора для матрицы $(\mathbf{A}^{j,k}_l)^{-1}$ предлагается использовать обратное гамма-распределение:
$
    (\A^{j,k}_l)^{-1} \sim \text{inv-gamma}(\lambda_1,\lambda_2),
$
где $\lambda_1,\lambda_2 \in \lam$ --- метапараметры оптимизации. 

Таким образом, предлагаемая вероятностная модель содержит следующие компоненты:
\begin{enumerate}
\item Параметры $\w$ модели, распределенные нормально.
\item Структура модели $\G$, содержащая все структурные параметры $\{\g^{j,k}, (j,k) \in E\}$, распределенные по распределению Gumbel-Softmax.
\item Гиперпараметры $\h = [\text{diag}(\A), \s]$, где $\A$ --- конкатенация матриц $\A^{j,k}, (j,k) \in E,$ $\s$ --- конкатенация параметров Gumbel-Softmax распределений $\s^{j,k}, (j,k) \in E$, где $E$ --- множество ребер, соответствующих графу рассматриваемого параметрического семейства моделей $\F$.
\item Метапараметры: $\lam = [\lambda_1, \lambda_2, \lamT].$ Эти параметры не подлежат оптимизации и задаются экспертно. 
\end{enumerate}

В качестве критерия выбора гиперпараметров предлагается использовать апостериорную вероятность гиперпараметров:
\begin{equation}
\label{eq:optimal_hyper}
    \posth \propto \EV \priorh \to \max_{\h \in \Hb}.
\end{equation}
Структура и параметры модели выбираются на основе полученных значений гиперпараметров:
$
    \w^*,\G^* = \argmax_{\w \in \Wb, \G \in \Gb} p(\w, \G|\y, \X, \h^*, \lam ),
$
где $\h^*$ --- решение задачи оптимизации~\eqref{eq:optimal_hyper}.

Для вычисления обоснованности модели $\EV$ из~\eqref{eq:optimal_hyper} предлагается использовать нижнюю вариационную оценку обоснованности.

\begin{theorem}
Пусть $\q = \qw \qG$ --- вариационное распределение c параметрами $\teta= [\tetaw, \tetaG ]$, аппроксимирующее апостериорное распределение структуры и параметров:
\[
   \qw  \approx \postw, \quad    \qG \approx \postG.
\]

Тогда справедлива следующая оценка:
\begin{equation}
\label{eq:full_elbo}
\log \EV \geq
\end{equation}
\[
 \E_{\q}  \log \LL - \KL{\qG}{\priorG} - 
\]
\[
 - \KL{\qw}{\priorw},
\]
где $\KL{\qw}{\priorw}$ вычисляется по формуле условной дивергенции:
\[
\KL{\qw}{\priorw} = \E_{\G \sim \qG} \E_{\w \sim \qw} \log \left(\frac{\qw}{\priorw}\right).
\]
\end{theorem}


Для анализа сложности полученной модели введем понятие \textit{параметрической сложности}. 
\begin{defin} 
Параметрической сложностью  $C_p(\teta|\Uh, \lam)$ модели с вариационными параметрами $\teta$ на компакте $\Uh \subset \Hb$ назовем минимальную дивергенцию между вариационным и априорным распределением:
\[
C_p(\teta|\Uh, \lam) = \min_{\h \in \Uh} \KL{\q}{\prior}.
\]
\end{defin}

Одним из критериев удаления неинформативных параметров в вероятностных моделях является отношение вариационной плотности параметров в нуле к вариационной плотности параметра в моде распределения.
\begin{defin}
Относительной вариационной   плотностью параметра $w \in \w$  при условии структуры $\G$ и гиперпараметров $\h$ назовем отношение вариационной плотности в моде априорного распределения параметра к вариационной плотности в моде вариационного распределения параметра:
\[
\rho(w|\G,\tetaw, \h,\lam)=\frac{\qw[\text{mode}~\priorw[w]]}{\qw[\text{mode}~{\qw[w]}]}.
\]
Относительной вариационной плотностью вектора параметров $\mathbf{w}$ назовем следующее выражение:
\begin{equation}
\label{eq:rho_general}
    {\rho}(\w|\G, \tetaw, \h,\lam) = \prod_{w \in \w}\rho(w|\G,\tetaw,\h,\lam).
\end{equation}

\end{defin}

\begin{theorem}
Пусть
\begin{enumerate}

\item Заданы компактные множества $\Uh \subset \Hb, \Utetaw \subset \Tetawb, \UtetaG \subset \TetaGb$.

\item Вариационное распределение $\qw$  является абсолютно непрерывным и унимодальным на  $U_{\boldsymbol{\theta}}$.
Его мода и матожидание совпадают.




\item Априорное распределение $\priorw$ является абсолютно непрерывным и унимодальным на  $U_\mathbf{h}$. Его мода и матожидание совпадают и не зависят от гиперпараметров $\h$  на $\Uh$ и структуры $\G$ на $\UtetaG$:
\[
\E_{\priorw}~\w = \text{mode}~\priorw[][\G_1][\h_1]=\text{mode}~\priorw[][\G_1][\h_2]=\mathbf{m}
\]
\text{ для любых }$~\h_1,\h_2 \in \Uh, \G_1,\G_2 \in \UG$.


\item Параметры модели $\w$ имеют конечные вторые моменты по маргинальным распределениям:
$
   \int_{\G}\qG\qw d\G, \quad \int_{\G}\qG\priorw d\G
$
при любых $\tetaw \in \Utetaw, \tetaG \in \UtetaG, \h \in \Uh.$


\item Вариационное распределение $\qw$ является липшицевым по $\w$.

\item Значение $\qw$ не равно нулю при любых $\teta \in \Uteta, \G \in \Gb$.

\item Точная нижняя грань $\inf_{\G \in \Gb, \tetaw \in \Utetaw} \qw[\mathbf{m}]$ не равна нулю.

%\item Вариационное распределение $\qw$ и априорное распределение $\priorw$  являются абсолютно непрерывными.

\item Решение задачи $
\h^{*} = \argmin_{\h \in \Uh} \KL{\q}{\prior}$ единственно для любого $\teta \in \Uteta$.



\item Задана бесконечная последовательность векторов вариационных параметров $\teta[1],\teta[2],\dots,\teta[i],\dots \in \Uteta$, такая, что $\lim_{i \to \infty}C_p(\teta[i]|\Uh,\lam) = 0.$
\end{enumerate}
Тогда следующее выражение стремится к единице:
$
   \lim_{i \to \infty} \E_{\qG[][{\tetaG[i]}]} {\rho}(\w|\G, {\tetaw[i]}, {\h[i]}, \lam)^{-1}.
$

\end{theorem}

Рассмотрим основные статистические критерии выбора вероятностных моделей. 
\begin{enumerate}
\item Критерий максимального правдоподобия:
$\log \LL \to \max_{\w \in \Uw, \G \in \UG}.$
Для использования данного критерия в качестве задачи выбора модели предлагается следующее обобщение:
\begin{equation}
\label{eq:optim_ml}
    \Loss =  \E_{\q} \log~\LL.
\end{equation}
%Данное обобщение~\eqref{eq:optim_ml} эквивалентно  критерию максимального правдоподобия при выборе в качестве $\q$ распределения, основанного на запуске нескольких  оптимизаций параметров~\eqref{eq:loss_func} и структуры.
Метод не предполагает оптимизации гиперпараметров $\h$. Для формального соответствия данной задачи задаче выбора модели~\eqref{eq:optim_problem}, положим $\Loss=\Val:$
\[
    \Loss =  \E_{\q}\log \LL \to \max_{\teta \in \Uteta},
\]
\[
    \Val =  \E_{\q}\log \LL \to \max_{\h \in \Uh}.
\]



\item Метод максимальной апостериорной вероятности:
$\log \LL \prior \to \max_{\w  \in \Uw, \G \in \UG}.$
Аналогично предыдущему методу сформулируем вариационное обобщение данной задачи:
\begin{equation}
\label{eq:optim_map}
\Loss = \Val = 
\end{equation}
\[
 = \E_{\q} \bigl( \log\LL + \log\prior \bigr).
\]
Т.к. в рамках данной задачи~\eqref{eq:optim_map} не предполагается оптимизации гиперпараметров $\h$, положим параметры распределения $\prior$ фиксированными:
$
   \lam = [\lambda_1, \lambda_2, \lamT, \s, \text{diag}(\A)].
$

\item Полный перебор структуры:
\begin{equation}
\label{eq:optim_struct}
    \Loss = \Val = \E_{\q} \log \LL [\qG = p']
\end{equation}
где $p'$ --- некоторое распределение на структуре $\G$, выступающее в качестве метапараметра.




\item Критерий Акаике:
$
   \text{AIC} =  2\log \LL - 2|\Wb| \to \max.
$
Для использования критерия Акаике для сравнения моделей, принадлежащих одному параметрическому семейству~$\F$ предлагается следующая переформулировка:
\begin{equation}
\label{eq:optim_aic}
    \Loss = \Val = \E_{\q}\log \LL - 
\end{equation}
\[
 - |\{w: \KL{\qw[w]}{\priorw[w]}<\lambda_\text{prune}\}|,
\]
где 
\begin{equation}\label{eq:aic_compl}\h=\argmin_{\h' \in \Uh} \KL{\q}{\prior},\end{equation} $\lambda_{\text{prune}}$ --- метапараметр алгоритма, $\Uh  \subset \Hb$ --- область определения задачи по гиперпараметрам. Предложенное обобщение~\eqref{eq:optim_aic} применимо только в случае, если выражение~\eqref{eq:aic_compl} определено однозначно, т.е. существует единственный вектор гиперпараметров  $\h \in \Uh,$ доставляющий минимум дивергенции $\KL{\q}{\prior}.$

\item Информационный критерий Шварца:
$
    \text{BIC} = 2\log \LL - |\Wb| \log m   \to \max.
$
Переформулируем данный критерий аналогично критерию AIC:
\begin{equation}
\label{eq:optim_bic}
    \Loss = \Val = \log \E_{\q}\LL - 
\end{equation}
\[
 - 0.5 \log m |\{w: \KL{\qw[w]}{\priorw[w]}\}|,
\]
метапараметр $\lambda_{\text{prune}}$ определен аналогично~\eqref{eq:aic_compl}.

\item Метод вариационной оценки обоснованности:
\begin{equation}
\label{eq:optim_elbo_method}   
    \Loss = 
\end{equation}
\[
= \E_{\q} \log \LL - \KL{\q}{\prior} + 
\]
\[
+\log\priorh \to \max_{\teta \in \Uteta}, \quad    \Val = 
\]
\[
= \E_{\q} \log \LL - \KL{\q}{\prior} +
\]
\[+ \log\priorh \to \max_{\h \in \Uh},
\]
В рамках данной задачи функции $\Loss$ и $\Val$ совпадают, все гиперпараметры $\h$ подлежат оптимизации.

\item Валидация на отложенной выборке:
\begin{equation}
\label{eq:optim_hold_out}
    \Loss = \E_{\q} \log \LL[\y_\text{train}][\X_\text{train}] + \log \prior \to \max_{\teta \in \Uteta},
\end{equation}
\[
    \Val = \E_{\q} \log \LL[\y_\text{test}][\X_\text{test}] \to \max_{\h \in \Uh},
\]
где $(\X_\text{train}, \y_\text{train}), (\X_\text{test}, \y_\text{test})$ --- разбиение выборки на обучающую и контрольную подвыборку.
В рамках данной задачи все гиперпараметры $\h$ подлежат оптимизации.

\end{enumerate}

\begin{defin}
Двухуровневую задачу оптимизации будем называть \textit{обобщающей} на компакте $U = \Utetaw \times \UtetaG \times \Uh \times \Ulam \subset \Tetawb \times \TetaGb \times \Hb \times \Lamb,$ если она удовлетворяет следующим критериям.
\begin{enumerate}
\item Область определения каждого параметра $w \in \w$, гиперпараметра $h \in \h$ и метапараметра $\lambda \in \lam$ не  является пустым множеством и не является точкой.
\item Для каждого значения гиперпараметров $\h$ оптимальное решение нижней задачи оптимизации~\eqref{eq:optim_problem} 
$
\teta^{*}(\h) = \argmax_{\teta \in \Tetab} \Loss
$
определено однозначно при любых значениях метапараметров $\lam \in \Ulam$.

\item Критерий максимизации правдоподобия выборки: существует $\lam \in \Ulam$ и  $K_1>0$,$K_1 < \max_{\h_1,\h_2 \in \Uh} \Val[\h_1][][][\teta^{*}(\h_1)] - \Val[\h_2][][][\teta^{*}(\h_2)],$ такие, что для любых векторов гиперпараметров $\h_1, \h_2 \in \Uh,$ удовлетворяющих неравенству $\Val[\h_1][][][\teta^{*}(\h_1)] - \Val[\h_2][][][\teta^{*}(\h_2)] > K_1,$ выполняется неравенство $\E_{\q[\teta^{*}(\h_1)]}\log\LL > \E_{\q[\teta^{*}(\h_2)]}\log\LL.$

\item Критерий минимизации параметрической сложности:  существует  $\lam \in \Ulam$ и $K_2>0,$ $K_2 < \max_{\h_1, \h_2 \in \Uh} \Val[\h_1][][][\teta^{*}(\h_1)] - \Val[\h_2][][][\teta^{*}(\h_2)],$ такие, что для любых векторов гиперпараметров $\h_1,\h_2 \in \Uh$, удовлетворяющих неравенству $\Val[\h_1][][][\teta^{*}(\h_1)] - \Val[\h_2][][][\teta^{*}(\h_2)]>K_2,$ параметрическая сложность первой модели меньше, чем второй: $C_p(\teta^{*}(\h_1)|\Uh,\lam)<C_p(\teta^{*}(h_2)|\Uh,\lam).$

\item Критерий приближения оценки обоснованности: существует значение гиперпараметров $\lam$, такое, что значение функций потерь $\Val$ как сложной функции от $\Loss$ пропорционально вариационной оценки обоснованности модели: $$\Val[][][][\teta^{*}(\h)] \propto $$
$$\propto
\E_{\q[\teta'(\h)]}\log\LL - \KL{\q[\teta'(\h)]}{\prior} + \log\priorh$$ для всех $\h \in \Uh,$
где в качестве гиперпараметров $\h$ рассматриваются все гиперпараметры модели, вне зависимости от критерия и особенности оптимизации гиперпараметров, соответствующих критерию: $\h = [\A, \s],$
где $$\teta'(\h) = \argmax_{\teta \in \Uh} \E_{\q[\teta]}\log\LL - \KL{\q[\teta]}{\prior}.$$

\item Критерий перебора оптимальных структур: существует константа $K_3>0$ и набор метапараметров $\lam$, такие, что существует хотя бы одна пара гиперпараметров $\h_1, \h_2 \in \Uh,$ удовлетворяющая неравенствам:
$$\KL{\priorG[][\h_1]}{\priorG[][\h_2]} > K_3,\KL{\priorG[][\h_2]}{\priorG[][\h_1]}>K_3,$$ и для произвольных локальных оптимумов  $\h_1,\h_2$ задачи оптимизации $\Val$, полученных при метапараметрах $\lam$ и удовлетворяющих неравенствам $$\KL{\priorG[][\h_1]}{\priorG[][\h_2]} > K_3, \KL{\priorG[][\h_2]}{\priorG[][\h_1]}>K_3,$$$$\Val[\h_1] > \Val[\h_2],$$  существует значение метапараметров $\lam' \neq \lam$, такое, что
\begin{enumerate}
\item соответствие между вариационными параметрами $\teta^{*}(\h_1),\teta^{*}(\h_2)$ сохраняется при  $\lam'$,
\item выполняется неравенство $\Val[\h_1][][][][\lam'] < \Val[\h_2][][][][\lam']$.
\end{enumerate}


\item Критерий непрерывности: функции $\Loss$ и $\Val$ непрерывны по метапараметрам $\lam \in \Ulam$.
\end{enumerate}
\end{defin}
%Первый критерий является техническим и используется для исключения из рассмотрения вырожденных задач оптимизации.  
%Второй критерий говорит о том, что решение первого и второго уровня должны быть согласованы и определены однозначно.
%Критерии 3-5 определяют возможные критерии оптимизации, которые должны приближаться обобщающей задачей.
%Критерий 6 говорит о возможности перехода между различными структурами модели. Данный критерий говорит о том, что мы можем перейти от одного набора гиперпараметров $\h_1$ к другим $\h_2$, если они соответствуют локальным оптимумам задачи оптимизации, и дивергенция соответствующих априорных  распределений на структурах $\priorG$ значимо высока. При этом соответствующие вариационные распределения $\qG$ могут оказаться достаточно близки, несмотря на значимые различия априорных распределений. Поэтому возможным дополнением этого критерия был бы критерий, позволяющий переходить от структуры к структуре, если соответствующие распределения $\qG$ различаются значимо.
%Последний критерий говорит о том, что обобщающая задача должна позволять производить переход между различными методами выбора  параметров и структуры модели непрерывно.

\begin{theorem}Рассмотренные задачи~\eqref{eq:optim_ml},\eqref{eq:optim_map},\eqref{eq:optim_struct},\eqref{eq:optim_aic},\eqref{eq:optim_bic},\eqref{eq:optim_hold_out} не являются обобщающими.
\end{theorem}

\begin{theorem}
Пусть $q_{\boldsymbol{\Gamma}}$ --- абсолютно непрерывное распределение с дифференцируемой плотностью, такой, что:
\begin{enumerate}
\item Градиент плотности $\nabla_{\boldsymbol{\theta}_{\boldsymbol{\Gamma}}} q(\boldsymbol{\Gamma}|\boldsymbol{\theta}_{\boldsymbol{\Gamma}})$ является ненулевым почти всюду.
\item Выражение $\nabla_{\boldsymbol{\theta}_{\boldsymbol{\Gamma}}} q(\boldsymbol{\Gamma}|\boldsymbol{\theta}_{\boldsymbol{\Gamma}}) \text{log}~p(\boldsymbol{\Gamma}|\mathbf{h}, \boldsymbol{\lambda})$ ограничено на $U_{\boldsymbol{\theta}}$ абсолютно непрерывной случайной величиной, не зависящей от $\G$, с конечным первым моментом.
\end{enumerate}
Тогда задача~\eqref{eq:optim_elbo_method} не является обобщающей.
\end{theorem}


В качестве обобщающей задачи оптимизации предлагается оптимизационную задачу следующего вида:
\begin{equation}
\label{eq:qopt}
\h^{*} = \argmax_{\h} \Val = 
\end{equation}
\[
= \lamLL \E_{\q[\teta^{*}]} \log\LL - \lamCQ \KL{\q[\teta^{*}]}{\prior} -
\]
\[
    - \sum_{p' \in \mathfrak{P},\lambda \in \lamS} \lambda\KL{\q[\teta^{*}]}{p'} + \log\priorh, 
\]
\begin{equation}
\label{eq:lopt}
%\tag{$L^{*}$}
{\teta}^{*} = \argmax_{\teta} \Loss = 
\end{equation}
\[=
\E_{\q} \log \LL - \lamCL \KL{\q[\teta^{*}]}{\prior},
\]
где $\mathfrak{P}$ --- непустое множество распределений на структуре $\G$, $\lamCQ, \lamCL, \lamS$~---~некоторые числа. Множество распределений $\mathfrak{P}$ отвечает за перебор структур $\G$ в процессе оптимизации модели.
В предельном случае, когда температура $\lamT$ близка к нулю, а множество $\mathfrak{P}$ состоит из распределений, близких к дискретным, соответствующим всем возможным структурам, калибровка $\lamS$ порождает последовательность задач оптимизаций, схожую с перебором структур.

\begin{theorem}
Пусть
\begin{enumerate}%[label={\arabic*)}] 

\item Задан компакт  $U = \Utetaw \times \UtetaG \times \Uh \times \Ulam,$ где априорное распределение $\prior$ и распределение $\priorh$ непрерывны на $\Uh \times \Ulam$.

\item Задано непустое множество  $\mathfrak{P}$ абсолютно непрерывных распределений на структуре, чьи плотности непрерывны и не принимают нулевое значение, где хотя бы одно распределение $p_1 \in \mathfrak{P}$ является Gumbel-Softmax распределением, и для каждого значения $\s \in \Uh, \lamT \in \Ulam$, существует значение параметров распределения $p_1$, такое, что $p_1 = \priorG$. Параметры распределений $p \in \mathfrak{P}$ принадлежат множеству метапараметров $\lam \in \Ulam$.

\item Вариационное распределение $\q$ является  абсолютно непрерывным, плотность которого непрерывна по метапараметрам $\boldsymbol{\lambda} \in \Ulam$ и не принимает нулевое значение.

\item Область определения каждого параметра $w \in\w$, гиперпараметра $h \in \h$ и метапараметра $\lambda \in \lam$ не является пустым и не является точкой.

\item Для каждого значения гиперпараметров $\h \in \Uh$ оптимальное решение нижней задачи оптимизации $\teta^{*}$ определено однозначно на $\Uteta = \Utetaw \times \UtetaG$ при любых значениях метапараметров $\lam \in \Ulam$.

\item Область значений метапараметров $\lamLL, \lamCQ, \lamCL, \lamS$ включает отрезок от нуля до единицы.

\item Существует значение метапараметров $\lambda_1>0, \lambda_2>0, \lamLL>0  \in \Ulam,$ такое, что
\[
\max_{\h \in \Uh} \log \priorh -\min_{\h \in \Uh}\log \priorh < \max_{\h \in \Uh} \Val - \min_{\h \in \Uh} \Val
\] 
при $\lamS = \mathbf{0}, \lamCQ = 0$.

\item Существует значение метапараметров $\lamCL>0, \lamCQ>0, \lambda_1>0$, $\lambda_2>0, \lamT>0 \in \Ulam,$ такое, что 
\[
    \max_{\h \in \Uh} \frac{1}{\lamCQ}\log  \priorh - \min_{\h \in \Uh} \frac{1}{\lamCQ}\log  \priorh +
\]
\[
 + \max_{\h \in \Uh} \min_{\teta \in \Uteta} \KL{\q}{\prior} -
\]
\[ -\min_{\h \in \Uh, \teta \in \Uteta}  \KL{\q}{\prior} + \max_{\teta \in \Uteta}\frac{1}{\lamCL}\E_{\q} \log \LL - 
\]
\[
 - \min_{\teta \in \Uteta}\frac{1}{\lamCL}\E_{\q} \log \LL  <
\]
\[ 
< \max_{\teta \in \Uteta, \h \in \Uh} \KL{\q}{\prior} -
\]
\[
-\min_{\teta \in \Uteta, \h \in \Uh} \KL{\q}{\prior}
\]
при $\lamS = \mathbf{0}, \lamLL = 0.$

\item Существуют значения метапараметров $\lamCQ>0,\lamLL>0, \lambda_1>0, \lambda_2>0, \lamT>0  \in \Ulam,$ такие, что существуют гиперпараметры $\h_1, \h_2 \in \Uh$:
\[
\KL{\prior[][][\h_1]}{\prior[][][\h_2]} > 
\]
\[
>\frac{\max_{\h} \Val - \min_{\h} \Val }{m_{\lambda}},
\]
\[
\KL{\prior[][][\h_2]}{\prior[][][\h_1]} >
\]
\[
> \frac{\max_{\h} \Val - \min_{\h} \Val }{m_{\lambda}}
\]
при $\lamS = \mathbf{0},$ где $m_{\lambda}$ --- максимальное значение $\lamS$ перед распределением $p_1$ из первого условия теоремы.

\end{enumerate}
Тогда задача~\eqref{eq:qopt} является обобщающей на $U$.
\end{theorem}



Следующие теоремы говорят о соответствии предлагаемой обобщающей задачи вероятностной модели. В частности, задача оптимизации параметров и гиперпараметров соответствует двухуровневому байесовскому выводу.
~\\
\begin{theorem}
Пусть ${\lamCQ} = \lamCL=\lamLL = 1, \lamS=\mathbf{0}$. Тогда:
\vspace{0.1cm}
\begin{enumerate}
\item Задача оптимизации~\eqref{eq:qopt} доставляет максимум апостериорной вероятности гиперпараметров с использованием вариационной оценки обоснованности:
\[
    \E_{\q}\log \LL - \KL{\q}{\prior} +
\]
\[
+ \log \prior \to \max_{\h}.
\]
\item Вариационное распределение $\q$ приближает апостериорное распределение $\post$ наилучшим образом:
\[
    \KL{\q}{\post} \to \min_{\teta}.
\]


\item Если существуют такие значения параметров $\tetaw, \tetaG,$ что $\postw = \qw, \postG = \qG,$
то решение задачи оптимизации $\Loss$ доставляет эти значения вариационных параметров.  
\end{enumerate}
\end{theorem}

Докажем, что варьирование коэффициента $\lamCL$ приводит к оптимизации вариационной оценки обоснованности для выборки из той же генеральной совокупности, но другой мощности.
\vspace{0.1cm}
\begin{theorem}
\label{th:elbo_size}
Пусть $m \gg 0$, $\lamCL > 0, \frac{m}{\lamCL}   \in \mathbb{N}, \frac{m}{\lamCL}  \gg 0.$ Тогда оптимизация функции
\[
\Loss = \E_{\q} \log \LL - \lamCL\KL{\q}{\prior}
\]
 эквивалентна оптимизации вариационной оценки обоснованности  $$\E_{\q} \log \LL[\hat{\y}][\hat{\X}] - \KL{\q}{\prior}$$
для произвольной случайной подвыборки $\hat{\y}, \hat{\X}$ мощности $\frac{m}{{\lamCL}}$ из генеральной совокупности.
\end{theorem}


\begin{theorem}
Пусть
\begin{enumerate}
\item Задан компакт $U = \Uh \times \Uteta$ и  $\lamS=\mathbf{0}$. 
\item Решение задачи
\begin{equation}
\label{eq:cp_theorem_kl}
\min_{\h \in \Uh} \KL{\q[\teta_2]}{\prior}
\end{equation}
 является единственным для некоторых ${\lamCQ}_1, {\lamCQ}_2, {\lamCQ}_1 > {\lamCQ}_2$ на $U$ при некоторых фиксированных $\lamLL, \lamCL, \lamT, \lambda_1, \lambda_2$.

\item Решения задачи~\eqref{eq:qopt},\eqref{eq:lopt} являются единственными на $U$ при ${\lamCQ}_1, {\lamCQ}_2$ и $\lamLL, \lamCL, \lamT, \lambda_1, \lambda_2$.
\item Функция $\Val[][\teta_2]$ является вогнутой по $\h \in \Uh$ при  $\lamCQ = {\lamCQ}_2$.
\item Решение задачи~\eqref{eq:cp_theorem_kl}  единственно при  $\lamCQ = {\lamCQ}_2$.
\item Все стационарные точки $\teta \in \Uteta$  функции $\Loss$ являются решениями нижней задачи оптимизации при  $\lamCQ = {\lamCQ}_2$ с обратимым гессианом.
\item Значения $\priorh$ приблизительно равны на $\Uh$: 
$
    \priorh[\h_1] \approx \priorh[\h_2] \text{ для всех }\h_1, \h_2 \in \Uh. 
$
\end{enumerate}
Тогда справедлива следующая оценка разности параметрических сложностей:
\[
    C_p(\teta_1|\Uh, \lam_1) - C_p(\teta_2|\Uh, \lam_2)  < \frac{\lamCL }{{\lamCQ}_2} ({\lamCQ}_2- \lamCL) \times  
\]
\[
 \times \max_{\h \in \Uh, \teta \in \Uteta}  \nabla_{\teta, \h} (\KL{\q}{[\prior})^\mathsf{T}  \nabla_{\teta}^2(\Loss[][][][][\lam_2])^\mathsf{-1} \times
\]
\[\times \nabla_{\teta} \KL{\q}{\prior} \max_{\h_1, \h_2 \in \Uh} ||\h_1 - \h_2||.
\]
\end{theorem}

\begin{theorem}
\label{theorem:onelvl}
Пусть $\frac{\lamCQ}{\lamLL} = \lamCL$. 
Тогда задача оптимизации~\eqref{eq:qopt} представима в виде одноуровневой задачи оптимизации:
\[
\lamLL \E_{\q}\LL - \lamCQ \KL{\q}{\prior} -
\]
\[
-\sum_{p' \in \mathfrak{P}, \lambda \in \lamS} \KL{\priorG}{p'} - \log \priorh \to \max_{\h, \teta}. 
\]
\end{theorem}


В \textbf{главе 5} продемонстрировано применение предложенных методов к прикладным задачам классификации и регрессии, задаче определения схожести предложений на основе их векторных представлений, а также к задачам прореживания моделей глубокого обучения.

\vspace{0.5cm}
\textbf{В заключении} представлены основные результаты диссертационной работы.

\begin{enumerate}
\item Предложен метод байесовского выбора оптимальной и субоптимальной структуры модели глубокого обучения с использованием автоматического определения релевантности параметров.
\item Предложены критерии оптимальной и субоптимальной сложности модели глубокого обучения.
\item Предложен метод графового описания моделей глубокого обучения.
Предложено обобщение задачи оптимизации структуры модели, включающее ранее описанные методы выбора модели: оптимизация обоснованности модели, последовательное увеличение сложности модели, последовательное снижение сложности модели, полный перебор вариантов структуры модели.
\item Предложен метод оптимизации вариационной оценки обоснованности модели на основе метода мультистарта задачи оптимизации.
\item Предложен алгоритм оптимизации параметров, гиперпараметров и структурных параметров моделей глубокого обучения.
\item Исследованы свойства оптимизационной задачи при различных значениях метапараметров. Рассмотрены ее асимптотические свойства.
\item Рассмотрено применение предложенных  методов для построения моделей глубокого обучения в прикладных задачах регрессии и классификации.
\end{enumerate}

\subsection*{Публикации соискателя по теме диссертации}
Публикации в журналах из списка ВАК.
\vspace{0.3cm}

\begin{enumerate}
\item Бахтеев О.Ю., Попова М.С., Стрижов В.В., “Системы и средства глубокого обучения в задачах классификации”, Системы и средства информатики, 26:2 (2016), 4–22.
\item Bakhteev, O., Kuznetsova, R., Romanov, A. and Khritankov, A., 2015, November. A monolingual approach to detection of text reuse in Russian-English collection. In 2015 Artificial Intelligence and Natural Language and Information Extraction, Social Media and Web Search FRUCT Conference (AINL-ISMW FRUCT) (pp. 3-10). IEEE.
\item Romanov, A., Kuznetsova, R., Bakhteev, O. and Khritankov, A., 2016. Machine-Translated Text Detection in a Collection of Russian Scientific Papers. Computational Linguistics and Intellectual Technologies. 2016. 
\item Bakhteev, O. and Khazov, A., 2017. Author Masking using Sequence-to-Sequence Models. In CLEF (Working Notes). 2017.
\item Бахтеев О.Ю., Стрижов В.В., “Выбор моделей глубокого обучения субоптимальной сложности”, Автоматика и телемеханика, 2018, № 8, 129–147; Automation Remote Control, 79:8 (2018), 1474–1488.
\item Огальцов А.В., Бахтеев О.Ю., “Автоматическое извлечение метаданных из научных PDF-документов”, Информатика и её применения, 12:2 (2018), 75–82.
\item Смердов А.Н., Бахтеев  О.Ю., Стрижов В.В., “Выбор оптимальной модели рекуррентной сети в задачах поиска парафраза”, Информатика и её применения, 12:4 (2018), 63–69.
\item Грабовой А.В., Бахтеев О.Ю., Стрижов В.В. “Определение релевантности параметров нейросети”, Информатика и её применения. 13:2 (2019), 62-71.
\item Bakhteev, O.Y. and Strijov, V.V., 2019. Comprehensive analysis of gradient-based hyperparameter optimization algorithms. Annals of Operations Research, pp.1-15.
\end{enumerate}
\vspace{0.4cm}
{Прочие публикации.}
\vspace{0.2cm}

\begin{enumerate}
\setcounter{enumi}{9}
\item Бахтеев О.Ю. Восстановление панельной матрицы и ранжирующей модели по метризованной выборке в разнородных данных. // Машинное обучение и анализ данных. 2016. № 7. С. 72-77.
\item Бахтеев О.Ю. Восстановление пропущенных значений в разнородных шкалах с большим числом пропусков. // Машинное обучение и анализ данных. 2015. № 11. С. 1-11.
\end{enumerate}
\bibliographystyle{gost780s}
%\bibliography{dis_literature}
\end{document}
