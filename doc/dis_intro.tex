\newpage{}
\addcontentsline{toc}{section}{Введение}
\chapter*{Введение}


\textbf{Актуальность темы.} В работе рассматривается задача автоматического построения моделей глубокого обучения оптимальной и субоптимальной сложности. 

Под сложностью модели понимается \emph{минимальная длина описания}~\cite{mdl}, т.е. минимальное количество информации, которое требуется для передачи информации о модели и о выборке. Вычисление минимальной длины описания модели является вычислительно сложной процедурой. В работе предлагается получение ее приближенной оценки, основанной на связи минимальной длины описания и \emph{обоснованности модели}~\cite{mdl}. Для получения оценки обоснованности используются вариационные методы получения оценки обоснованности~\cite{bishop}, основанные на аппроксимации неизвестного апостериорного распределения другим заданным распределением. Под субоптимальной сложностью понимается вариационная оценка обоснованности модели.

Одна из проблем построения моделей глубокого обучения --- большое количество параметров моделей~\cite{hinton_rbm, hinton_init}. Поэтому задача выбора моделей глубокого обучения включает в себя выбор стратегии построения модели, эффективной по вычислительным ресурсам. В работе~\cite{greed} приводятся теоретические оценки построения нейросетей с использованием жадных стратегий,  при которых построение модели производится итеративно последовательным увеличением числа нейронов в сети. В работе~\cite{greed_mlp} предлагается жадная стратегия выбора модели нейросети с использованием релевантных априорных распределений, т.е. параметрических распределений, оптимизация параметров которых позволяет удалить часть параметров из модели. Данный метод был также применялся в задаче построения модели метода релевантных векторов~\cite{rvm}. Альтернативой данным алгоритмам построения моделей являются методы, основанные на прореживании сетей глубокого обучения~\cite{obd, popova, nvidia_prune}, т.е. последовательного удаления параметров, не дающих существенного прироста качества модели. 
В работах~\cite{Bengio, hd} рассматривается послойное построение модели с отдельным критерием оптимизации для каждого слоя. В работах~\cite{Kingma, gendis_pictures, gendis_phd} предлагается декомпозиция модели на порождающую и разделяющую, оптимизируемые последовательно. В работе~\cite{adanet} предлагается метод автоматического построения сети, основанный на бустинге. В качестве оптимизируемого функционала предлагается линейная комбинация функции правдоподобия выборки и сложности модели по Радемахеру. 
В работах~\cite{reinf,reinf_predict,reinf_deep2net,reinf_transfer} предлагается метод автоматического построения сверточной сети с использованием обучения с подкреплением. В~\cite{darts} используется схожее представление сверточной сети, вместо обучения с подкреплением используется градиентная оптимизация параметров, задающих структуру нейронной сети.

В качестве порождающих моделей в сетях глубокого обучения выступают ограниченные машины Больцмана~\cite{hinton_rbm} и автокодировщики~\cite{founds}. В работе~\cite{contractive} рассматриваются некоторые типы регуляризации автокодировщиков, позволяющие формально рассматривать данные модели как порождающие модели с использованием байесовского вывода. В работе~\cite{score} также рассматриваются регуляризованные автокодировщики и свойства оценок их правдоподобия. В работе~\cite{vae} предлагается обобщение автокодировщика с использованием вариационного байесовского вывода~\cite{bishop}. В работе~\cite{train_generative} рассматриваются модификации вариационного автокодировщика и ступенчатых сетей (англ. ladder network)~\cite{ladder} для случая построения многослойных порождающих моделей. 

В качестве критерия выбора модели в ряде работ~\cite{mackay,bishop,tokmakova,zaitsev,strijov_webber, strijov_dsc} выступает обоснованность модели. В работах~\cite{tokmakova,zaitsev,strijov_webber, strijov_dsc} рассматривается проблема выбора модели и оценки гиперпараметров в задачах регрессии. Альтернативным критерием выбора модели является минимальная длина описания~\cite{mdl}, являющаяся показателем статистической сложности модели и заданной выборки. 
В работе~\cite{perekrestenko} рассматривается перечень критериев сложности моделей глубокого обучения и их взаимосвязь. В работе~\cite{vladis} в качестве критерия сложности модели выступает показатель нелинейности, характеризуемый степенью полинома Чебышева, аппроксимирующего функцию. В работе~\cite{need_prune} анализируется показатель избыточности параметров сети. Утверждается, что по небольшому набору параметров в глубокой сети с большим количеством избыточных параметров можно спрогнозировать значения остальных. В работе~\cite{rob} рассматривается показатель робастности моделей, а также его взаимосвязь с топологией выборки и классами функций, в частности рассматривается влияние функции ошибки и ее липшицевой константы на робастность моделей. Схожие идеи были рассмотрены в работе~\cite{intrig}, в которой исследуется устойчивость классификации модели под действием шума. 

Одним из методов получения приближенного значения обоснованности является вариационный метод получения нижней оценки интеграла~\cite{bishop}. В работе~\cite{hoffman} рассматривается стохастическая версия вариационного метода. В работе~\cite{nips} рассматривается алгоритм получения вариационной нижней оценки обоснованности  для оптимизации гиперпараметров моделей глубокого обучения. В работе~\cite{varmc} рассматривается получение вариационной нижней оценки интеграла с использованием модификации методов Монте-Карло. В работе~\cite{early} рассматривается стохастический градиентный спуск в качестве оператора, порождающего распределение, аппроксимирующее апостериорное распределение параметров модели. Схожий подход рассматривается в работе~\cite{sgd_cont}, где также рассматривается стохастический градиентный спуск в качестве оператора, порождающего апостериорное распределение параметров. В работе~\cite{langevin} предлагается модификация стохастического градиентного спуска, аппроксимирующая апостериорное распределение. 

Альтернативным методом выбора модели является выбор модели на основе скользящего контроля~\cite{cv_ms, tokmakova}. Проблемой такого подхода является возможная высокая вычислительная сложность~\cite{expensive, expensive2}. В работах~\cite{bias,bias2} рассматривается проблема смещения оценок качества модели при гиперпараметрах, получаемых с использованием $k$-fold метода скользящего контроля, при котором выборка делится на $k$ частей с обучением на $k-1$ части и валидацией результата на оставшейся части выборки. 

Задачей, связанной с проблемой выбора модели, является задача оптимизации гиперпараметров~\cite{mackay,bishop}. В работе~\cite{tokmakova} рассматривается оптимизация гиперпараметров с использованием метода скользящего контроля и методов оптимизации обоснованности моделей, отмечается низкая скорость сходимости гиперпараметров при использовании метода скользящего контроля. В ряде работ~\cite{hyper, hyper2} рассматриваются градиентные методы оптимизации гиперпараметров, позволяющие оптимизировать большое количество гиперпараметров одновременно. В работе~\cite{hyper} предлагается метод оптимизации гиперпараметров с использованием градиентного спуска с моментом, в качестве оптимизируемого функционала рассматривается ошибка на валидационной части выборки. В работе~\cite{approx_hyper} предлагается метод аппроксимации градиента функции потерь по гиперпараметрам, позволяющий использовать градиентные методы в задаче оптимизации гиперпараметров на больших выборках. В работе~\cite{greed_hyper} предлагается упрощенный метод оптимизации гиперпараметров с градиентным спуском: вместо всей истории обновлений параметров для оптимизации используется только последнее обновление. В работе~\cite{sgd_cont} рассматривается задача оптимизации параметров градиентного спуска с использованием нижней вариационной оценки обоснованности. 


\vspace{0.5cm}
\textbf{Цели работы.}
\vspace{0.2cm}
\begin{enumerate}
\item Исследовать методы построения моделей глубокого обучения оптимальной и оптимальной сложности.
%\item Проанализировать различные подходы к решению задачи автоматического построения моделей глубокого обучения и оптимизации параметров модели.
\item Предложить критерии оптимальной и субоптимальной сложности модели глубокого обучения.
\item Предложить метод выбора субоптимальной структуры модели глубокого обучения.
\item Предложить алгоритм построения модели субоптимальной сложности и оптимизации параметров.
%\item Предложить алгоритм построения модели субоптимальной сложности и оптимизации параметров модели и
\end{enumerate}


\vspace{0.5cm}
\textbf{Методы исследования.} Для достижения поставленных целей используются методы вариационного байесовского вывода~\cite{mackay, bishop, early}. Рассматривается графовое представление нейронной сети~\cite{reinf,darts}. Для получения вариационных оценок обоснованности модели используется метод, основанный на градиентном спуске~\cite{sgd_cont, early}. В качестве метода получения модели субоптимальной сложности используется метод автоматического определения релевантности параметров~\cite{mackay,vae_ard} с использованием градиентных методов оптимизации гиперпараметров~\cite{hyper, hyper2, greed_hyper, approx_hyper}.

\vspace{0.5cm}
\textbf{Основные положения, выносимые на защиту.}
\vspace{0.3cm}
\begin{enumerate}
\item Предложен метод байесовского выбора субоптимальной структуры модели
глубокого обучения с использованием автоматического определения
релевантности параметров.
\item Предложены критерии оптимальной и субоптимальной сложности модели глубокого обучения.
\item Предложен метод графового описания моделей глубокого обучения.
%\item Проведено исследование свойства оптимизационных алгоритмов выбора модели.
\item Предложена задача оптимизации модели, обобщающая ранее
описанные методы выбора модели:
оптимизация обоснованности модели,
последовательное увеличение сложности модели,
последовательное снижение сложности модели,
полный перебор вариантов структуры модели.
\item Предложен метод оптимизации вариационной оценки обоснованности на основе мультистарта оптимизации модели.
\item Предложен алгоритм оптимизации параметров, гиперпараметров и структурных параметров моделей глубокого обучения.
\item Проведено исследование свойств оптимизационной задачи при различных значениях метапараметров. Рассмотрены ее асимптотические свойства.
\end{enumerate}


\vspace{0.5cm}
\textbf{Научная новизна.} Разработан новый подход к построению моделей глубокого обучения. Предложены критерии субоптимальной и оптимальной сложности модели, а также исследована их связь. Предложен метод построения модели глубокого обучения субоптимальной сложности. Исследованы методы оптимизации гиперпараметров и параметров модели.  Предложена обобщенная задача выбора модели глубокого обучения.

\vspace{0.5cm}
\textbf{Теоретическая значимость.} В целом, данная диссертационная работа носит теоретический характер. В работе предлагаются критерии субоптимальной и оптимальной сложности, основанные на принципе минимальной длины описания. Исследуется взаимосвязь критериев оптимальной и субоптимальной сложности. Предлагаются градиентные методы для получения оценок сложности модели. Доказывается теорема об оценке энтропии эмпирического распределения параметров модели, полученных под действием оператора оптимизации.
Доказывается теорема об обобщенной задаче выбора модели глубокого обучения.


\vspace{0.5cm}
\textbf{Практическая значимость.} Предложенные в работе методы предназначены для построения моделей глубокого обучения в прикладных задачах регрессии и классификации; оптимизации гиперпараметров полученной модели; выбора модели из конечного множества заданных моделей; получения оценок переобучения модели.


\vspace{0.5cm}
\textbf{Степень достоверности и апробация работы.} Достоверность результатов подтверждена математическими доказательствами, экспериментальной проверкой полученных методов на реальных задачах выбора моделей глубокого обучения; публикациями результатов исследования в рецензируемых научных изданиях, в том числе рекомендованных ВАК. Результаты работы докладывались и обсуждались на следующих научных конференциях.
\begin{enumerate}
\item ``Восстановление панельной матрицы и ранжирующей модели в разнородных шкалах'', Всероссийская конференция <<57-я научная конференция МФТИ>>, 2014.
\item ``A monolingual approach to detection of text reuse in Russian-English collection'', Международная конференция <<Artificial Intelligence and Natural Language Conference>>, 2015~\cite{monolingual}.
\item ``Выбор модели глубокого обучения субоптимальной сложности с использованием вариационной оценки правдоподобия'', Международная конференция <<Интеллектуализация обработки информации>>, 2016~\cite{ioi16}.
\item ``Machine-Translated Text Detection in a Collection of Russian
Scientific Papers'', Международная конференция по компьютерной лингвистике и интеллектуальным технологиям <<Диалог-21>>, 2017~\cite{dialog}.
\item ``Author Masking using Sequence-to-Sequence Models'', Международная конференция <<Conference and Labs of the Evaluation Forum>>, 2017~\cite{pan_s2s}.
\item ``Градиентные методы оптимизации гиперпараметров моделей глубокого обучения'', Всероссийская конференция <<Математические методы распознавания образов ММРО>>, 2017~\cite{mmro17_hyper}.
\item ``Детектирование переводных заимствований в текстах научных статей из журналов, входящих в РИНЦ'', Всероссийская конференция <<Математические методы распознавания образов ММРО>>, 2017~\cite{mmro17_plag}.
\item ``ParaPlagDet: The system of paraphrased plagiarism detection'', Международная конференция <<Big Scholar at conference on knowledge discovery and data mining>>, 2018.
\item ``Байесовский выбор наиболее правдоподобной структуры модели глубокого обучения'', Международная конференция <<Интеллектуализация обработки информации>>, 2018~\cite{ioi18}.
\item ``Variational learning across domains with triplet
information'', Международная конференция <<Visually Grounded Interaction and Language workshop, Conference on Neural Information Processing Systems>>, 2018.
\end{enumerate}

Работа поддержана грантами Российского фонда фундаментальных исследований.
\begin{enumerate}
\item 19-07-00875, Развитие методов автоматического построения и выбора вероятностных моделей субоптимальной сложности в задачах глубокого обучения.
\item 16-37-00488, Разработка алгоритмов построения сетей глубокого обучения как суперпозиций универсальных моделей.
\item 16-07-01158, Развитие теории построения суперпозиций универсальных моделей классификации сигналов.
\item 14-07-3104,  Построение и анализ моделей классификации для выборок малой мощности.
\end{enumerate}

\vspace{0.5cm}
\textbf{Публикации по теме диссертации.} Основные результаты по теме диссертации изложены в 11 печатных изданиях, 9 из которых изданы в журналах, рекомендованных ВАК.
\begin{enumerate}
\item О. Ю. Бахтеев, М. С. Попова, В. В. Стрижов, “Системы и средства глубокого обучения в задачах классификации”, Системы и средства информатики, 26:2 (2016), 4–22~\cite{popova2}.
\item Bakhteev, O., Kuznetsova, R., Romanov, A. and Khritankov, A., 2015, November. A monolingual approach to detection of text reuse in Russian-English collection. In 2015 Artificial Intelligence and Natural Language and Information Extraction, Social Media and Web Search FRUCT Conference (AINL-ISMW FRUCT) (pp. 3-10). IEEE~\cite{monolingual}.
\item Romanov, A., Kuznetsova, R., Bakhteev, O. and Khritankov, A., 2016. Machine-Translated Text Detection in a Collection of Russian Scientific Papers. Компьютерная лингвистика и интеллектуальные технолгии. 2016~\cite{dialog}. 
\item Bakhteev, O. and Khazov, A., 2017. Author Masking using Sequence-to-Sequence Models. In CLEF (Working Notes). 2017~\cite{pan_s2s}.
\item О. Ю. Бахтеев, В. В. Стрижов, “Выбор моделей глубокого обучения субоптимальной сложности”, Автоматика и телемеханика, 2018, № 8, 129–147; Automation Remote Control, 79:8 (2018), 1474–1488~\cite{var_ait}.
\item А. В. Огальцов, О. Ю. Бахтеев, “Автоматическое извлечение метаданных из научных PDF-документов”, Информатика и её применения, 12:2 (2018), 75–82~\cite{ogaltsov}.
\item А. Н. Смердов, О. Ю. Бахтеев, В. В. Стрижов, “Выбор оптимальной модели рекуррентной сети в задачах поиска парафраза”, Информатика и её применения, 12:4 (2018), 63–69~\cite{smerdov}.
\item Грабовой А.В., Бахтеев О.Ю., Стрижов В.В. “Определение релевантности параметров нейросети”, Информатика и её применения. 13:2 (2019), 62-71.
\item Bakhteev, O.Y. and Strijov, V.V., 2019. Comprehensive analysis of gradient-based hyperparameter optimization algorithms. Annals of Operations Research, pp.1-15~\cite{hyper_bakhteev}.


\item Бахтеев О.Ю. Восстановление панельной матрицы и ранжирующей модели по метризованной выборке в разнородных данных. // Машинное обучение и анализ данных. 2016. № 7. С. 72-77~\cite{panel}.
\item Бахтеев О.Ю. Восстановление пропущенных значений в разнородных шкалах с большим числом пропусков. // Машинное обучение и анализ данных. 2015. № 11. С. 1-11~\cite{knn}.

\end{enumerate}


\vspace{0.5cm}
\textbf{Личный вклад.} Все приведенные результаты, кроме отдельно оговоренных случаев, получены диссертантом лично при научном руководстве д.ф.-м.н. В. В. Стрижова.


\vspace{0.5cm}
\textbf{Структура и объем работы.} Диссертация состоит из оглавления, введения, четырех разделов, заключения, списка иллюстраций, списка таблиц, перечня основных обозначений и списка литературы из \total{citnum} наименований. Основной текст занимает \pageref{LastPage} страницы.

\vspace{0.5cm}
\textbf{Краткое содержание работы по главам.} В первой главе вводятся основные понятия и определения, формулируются задачи построения моделей глубокого обучения. Рассматриваются основные критерии выбора моделей. Рассматриваются существующие алгоритмы построения моделей глубокого обучения.

Во второй главе предлагается алгоритм построения субоптимальной модели глубокого обучения. Предлагаются методы оценки сложности модели.

В третьей главе исследуются методы оптимизации гиперпараметров модели.

В четвертой главе рассматривается задача выбора оптимальной и субоптимальной структуры модели глубокого обучения. Предлагается обобщающая задача выбора структуры модели глубокого обучения, исследуются ее асимптотические свойства. 

В пятой главе на базе предложенных методов описывается разработанный программный комплекс, позволяющий автоматически построить модель глубокого обучения субпотимальной сложности для заданной выборки для задачи классификации и регрессии. Работа данного комплекса анализируется на ряде выборок для задач классификации и регрессии. Результаты, полученные с помощью предложенных методов, сравниваются с результатами известных алгоритмов.

%\vspace{0.5cm}
%\textbf{Благодарности.}\\
