\newpage{}
\addcontentsline{toc}{section}{Введение}
\chapter*{Введение}


\textbf{Актуальность темы.} В работе рассматривается задача автоматического построения моделей глубокого обучения субоптимальной сложности. 

Под сложностью модели понимается \emph{минимальная длина описания}~\cite{mdl}, т.е. минимальное количество информации, которое требуется для передачи информации о модели и о выборке. Вычисление минимальной длины описания модели является вычислительно сложной процедурой. В работе предлагается получение ее приближенной оценки, основанной на связи минимальной длины описания и \emph{правдоподобия модели}~\cite{mdl}. Для получения оценки правдоподобия используются вариационные методы получения оценки правдоподобия~\cite{Bishop}, основанные на аппроксимации неизвестного другим заданным распределением. Под субоптимальной сложностью понимается вариационная оценка правдоподобия модели.

Одна из проблем построения моделей глубокого обучения --- большое количество параметров моделей~\cite{hinton_rbm, hinton_init}. Поэтому задача выбора моделей глубокого обучения включает в себя выбор стратегии построения модели, эффективной по вычислительным ресурсам. В работе~\cite{greed} приводятся теоретические оценки построения нейросетей с использованием ,  при которых построение модели производится итеративно последовательным увеличением числа нейронов в сети. В работе~\cite{greed_mlp} предлагается жадная стратегия выбора модели нейросети с использованием релевантных априорных распределений, т.е. параметрических распределений, оптимизация параметров которых позволяет удалить часть параметров из модели. Данный метод был к задаче построения модели метода релевантных векторов~\cite{rvm}. Альтернативой данным алгоритмам построения моделей являются методы, основанные на прореживании сетей глубокого обучения~\cite{obd, popova, nvidia_prune}, т.е. последовательного удаления параметров, не дающих существенного прироста качества модели. 
В работах~\cite{Bengio, hd} рассматривается послойное построение модели с отдельным критерием оптимизации для каждого слоя. В работах~\cite{Kingma, gendis_pictures, gendis_phd} предлагается декомпозиция модели на порождающую и разделяющую, оптимизируемых последовательно. В работе~\cite{adanet} предлагается метод автоматического построения сети, основанный на бустинге. В качестве оптимизируемого функционала предлагается линейная комбинация функции правдоподобия выборки и сложности модели по Радемахеру. 
В работах~\cite{reinf,reinf_predict,reinf_deep2net,reinf_transfer} предлагается метод автоматического построения сверточной сети с использованием обучения с подкреплением. В~\cite{darts} используется схожее представление сверточной сети, вместо обучения с подкреплением используется градиентная параметров, задающих структуру нейронной сети.

В качестве порождающих моделей в сетях глубокого обучения выступают ограниченные машины Больцмана~\cite{hinton_rbm} и автокодировщики~\cite{founds}. В работе~\cite{contractive} рассматриваются некоторые типы регуляризации автокодировщиков, позволяющие формально рассматривать данные модели как порождающие модели с использованием байесового вывода. В работе~\cite{score} также рассматриваются регуляризованные автокодировщики и свойства оценок их правдоподобия. В работе~\cite{vae} предлагается обобщение автокодировщика с использованием вариационного байесовского вывода~\cite{Bishop}. В работе~\cite{train_generative} рассматриваются модификации вариационного автокодировщика и ступенчатых сетей (англ. ladder network)~\cite{ladder} для случая построения многослойных порождающих моделей. 

В качестве критерия выбора модели в ряде работ~\cite{MacKay,Bishop,tokmakova,zaitsev,strijov_webber, strijov_dsc} выступает правдоподобие модели. В работах~\cite{tokmakova,zaitsev,strijov_webber, strijov_dsc} рассматривается проблема выбора модели и оценки гиперапараметров в задачах регрессии. Альтернативным критерием выбора модели является минимальная длина описания~\cite{mdl}, являющаяся показателем статистической сложности модели и заданной выборки. 
В работе~\cite{perekrestenko} рассматривается перечень критериев сложности моделей глубокого обучения и их взаимосвязь. В работе~\cite{vladis} в качестве критерия сложности модели выступает показатель нелинейности, характеризуемый степенью полинома Чебышева, аппроксимирующего функцию. В работе~\cite{need_prune} анализируется показатель избыточности параметров сети. Утверждается, что по небольшому набору параметров в глубокой сети с большим количеством избыточных параметров можно спрогнозировать значения остальных. В работе~\cite{rob} рассматривается показатель робастности моделей, а также его взаимосвязь с топологией выборки и классами функций, в частности рассматривается влияние функции ошибки и ее липшицевой константы на робастность моделей. Схожие идеи были рассмотрены в работе~\cite{intrig}, в которой исследуется устойчивость классификации модели под действием шума. 

Одним из методов получения приближенного значения интеграла правдоподобия является вариационный метод получения нижней оценки интеграла~\cite{Bishop}. В работе~\cite{hoffman} рассматривается стохастическая версия вариационного метода. В работе~\cite{nips} рассматривается алгоритм получения вариационной нижней оценки правдоподобия  для оптимизации гиперпараметров моделей глубокого обучения. В работе~\cite{varmc} рассматривается получение вариационной нижней оценки интеграла с использованием модификации методов Монте-Карло. В работе~\cite{early} рассматривается стохастический градиентный спуск в качестве оператора, порождающего распределение, аппроксимирующее апостериорное распределение параметров модели. Схожий подход рассматривается в работе~\cite{sgd_cont}, где также рассматривается стохастический градиентный спуск в качестве оператора, порождающего апостериорное распределение параметров. В работе~\cite{langevin} предлагается модификация стохастического градиентного спуска, аппроксимирующая апостериорное распределение. 

Альтернативным методом выбора модели является выбор модели на основе скользящего контроля~\cite{cv_ms, tokmakova}. Проблемой такого подхода является возможная высокая вычислительная сложность~\cite{expensive, expensive2}. В работах~\cite{bias,bias2} рассматривается проблема смещения оценок качества модели и гиперпараметров, получаемых при использовании $k$-fold метода скользящего контроля, при котором выборка делится на $k$-частей с обучением на $k-1$ части и валидацией результата на оставшейся части выборки. 

Задачей, связанной с проблемой выбора модели, является задача оптимизации гиперпараметров~\cite{MacKay,Bishop}. В работе~\cite{tokmakova} рассматривается оптимизация гиперпарамтров с использованием метода скользящего контроля и методов оптимизации интеграла правдодобия моделей, отмечается низкая скорость сходимости гиперпараметров при использовании метода скользящего контроля. В ряде работ~\cite{hyper, hyper2} рассматриваются градиентные методы оптимизации гиперпараметров, позволяющие оптимизировать большое количество гиперпараметров одновременно. В работе~\cite{hyper} предлагается метод оптимизации гиперпараметров с использованием градиентного спуска с моментом, в качестве оптимизируемого функционала рассматривается ошибка на валидационной части выборки. В работе~\cite{approx_hyper} предлагается метод аппроксимации градиента функции потерь по гиперпараметрам, позволяющий использовать градиентные методы в задаче оптимизации гиперпараметров на больших выборках. В работе~\cite{greed_hyper} предлагается упрощенный метод оптимизации гиперпараметров с градиентным спуском: вместо всей истории обновлений параметров для оптимизации используется только последнее обновление. В работе~\cite{sgd_cont} рассматривается задача оптимизации параметров градиентного спуска с использованием нижней вариационной оценки интеграла правдоподобия. 


\vspace{0.5cm}
\textbf{Цели работы.}
\vspace{0.2cm}
\begin{enumerate}
\item Исследовать методы построения моделей глубокого обучения оптимальной сложности.
\item Предложить критерии оптимальной и субоптимальной сложности модели глубокого обучения.
\item Предложить метод выбора структуры модели глубокого обучения.
\item Предложить алгоритм построения модели субоптимальной сложности и оптимизации параметров.
\item Разработать алгоритм построения модели и проанализировать различные подходы к решению задачи автоматического построения моделей глубокого обучения и оптимизации параметров модели.
\end{enumerate}


\vspace{0.5cm}
\textbf{Методы исследования.} Для достижения поставленных целей используются методы вариационного байесовского вывода~\cite{MacKay, Bishop, early}. Рассматриваются графовое представление нейронной сети~\cite{reinf,darts}. Для получения вариационных оценок правдоподобия модели используется метод, основанный на градиентном спуске~\cite{sgd_cont, early}. В качестве метода получения модели субоптимальной сложности используется метод автоматического определения релевантности параметров~\cite{MacKay, ard} с использоваением градиентных методов оптимизации гиперпараметров~\cite{hyper, hyper2, greed_hyper, approx_hyper}.

\vspace{0.5cm}
\textbf{Основные положения, выносимые на защиту.}
\vspace{0.3cm}
\begin{enumerate}
\item Предложен метод построения модели глубокого обучения субоптимальной сложности.
\item Предложен алгоритм оптимизации параметров, гиперпараметров и структурных параметров моделей глубокого обучения.
\item Проведено исследование свойства оптимизационных алгоритмов выбора модели.
\item Предложен метод выбора модели наиболее правдоподобной структуры, обобщающий различные алгоритмы оптимизации:
оптимизация правдоподобия,
последовательное увеличение сложности модели,
последовательное снижение сложности модели,
полный перебор вариантов структуры модели.
\item Предложены методы оптимизации параметров и гиперпараметров модели.
\item Предложен обобщенный метод выбора модели глубокого обучения.
\item Разработан программный комплекс для построения моделей глубокого обучения для задач классификации и регрессии.
\end{enumerate}


\vspace{0.5cm}
\textbf{Научная новизна.} Разработан новый подход к построению моделей глубого обучения. Предложены критерии субоптимльной и оптимальной сложности модели, а также исследована их связь. Предложен метод построения модели глубокого обучения субоптимальной сложности. Предложен метод оптимизации гиперпараметров модели, а также методов оптимизации модели.  Предложен обобщенный метод выбора модели глубокого обучения.

\vspace{0.5cm}
\textbf{Теоретическая значимость.} В целом, данная диссертационная работа носит теоретический характер. В работе предлагаются критерии субоптимальной и оптимальной сложности, основанные на принципе минимальной длины описания. Исследуется взаимосвязь критериев оптимальной и субоптимальной сложности. Предлагаются градиентные методы для получения оценок сложности модели. Доказывается теорема об оценке энтропии эмпирического распределения параметров модели, полученных под действием оператора оптимизации.
Доказывается теорема об обобщенном методе выбора модели глубокого обучения.


\vspace{0.5cm}
\textbf{Практическая значимость.} Предложенные в работе методы предназначены для построения моделей глубокого обучения в задачах регрессии и классификации; оптимизации гиперпараметров полученной модели; выборе модели из конечного множества заданных моделей; получения оценок переобучения модели.


\vspace{0.5cm}
\textbf{Степень достоверности и апробация работы.} Достоверность результатов подтверждена математическими доказательствами, экспериментальной проверкой полученных методов на реальных задачах выбора моделей глубокого обучения; публикациями результатов исследования в рецензируемых научных изданиях, в том числе рекомендованных ВАК. Результаты работы докладывались и обсуждались на следующих научных конференциях.
\begin{enumerate}
\item Всероссийская конеренция <<57я научная конеренция МФТИ>>, 2014.
\item Международная конференция <<Artificial Intelligence and Natural Language Conference>>, 2015~\cite{monolingual}.
\item Международная конференция <<Интеллектуализация обработки инормации>>, 2016~\cite{ioi16}.
\item Международная конференция по компьютерной лингвистике и интеллектуальным технологиям <<Диалог-21>>, 2017~\cite{dialog}.
\item Международная конференция <<Conference and Labs of the Evaluation Forum>>, 2017.
\item Всероссийская конференция <<Математические методы распознавания образов ММРО>>, 2017~\cite{mmro17_hyper,mmro17_plag}.
\item Международная конференция <<Big Scholar at conference on knowledge discovery and data mining>>, 2018.
\item Международная конференция <<Интеллектуализация обработки инормации>>, 2018~\cite{ioi18}.
\item Международная конференция <<Conference on Neural Information Processing Systems>>, 2018.
\end{enumerate}

Работа поддержана грантами Российского фонда фундаментальных исследований и Министерства образования и науки РФ.
\begin{enumerate}
\item 16-37-00488, Российский фонд фундаментальных исследований в рамках гранта ``Разработка алгоритмов построения сетей глубокого обучения как суперпозиций универсальных моделей''.
\item ???
\end{enumerate}

\vspace{0.5cm}
\textbf{Публикации по теме диссертации.} Основные результаты по теме диссертации изложены в ? печатных изданиях, ? из которых изданы в журналах, рекомендованных ВАК.
\begin{enumerate}
\item Бахтеев О.Ю. Восстановление панельной матрицы и ранжирующей модели по метризованной выборке в разнородных данных. // Машинное обучение и анализ данных. 2016. № 7. С. 72-77~\cite{panel}.
\item Бахтеев О.Ю. Восстановление пропущенных значений в разнородных шкалах с большим числом пропусков. // Машинное обучение и анализ данных. 2015. № 11. С. 1-11~\cite{knn}.
\item Бахтеев О.Ю., Попова М.С., Стрижов В.В. Системы и средства глубокого обучения в задачах классификации. // Системы и средства информатики. 2016. № 26.2. С. 4-22~\cite{popova}.
\item Бахтеев О.Ю., Стрижов В.В. Выбор моделей глубокого обучения субоптимальной сложности. // Автоматика и телемеханика. 2018. №8. С. 129-147~\cite{var_ait}.
\item Огальцов А.В., Бахтеев О.Ю. Автоматическое извлечение метаданных из научных PDF-документов. // Информатика и её применения. 2018.
\item Смердов А.Н., Бахтеев О.Ю., Стрижов В.В. Выбор оптимальной модели рекуррентной сети в задачах поиска парафраза. // Информатика и ее применения. 2019.
\item Грабовой А.В., Бахтеев О.Ю., Стрижов В.В. Определение релевантности параметров нейросети. // Информатика и её применения. 2019.
\end{enumerate}


\vspace{0.5cm}
\textbf{Личный вклад.} Все приведенные результаты, кроме отдельно оговоренных случаев, получены диссертантом лично при научном руководстве д.ф.-м.н. В. В. Стрижова.


\vspace{0.5cm}
\textbf{Структура и объем работы.} Диссертация состоит из оглавления, введения, четырех разделов, заключения, списка иллюстраций, списка таблиц, перечня основных обозначений и списка литературы из ? наименований. Основной текст занимает ? страниц.

\vspace{0.5cm}
\textbf{Краткое содержание работы по главам.} В первой главе вводятся основные понятия и определения, формулируются задачи построения моделей глубокого обучения. Рассматриваются основные критерии выбора моделей. Рассматриваются существующие алгоритмы построения моделей глубокого обучения.

Во второй главе предлагается алгоритм построения суботпимальной модели глубокого обучения. Предлагаются методы оценки сложности модели.

В третьей главе рассматриваются методы оптимизации гиперпараметров модели.

В четвертой главе рассматривается обобщенный метод выбора модели глубокого обучения. 

В пятой главе на базе предложенных методов описывается разработанный программный комплекс, позволяющий автоматически построить модель глубокого обучения субпотимальной сложности для заданной выборки для задачи классификации и регрессии. Работа данного комплекса анализируется на ? выборках. Результаты, полученные с помощью предложенных методов, сравниваются с результатами известных алгоритмов.

%\vspace{0.5cm}
%\textbf{Благодарности.}\\
