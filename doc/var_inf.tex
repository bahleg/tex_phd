\newpage{}
\addcontentsline{toc}{section}{Выбор модели с использованием вариационного вывода}
\chapter*{Выбор модели с использованием вариационного вывода}
\begin{defin}
Сложностью модели $\mathbf{f}$ назовем правдоподобие модели: 
\begin{equation}
\label{eq:evidence}
	Q = p(\mathbf{y}|\mathbf{X},\mathbf{h}) = \int_{\mathbf{W} \in \mathbb{R}^|\mathbf{W}|, \boldsymbol{\Gamma} \in \mathbb{R}^|\boldsymbol{\Gamma}|} p(\mathbf{y}|\mathbf{X},\mathbf{W},\boldsymbol{\Gamma}, \mathbf{h})p(\mathbf{W, \boldsymbol{\Gamma}}|\mathbf{h})d\mathbf{W}.
\end{equation}
\end{defin}

В данной главе будем полагать, что для каждой модели $\mathbf{f} \in \mathfrak{F}$ структура $\boldsymbol{\Gamma}$ фиксированна и определена однозначно.
Модели $\mathbf{f} \in \mathfrak{F}$ имеют различные размерности $d$ соответствующих векторов параметров $\mathbf{W}$.
%Заметим, что в общем случае оценки правдоподобия модели и сложности модели могут различаться~\cite{grun}.

Модель классификации $\mathbf{f}$ назовем оптимальной среди моделей $\mathfrak{F}$, если достигается максимум интеграла~\eqref{eq:evidence}.

Требуется найти оптимальную модель $\mathbf{f}$ среди заданного множества моделей $\mathfrak{F}$, а также значения ее параметров $\mathbf{W}$, доставляющие максимум апостериорной вероятности
\begin{equation}
\label{eq:posterior}
	p(\mathbf{W}|\mathbf{y},\mathbf{X},\mathbf{h}) = \frac{p(\mathbf{y}|\mathbf{X}, \mathbf{W}, \mathbf{h})p(\mathbf{W}|\mathbf{h})}{p(\mathbf{y}|\mathbf{X}, \mathbf{h})}.
\end{equation}


%\begin{example_empty}
\textsl{П\,р\,и\,м\,е\,р~\,1.} Рассмотрим задачу линейной регрессии:
\[
	\mathbf{y} =\mathbf{X} \mathbf{W} + \boldsymbol{\varepsilon},\quad \boldsymbol{\varepsilon}  \sim \mathcal{N}(\mathbf{0},\mathbf{1}),\quad \mathbf{W} \sim  \mathcal{N}(\mathbf{0},\mathbf{A}^{-1}),
\]
где $\mathbf{A}$ --- диагональная матрица. 
Правдоподобие зависимой переменной имеет вид
\begin{equation}
\label{eq:example1}
	p(\mathbf{y}|  \mathbf{X}, \mathbf{W},\mathbf{h}) = (2\pi) ^{-\frac{m}{2}} \textnormal{exp} \bigl(-\frac{1}{2}(\mathbf{y} -\mathbf{X} \mathbf{W})^\mathsf{T}(\mathbf{y} - \mathbf{X}\mathbf{W})\bigr),
\end{equation}
априорное распределение параметров модели имеет вид
\begin{equation}
\label{eq:prior}	
p(\mathbf{W}|\mathbf{A}) =  (2\pi) ^{-\frac{n}{2}} |\mathbf{A}|^{\frac{1}{2}} \textnormal{exp} (-\frac{1}{2}\mathbf{W}^\mathsf{T}\mathbf{A}\mathbf{W}), \quad, \mathbf{h} = \text{diag}(\mathbf{A}).
\end{equation}

Правдоподобие модели~\eqref{eq:evidence} в этом примере вычисляется аналитически~\cite{hyperopt}:
\begin{equation}
\label{eq:ground}
	p(\mathbf{y}|\mathbf{X},\mathbf{h})  =  (2\pi) ^{-\frac{m}{2}} |\mathbf{A}|^{\frac{1}{2}} |\mathbf{H}|^{-\frac{1}{2}}  \textnormal{exp}\bigl(-\frac{1}{2}(\mathbf{y} -\mathbf{X} \hat{\mathbf{W}})^\mathsf{T}(\mathbf{y} - \mathbf{X}\hat{\mathbf{W}})\bigr)\textnormal{exp} \bigl(-\frac{1}{2}\hat{\mathbf{W}}^\mathsf{T}\mathbf{A}\hat{\mathbf{W}}\bigr),
\end{equation}
где $\hat{\mathbf{W}}$ --- значение наиболее вероятных~\eqref{eq:posterior} параметров модели:
\[
	\hat{\mathbf{W}} = \argmax p(\mathbf{W}|\mathbf{y}, \mathbf{X}, \mathbf{h}) = (\mathbf{A} + \mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}\mathbf{y},
\]
$\mathbf{H}$ --- гессиан функции потерь $L$ модели:
\[
	\mathbf{H}	= \nabla \nabla_\mathbf{W} \left(\frac{1}{2} (\mathbf{y} -\mathbf{X} {\mathbf{W}})^\mathsf{T}(\mathbf{y} - \mathbf{X}{\mathbf{W}}) + \frac{1}{2}\mathbf{W}^\mathsf{T}\mathbf{A}\mathbf{W} \right) = \mathbf{A} + \mathbf{X}^\mathsf{T}\mathbf{X},
\]

\[ 
	L = - \textnormal{log} p(\mathbf{y}|  \mathbf{X}, \mathbf{W},\mathbf{h}). 
\]
%\end{example_empty}

\textsl{П\,р\,и\,м\,е\,р~\,2.} Рассмотрим задачу классификации, в которой модель --- нейросеть с softmax-слоем на выходе:
\[
\mathbf{f} = \mathbf{f}_\textnormal{SM}(\mathbf{f}_1(\dots \mathbf{f}_K(\mathbf{x}))),
\]
$\mathbf{f}_1, \dots,\mathbf{h}_K$ --- дифференцируемые функции, $\mathbf{f}_\textnormal{SM}$ --- многомерная логистическая функция:
\[
	\mathbf{f}_\textnormal{SM} = \frac{\mathbf{f}_1(\dots \mathbf{f}_K(\mathbf{x}))}{\sum_{r=1}^Z \textnormal{exp}\bigl( {f}_{r,1}(\dots \mathbf{f}_K(\mathbf{x})) \bigr)},
\]
где ${f}_{r,1}$ --- $r$-я компонента функции $\mathbf{f}_1$. Компонента $r$ вектора $\mathbf{f}_\textnormal{SM}$ определяет вероятность принадлежности объекта $\mathbf{x}$ к классу $r$. Логарифм правдоподобия зависимой переменной аналогично~\eqref{eq:example1} имеет вид
\[
	\textnormal{log} p({y}|\mathbf{x}, \mathbf{W},\mathbf{h}) =  \textnormal{log}~\hat{f}_{\hat{r},\textnormal{SM}} (\mathbf{f}_1(\dots \mathbf{f}_K(\mathbf{x}))),
\]
где $\hat{f}_{\hat{r},\textnormal{SM}}$ соответствует ненулевой компоненте вектора ${y}$:
$$\hat{r} \in \{1,\dots,Z\}: y_{r} >0,$$
$y_{r}$ --- компонента вектора ${y}$. 

Интеграл правдоподобия~\eqref{eq:evidence} модели является трудновычислимым для данного семейства моделей. Одним из методов вычисления приближенного значения правдоподобия является получение вариационной оценки правдоподобия.  



{В качестве функции, приближающей логарифм интеграла~\eqref{eq:evidence}, будем рассматривать его нижнюю оценку, полученную при помощи неравенства Йенсена~\cite{Bishop}. Получим нижнюю оценку логарифма правдоподобия модели, используя неравенство}
\begin{equation} 
\label{eq:elbo}
\textnormal{log}~p(\mathbf{y}|\mathbf{X},\mathbf{h})  = \int_{\mathbf{W}} q(\mathbf{W})\textnormal{log}~\frac{p(\mathbf{y},\mathbf{W}|\mathbf{X},\mathbf{h})}{q(\mathbf{W})}d\mathbf{W} + \textnormal{D}_\textnormal{KL}  \bigl(q(\mathbf{W})||p(\mathbf{W}|\mathbf{y}, \mathbf{X},\mathbf{h})\bigr) \geq	
\end{equation} 
$$
\geq \int_{\mathbf{W}} q(\mathbf{W})\textnormal{log}~\frac{p(\mathbf{y},\mathbf{W}|\mathbf{X},\mathbf{h})}{q(\mathbf{W})}d\mathbf{W} =
$$

$$
= -\textnormal{D}_\textnormal{KL} \bigl(q(\mathbf{W})||p(\mathbf{W}|\mathbf{h})\bigr) + \int_{\mathbf{W}} q(\mathbf{W})\textnormal{log}~{p(\mathbf{y}|\mathbf{X},\mathbf{W},\mathbf{h})} d \mathbf{W},
$$
где $\textnormal{D}_\textnormal{KL}\bigl(q(\mathbf{W})||p(\mathbf{W} |\mathbf{h})\bigr)$ --- расстояние Кульбака--Лейблера между двумя распределениями: $$\textnormal{D}_\textnormal{KL}\bigl(q(\mathbf{W})||p(\mathbf{W} |\mathbf{h})\bigr) = -\int_{\mathbf{W}} q(\mathbf{W})\textnormal{log}~\frac{p(\mathbf{W} | \mathbf{h})}{q(\mathbf{W})}d\mathbf{W},$$
$$
p(\mathbf{y},\mathbf{W}|\mathbf{X},\mathbf{h}) = p(\mathbf{y}|\mathbf{X},\mathbf{h})p(\mathbf{W}|\mathbf{h}).
$$

\begin{defin}
Пусть задано аппроксимирующее распределение $q$.
Вариационной оценкой логарифма правдоподобия модели~\eqref{eq:evidence} $\textnormal{log}~p(\mathbf{y}|\mathbf{X},\mathbf{h})$ называется оценка $\textnormal{log}~\hat{p}(\mathbf{y}|\mathbf{X},\mathbf{h})$, полученная аппроксимацией неизвестного апостериорного распределения $p(\mathbf{W}| \mathbf{y}, \mathbf{X},\mathbf{h})$ заданным распределением $q(\mathbf{W})$.
\end{defin}

Будем рассматривать задачу нахождения вариационной оценки как задачу оптимизации. Пусть задано множество распределений $\mathfrak{Q} =\{q(\mathbf{W})\}$. Сведем задачу нахождения наиболее близкой вариационной нижней оценки интеграла~\eqref{eq:evidence} к оптимизации вида
\[
     \hat{q}(\mathbf{W}) = \argmax_{q \in \mathfrak{Q}}  \int_{\mathbf{W}} q(\mathbf{W})\textnormal{log}~\frac{p(\mathbf{y},\mathbf{W}|\mathbf{X},\mathbf{h})}{q(\mathbf{W})}d\mathbf{W}.
\]  
В данной работе в качестве множества $\mathfrak{Q}$ рассматривается нормальное распределение и распределение параметров, неявно получаемое оптимизацией градиентными методами. 

Оценка~\eqref{eq:elbo} является нижней, поэтому может давать некорректные оценки для правдоподобия~\eqref{eq:evidence}. Для того, чтобы оценить величину этой ошибки, докажем следующее утверждение.

\begin{utv}
\label{st:st1} Пусть задано множество $\mathfrak{Q} = \{q(\mathbf{W})\}$ непрерывных распределений. Максимизация вариационной нижней оценки $$\int_{\mathbf{W}} q(\mathbf{W})\textnormal{log}~\frac{p(\mathbf{y},\mathbf{W}|\mathbf{X},\mathbf{h})}{q(\mathbf{W})}d\mathbf{W}$$  логарифма интеграла~\eqref{eq:evidence}  эквивалентна минимизации расстояния Кульбака--Лейблера между распределением $q(\mathbf{W}) \in \mathfrak{Q}$ и апостериорным распределением параметров $p(\mathbf{W}|\mathbf{y}, \mathbf{X}, \mathbf{h})$:
\begin{equation}
\label{eq:optim}
    \hat{q} = \argmax_{q \in \mathfrak{Q}} \int_{\mathbf{W}} q(\mathbf{W})\textnormal{log}~\frac{p(\mathbf{y},\mathbf{W}|\mathbf{X},\mathbf{h})}{q(\mathbf{W})}d\mathbf{W} \Leftrightarrow 	
    \hat{q} = \argmin_{q \in \mathfrak{Q}} \textnormal{D}_\textnormal{KL}  \bigl(q(\mathbf{W})||p(\mathbf{W}|\mathbf{y}, \mathbf{X}, \mathbf{h})\bigr),
\end{equation}

\[
	\textnormal{D}_\textnormal{KL}  \bigl(q(\mathbf{W})||p(\mathbf{W}|\mathbf{y}, \mathbf{X}, \mathbf{h})\bigr) =  \int_\mathbf{W} q(\mathbf{W}) \frac{q(\mathbf{W})}{p(\mathbf{W}|\mathbf{y}, \mathbf{X}, \mathbf{h})} d\mathbf{W}.
\]
\end{utv}


\begin{proof}
Доказательство непосредственно следует из~\eqref{eq:elbo}. Вычитая из обеих частей равенства $\textnormal{D}_\textnormal{KL}  (q(\mathbf{w})||p(\mathbf{w}|\mathbf{y}, \mathbf{X},\mathbf{h}))$, получим
\[
\textnormal{log}~p(\mathbf{y}|\mathbf{X},\mathbf{h}) - \textnormal{D}_\textnormal{KL}  (q(\mathbf{w})||p(\mathbf{w}|\mathbf{y}, \mathbf{X},\mathbf{h}))  = \int_{\mathbf{w}} q(\mathbf{w})\textnormal{log}~\frac{p(\mathbf{y},\mathbf{w}|\mathbf{X},\mathbf{h})}{q(\mathbf{w})}d\mathbf{w},
\]
где $\textnormal{log}~p(\mathbf{y}|\mathbf{X},\mathbf{h})$ --- выражение, не зависящее от $q(\mathbf{w})$.
\end{proof}



Таким образом, задача нахождения вариационной оценки, близкой к значению интеграла~\eqref{eq:evidence} сводится к поиску распределения $\hat{q}$, аппроксимирующего распределение $p(\mathbf{W}|\mathbf{y}, \mathbf{X}, \mathbf{h})$ наилучшим образом.

Модель $\mathbf{f}$ назовем субоптимальной на множестве моделей $\mathfrak{F}$  по множеству распределений $\mathfrak{Q}$, если модель доставляет максимум нижней вариационной оценке интеграла~\eqref{eq:optim}
\begin{equation}
\label{eq:elbo2}
	\max_{q \in \mathfrak{Q}}\int_{\mathbf{W}} q(\mathbf{W})\textnormal{log}~\frac{p(\mathbf{y},\mathbf{W}|\mathbf{X},\mathbf{h})}{q(\mathbf{W})}d\mathbf{W}.
\end{equation}


\begin{defin}
Субоптимальность модели может быть также названа вариационной оптимальностью модели или LB-оптимальностью (\textit{Lower Bound --- нижняя граница}) модели.
\end{defin}

Вариационная оценка~\eqref{eq:elbo} интерпретируется как оценка сложности модели по принципу минимальной длины описания~\cite{mdl}, где первое слагаемое определяет количество информации для описания выборки, а второе слагаемое --- длину описания самой модели~\cite{nips}.
\[
	\textnormal{MDL}(\mathbf{y},\mathbf{h}) = \textnormal{Len}(\mathbf{y}|\hat{\mathbf{W}},\mathbf{h}) + \textnormal{COMP}(\mathbf{f}),
\]
где  $\textnormal{Len}(\mathbf{y}|\hat{\mathbf{W}},\mathbf{h})$ --- \emph{длина описания} матрицы $\mathbf{y}$ с использованием модели $\mathbf{f}$ и оценки вектора параметров $\hat{\mathbf{W}}$, полученных методом наибольшего правдоподобия, а $\textnormal{COMP}(\mathbf{f})$ --- величина, характеризующая \emph{параметрическую сложность} модели, т.е. способность модели описать произвольную выборки из $\mathbb{R}^n$~\cite{grun}.


В данной работе решается задача выбора субоптимальной модели при различных заданных множествах $\mathfrak{Q}$.

\section{Методы получения вариационной оценки правдоподобия}
Ниже представлены методы получения вариационных нижних оценок~\eqref{eq:elbo2} правдоподобия~\eqref{eq:evidence}. В первом подразделе рассматривается метод, основанный на аппроксимации апостериорного распределения $p( \mathbf{W}|\mathbf{y}, \mathbf{X}, \mathbf{h})$~\eqref{eq:posterior} многомерным гауссовым распределением с диагональной матрицей ковариаций. В последующих разделах рассматриваются методы, основанные на различных модификациях стохастического градиентного спуска. 

\subsection{Аппроксимация нормальным распределением}
В качестве множества $\mathfrak{Q} = \{q(\mathbf{W})\}$ задано параметрическое семейство нормальных распределений с диагональными матрицами ковариаций:
\begin{equation}
\label{eq:diag}
	q \sim \mathcal{N}(\boldsymbol{\mu}_q, \mathbf{A}^{-1}_q),
\end{equation}
где $\mathbf{A}_q$ --- диагональная матрица ковариаций, $\boldsymbol{\mu}_q$ --- вектор средних компонент.

Тогда оптимизация~\eqref{eq:optim} имеет вид
\begin{equation}
\label{eq:norm_max}
 \int_{\mathbf{W}} q(\mathbf{W})\textnormal{log}~{p(\mathbf{y}|\mathbf{X},\mathbf{W},\mathbf{h})} d \mathbf{W} - D_\textnormal{KL}\bigl(q (\mathbf{W} )|| p (\mathbf{W}|\mathbf{f})\bigr) \to \max_{\mathbf{A}_q, \boldsymbol{\mu}_q},
\end{equation}
где расстояние $D_\textnormal{KL}$ между двумя гауссовыми величинами рассчитывается как 
\[
	D_\textnormal{KL}\bigl(q (\mathbf{W}) || p (\mathbf{W}|\mathbf{f})\bigr) = \frac{1}{2} \bigl( \textnormal{Tr} [\mathbf{A}\mathbf{A}^{-1}_q] + (\boldsymbol{\mu}_q)^\mathsf{T}\mathbf{A}(\boldsymbol{\mu}_q) - u +\textnormal{ln}~|\mathbf{A}^{-1}| - \textnormal{ln}~|\mathbf{A}_q^{-1}| \bigr).
\]
В качестве приближенного значения интеграла $$\int_{\mathbf{W}} q(\mathbf{W})\textnormal{log}~{p(\mathbf{y}|\mathbf{X},\mathbf{W},\mathbf{h})} d \mathbf{W}$$ предлагается использовать формулу
\[
\int_{\mathbf{W}} q(\mathbf{W})\textnormal{log}~{p(\mathbf{y}|\mathbf{X},\mathbf{W},\mathbf{h})} d \mathbf{W} \approx \sum_{i=1}^m \textnormal{log}~p({y}_i|\mathbf{x}_i, \mathbf{W}_i),
\]
где $\mathbf{W}_i$  --- реализация случайной величины из распределения $q(\mathbf{W})$.

Итоговая функция оптимизации~\eqref{eq:norm_max} имеет вид
\begin{equation}
\label{eq:gaus}
	\mathbf{f} = \argmax_{\mathbf{A}_q, \boldsymbol{\mu}_q} \sum_{i=1}^m \textnormal{log}~p({y}_i|\mathbf{x}_i, \mathbf{W}_i) - D_\textnormal{KL}\bigl(q (\mathbf{W} )|| p (\mathbf{W}|\mathbf{f})\bigr).
\end{equation}

%\begin{example_empty} 
%\hspace{\parindent}
\textsl{П\,р\,и\,м\,е\,р~\,3.}
Пусть  задана выборка $\mathfrak{D}$, в которой переменная ${y}$ не зависит от $\mathbf{x}$:
\begin{equation}
\label{eq:example_post}
	{y} \sim \mathcal{N}(\mathbf{W}, \mathbf{B}^{-1}),
\end{equation}

\[
	\mathbf{B}^{-1} = \left( \begin{array}{cc}
	2 & 1,8 \\
	1,8 & 2\\
	\end{array}  \right),
\]
\[
	p(\mathbf{W}|\mathbf{f}) = \mathcal{N}(\mathbf{0}, \mathbf{I}).
\]

График аппроксимации распределения параметров представлен на рис.~\ref{fig:var},\textit{а}. Как видно из графика, с использованием метода~\eqref{eq:gaus} получено грубое приближение апостериорного распределения $p(\mathbf{W}|\mathbf{y}, \mathbf{X}, \mathbf{h})$, что может существенно занизить оценку правдоподобия модели.



{Данный пример показывает, что качество итоговой аппроксимации распределения $p(\mathbf{W}|\mathbf{y}, \mathbf{X}, \mathbf{h})$ значительно зависит от схожести распределений $\hat{q}$ и $p(\mathbf{W}|\mathbf{y}, \mathbf{X}, \mathbf{h})$. В силу диагональности матрицы $\mathbf{A}_q$ и полного ранга матрицы $\mathbf{B}$  итоговое распределение $\hat{q}$ не может адекватно приблизить данное распределение  $p(\mathbf{W}|\mathbf{y}, \mathbf{X}, \mathbf{h})$.}



%\end{example_empty}
\subsection{Аппроксимация с использованием градиентного метода}
В качестве множества распределений $\mathfrak{Q} = \{q(\mathbf{W})\}$, аппроксимирующих неизвестное распределение $\textnormal{log}~p(\mathbf{y}|\mathbf{X},\mathbf{h})$, используются распределения параметров, полученные в ходе их оптимизации. 

Представим неравенство~\eqref{eq:elbo}
\begin{equation}
\label{eq:elbo_entropy}
 \textnormal{log}~p(\mathbf{y}|\mathbf{X},\mathbf{h}) \geq \int_\mathbf{W} q(\mathbf{W})\textnormal{log}~\frac{p(\mathbf{y},\mathbf{W}|\mathbf{X}, \mathbf{h})}{q(\mathbf{W})}d\mathbf{W} =  \mathsf{E}_{q(\mathbf{w)}}\bigl(\textnormal{log~}p (\mathbf{y}, \mathbf{W}|\mathbf{X},\mathbf{h})\bigr) - \mathsf{S}\bigl({q(\mathbf{w)}}\bigr),
\end{equation}
где $\mathsf{S}$ --- энтропия распределения:
\[
\mathsf{S}\bigl({q(\mathbf{w)}}\bigr) = - \int_{\mathbf{W}} q(\mathbf{W})\textnormal{log}~q(\mathbf{W})d\mathbf{W},
\]
$$p (\mathbf{y}, \mathbf{W}|\mathbf{X},\mathbf{h}) = p (\mathbf{W}| \mathbf{f}) p (\mathbf{y}|\mathbf{X}, \mathbf{W},\mathbf{h}),$$
$\mathsf{E}_{q(\mathbf{w)}}\bigl(\textnormal{log~}p (\mathbf{y}, \mathbf{W}|\mathbf{X},\mathbf{h})\bigr)$ --- матожидание логарифма вероятности $\textnormal{log~}p (\mathbf{y}, \mathbf{W}|\mathbf{X},\mathbf{h})$:
\[
	\mathsf{E}_{q(\mathbf{w)}}\bigl(\textnormal{log~}p (\mathbf{y}, \mathbf{W}|\mathbf{X},\mathbf{h})\bigr) = \int_\mathbf{W} \textnormal{log~}p (\mathbf{y}, \mathbf{W}|\mathbf{X},\mathbf{h}) q(\mathbf{W}) d\mathbf{W}.
\]

Оценка распределений производится при оптимизации параметров. Оптимизация выполняется в режиме мультистарта~\cite{multi}, т.е. при запуске оптимизации параметров модели из нескольких разных начальных приближений. Основная проблема такого подхода~---~вычисление энтропии $\mathsf{S}$ распределений $q(\mathbf{W}) \in \mathfrak{Q}$. Ниже представлен метод получения оценок энтропии~\eqref{eq:entropy} ~$\mathsf{S}$ и оценок правдоподобия~\eqref{eq:elbo_entropy}.

Запустим $r$ процедур оптимизаций модели $\mathbf{f}$ из разных начальных приближений:
\[
	L(\mathbf{W}^1, \mathbf{y}, \mathbf{X}), \dots, L(\mathbf{W}^r, \mathbf{y}, \mathbf{X}) \to \min,
\] 
где $r$ --- число оптимизаций, $L$ --- оптимизируемая функция потерь
\begin{equation}
\label{eq:loss_func}
L = -\sum_{i=1}^m \textnormal{log}p({y}_i, \mathbf{W} |\mathbf{x}_i,\mathbf{h}) = -\textnormal{log}~p(\mathbf{W}|\mathbf{f}) - \sum_{i=1}^m \textnormal{log}p({y}_i |\mathbf{x}_i, \mathbf{W},\mathbf{h}).
\end{equation}

Пусть начальные приближения параметров $\mathbf{W}^1, \dots, \mathbf{W}^r$ порождены из некоторого начального распределения $q^0(\mathbf{W})$:
\[
	\mathbf{W}^1, \dots, \mathbf{W}^r \sim q^0(\mathbf{W}). 
\]

%Обозначим за  $\mathbf{W}^g, g \in \{1,\dots,r\}$ значения параметров $\mathbf{W}^1, \dots, \mathbf{W}^r$ на  текущем шаге оптимизации. 

Для описания произвольного градиентного метода оптимизации параметров модели введем понятие оператора оптимизации. Оно используется для вычисления оценки энтропии распределения, полученного под действием этой оптимизации.

\begin{defin}
Назовем оператором оптимизации алгоритм $T$ выбора вектора параметров $\mathbf{W}'$  по параметрам предыдущего шага $\mathbf{W}$:
\[
	\mathbf{W}' = T(\mathbf{W}).
\]
\end{defin}


Рассмотрим оператор градиентного спуска:
\begin{equation}
\label{eq:sgd}
	T(\mathbf{W}) = \mathbf{W} - \gamma \nabla L(\mathbf{W}, \mathbf{y}, \mathbf{X}), 
\end{equation}
где  $\gamma$ --- длина шага градиентного спуска.

Пусть значения $\mathbf{W}^1, \dots, \mathbf{W}^r$  --- реализации случайной величины из некоторого распределения $q(\mathbf{W})$. Начальная энтропия распределения $q(\mathbf{W})$ соответствует энтропии распределения $q^0(\mathbf{W})$, из которого были порождены начальные приближения оптимизации параметров $\mathbf{W}^1, \dots, \mathbf{W}^r$. Под действием оператора $T$ распределение параметров $\mathbf{W}_1, \dots, \mathbf{W}_r$ изменяется. Для учета энтропии распределений, полученных в ходе оптимизации,
{ формализуем метод,  представленный в~\cite{early}. }

\begin{theorem}
Пусть $T$ --- оператор градиентного спуска,
 $L$ --- функция потерь, градиент $\nabla L$ которой имеет константу Липшица $C_L$.  Пусть $\mathbf{W}^1,\dots,\mathbf{W}^r$ ---  начальные приближения оптимизации модели, где $r$ --- число начальных приближений. Пусть $\gamma$ --- длина шага градиентного спуска, такая что
\begin{equation}
\label{eq:ineq}
\gamma<\frac{1}{C_L}, \quad \gamma < \bigl(\max_{g \in \{1,\dots,r\}}\lambda_\textnormal{max} (\mathbf{H}(\mathbf{W}^g))\bigr)^{-1}, 
\end{equation}
где $\lambda_\textnormal{max}$ --- наибольшее по модулю собственное значение гессиана  $\mathbf{H}$ функции потерь $L$.

При выполнении неравенств~\eqref{eq:ineq} разность энтропий распределений $q'(\mathbf{W}), q(\mathbf{W})$ на смежных шагах почти наверное сходится к следующему выражению: 
\begin{equation}
\label{eq:entropy}
	\mathsf{S}\bigl(q'(\mathbf{W})) -  \mathsf{S}\bigl(q(\mathbf{W}))  \approx  \frac{1}{r}\sum_{g=1}^r \bigl(-\gamma \textnormal{Tr}[\mathbf{H}(\mathbf{W}'^g)] - \gamma \textnormal{Tr}[\mathbf{H}(\mathbf{W}'^g)\mathbf{H}(\mathbf{W}'^g)]  \bigr) + o_{\gamma^2 \to 0}(1),
\end{equation}
где $\mathbf{H}$ --- гессиан функции потерь $L$.
\end{theorem}

Предварительно приведем две леммы~\cite{sgd_conv,entropy}, требуемые для доказательства теоремы.
\begin{lemma} Пусть $T$ --- оператор градиентного спуска, $L$ --- дважды дифференцируемая функция потерь, градиент $\nabla L$ которой имеет константу Липшица $C_L$.  Пусть для длины шага $\gamma$ выполнено неравенство 
$
	\gamma<\frac{1}{C_L}.
$
Тогда $T$ является диффеоморфизмом.
\end{lemma}

\begin{lemma} Пусть $\mathbf{w}$ --- случайный вектор с непрерывным распределением $q(\mathbf{w})$. Пусть $T$ --- биективное отображение вектора $\mathbf{w}$ в пространство той же размерности. Пусть $q'(\mathbf{w})$ --- распределение вектора $T(\mathbf{w})$. Тогда справедливо утверждение
\begin{equation}
\tag{П.1}
\label{eq:entropy_biject}
	\mathsf{S}\bigl(q'(\mathbf{w})\bigr) -  \mathsf{S}\bigl(q(\mathbf{w})\bigr)  = \int_\mathbf{w}  q'(\mathbf{w}) \textnormal{log}~\left|\frac{\partial{T(\mathbf{w})}}{\partial{\mathbf{w}}}\right| d\mathbf{w}.
\end{equation}
\end{lemma}


\begin{proof}
Рассмотрим очередной шаг оптимизации. При $\gamma<\frac{1}{C}$ оператор градиентного спуска $T$ является диффеоморфизмом, а значит, и биекцией, справедлива формула~\eqref{eq:entropy_biject}.
По усиленному закону больших чисел 
\[
	\mathsf{S}\bigl(q'(\mathbf{w})\bigr) -  \mathsf{S}\bigl(q(\mathbf{w})\bigr)  \approx  \frac{1}{r}\sum_{g=1}^r \textnormal{log}~\left|\frac{\partial{T(\mathbf{w}'^g)}}{\partial{\mathbf{w}}}\right|.
\]
Логарифм якобиана  $\textnormal{log}~\left|\frac{\partial{T(\mathbf{w}'^g)}}{\partial{\mathbf{w}}}\right|$ оператора $T$ запишем как%~\cite{early}:
\begin{equation}
\tag{П.2}
\label{eq:to_taylor}
	\textnormal{log}~\left|\frac{\partial{T(\mathbf{w}'^g)}}{\partial{\mathbf{w}}}\right| = \textnormal{log}~|\mathbf{I} - \gamma\mathbf{H}| = \sum_{i=1}^{u} \textnormal{log}~(1-\gamma\lambda_i),
\end{equation}
где $\lambda_i$ --- $i$-е собственное значение гессиана $\mathbf{H}$.

При $(\gamma\lambda_i) ^ 2 \leq (\gamma\lambda_\textnormal{max})^2 < 1$ выражение~\eqref{eq:to_taylor} раскладывается в ряд Тейлора:
\[
	 \sum_{t=1}^{u} \textnormal{log}~(1-\gamma\lambda_i) =  -\gamma \textnormal{Tr}[\mathbf{H}(\mathbf{w}'^g)] - \gamma^2 \textnormal{Tr}[\mathbf{H}(\mathbf{w}'^g)\mathbf{H}(\mathbf{w}'^g)] + o_{\gamma^2 \to 0}(1).
\]
Просуммировав полученные выражения для каждой точки мультистарта и вынеся $o_{\gamma^2 \to 0}(1)$ за скобки, получим выражение~\eqref{eq:entropy}, что и требовалось доказать.
\end{proof}

Получим итоговую формулу для оценки правдоподобия модели.
Оценка~\eqref{eq:elbo_entropy} на шаге оптимизации $\tau$ представима в виде
\begin{equation}
\label{eq:ev_grad_full}
\textnormal{log}~\hat{p}(\mathbf{y}|\mathbf{X},\mathbf{h}) \approx \frac{1}{r} \sum_{g = 1}^r L(\mathbf{W}^g_\tau, \mathbf{X}, \mathbf{y})  + \mathsf{S}\big(q^0(\mathbf{W})\bigr) + \frac{1}{r}\sum_{b=1}^\tau\sum_{g=1}^r \bigl(-\gamma \textnormal{Tr}[\mathbf{H}(\mathbf{W}_b^g)] - \gamma^2 \textnormal{Tr}[\mathbf{H}(\mathbf{W}_b^g)\mathbf{H}(\mathbf{W}_b^g)]  \bigr) 
\end{equation}
с точностью до слагаемых вида $o_{\gamma^2 \to 0}(1)$,
где $\mathbf{W}_b^g$ --- $g$-я реализация параметров модели на шаге оптимизации $b$, $q^0(\mathbf{W})$ --- начальное распределение.



В~\cite{early} предлагается алгоритм приближенного вычисления для выражения, находящегося под знаком суммы в~\eqref{eq:ev_grad_full}:
\[
	-\gamma \textnormal{Tr}[\mathbf{H}(\mathbf{W}^g)] - \gamma^2 \textnormal{Tr}[\mathbf{H}(\mathbf{W}^g)\mathbf{H}(\mathbf{W}^g)]  \approx \mathbf{r}_0^\mathsf{T}\bigl(-2\mathbf{r}_0 + 3\mathbf{r}_1 -\mathbf{r}_2\bigr),
\]
где вектор $\mathbf{r}_0$  порождается из нормального распределения:
$$\mathbf{r}_0 \sim \mathcal{N}(\mathbf{0}, \mathbf{I}), \quad \mathbf{r}_1 = \mathbf{r}_0 - \gamma \mathbf{r}_0^\mathsf{T} \nabla \nabla L, \quad \mathbf{r}_2 = \mathbf{r}_1 - \gamma \mathbf{r}_1^\mathsf{T} \nabla \nabla L.$$


Заметим, что при приближении параметров модели к точке экстремума оценка правдоподобия устремляется в минус бесконечность в силу постоянно убывающей энтропии. Таким образом, чем ближе градиентный метод приближает параметры модели к точке экстремума, тем менее точной становится оценка правдоподобия модели. Один из методов борьбы с данной проблемой будет представлен далее.


\textbf{Модификация алгоритма оптимизации модели.} 
В качестве оператора $T$ предлагается использовать псевдослучайный стохастический градиентный спуск, т.е. градиентный спуск, оптимизирующий параметры $\mathbf{W}^1,\dots,\mathbf{W}^r$ по некоторой случайной подвыборке $\hat{\mathbf{X}}, \hat{\mathbf{y}}$, одинаковой для каждой точки старта $\mathbf{W}^1,\dots,\mathbf{W}^r$:
\begin{equation}
    \label{eq:sgd}
	T(\mathbf{W}) = \mathbf{W} -  \frac{m}{\hat{m}} \gamma \nabla L(\mathbf{W}, \hat{\mathbf{y}}, \hat{\mathbf{X}}),	
\end{equation}
где $\hat{\mathbf{X}}$ --- случайная подвыборка выборки ${\mathbf{X}}$, одинаковая для всех точек мультистарта, $\hat{\mathbf{y}}$ --- соответствующие метки классов, $$|\hat{\mathbf{X}}| = \hat{m}.$$

Как и версия алгоритма с использованием градиентного спуска~\eqref{eq:sgd}, основной проблемой модифицированного алгоритма оценки интеграла~\eqref{eq:elbo2} является грубость аппроксимации исходного распределения $p(\mathbf{W}|\mathbf{f},\mathfrak{D})$.

Рассмотрим пример 2~\eqref{eq:example_post}.
График аппроксимации распределения $p(\mathbf{W}|\mathbf{y}, \mathbf{X},\mathbf{h})$ представлен на рис.~\ref{fig:var},\textit{б}.
Как видно из графика, градиентный спуск сходится к моде распределения. При небольшом количестве итераций полученное распределение также слабо аппроксимирует апостериорное распределение. {При приближении к точке экстремума снижается вариационная оценка правдоподобия модели, что  интерпретируется как возможное начало переобучения~\cite{early}. Таким образом, снижение оценки~\eqref{eq:ev_grad_full} можно использовать как критерий остановки оптимизации модели для снижения эффекта переобучения.  }

На рис.~\ref{fig:var} представлена  {аппроксимация распределения $p(\mathbf{W}|\mathbf{Y}, \mathbf{X},\mathbf{h})$ различными методами: \textit{а}) нормальным распределением с диагональной матрицей ковариаций, \textit{б}) с помощью градиентного спуска, \textit{в}) с помощью стохастической динамики Ланжевена. Точками отмечены параметры модели $\mathbf{f}$, полученные в ходе нескольких запусков оптимизации и являющиеся реализациями случайной величины с распределением $q(\mathbf{W})$. Нормальное распределение слабо аппроксимирует распределение $p(\mathbf{W}|\mathbf{Y}, \mathbf{X},\mathbf{h})$ в силу диагональности матрицы ковариаций. Распределение, полученное с помощью градиентного спуска, слабо аппроксимирует распределение $p(\mathbf{W}|\mathbf{Y}, \mathbf{X},\mathbf{h})$, так как сходится к моде.}





\subsection{Аппроксимация с использованием динамики Ланжевена}
Для достижения нижней оценки интеграла~\eqref{eq:elbo2}, более близкой к реальному значению логарифма интеграла~\eqref{eq:evidence}, чем оценка с использованием градиентного спуска, предлагается использовать стохастическую динамику Ланжевена~\cite{langevin}. Стохастическая динамика Ланжевена представляет собой вариант стохастического градиентного спуска с добавлением гауссового шума:
\begin{equation}
\label{eq:langevin}
	T(\mathbf{W}) = \mathbf{W} -  \gamma \nabla L -\frac{m}{\hat{m}}\textnormal{log}p(\hat{\mathbf{y}}|\hat{\mathbf{X}}, \mathbf{W},\mathbf{h}) + \boldsymbol{\varepsilon}, \quad  \boldsymbol{\varepsilon} \sim \mathcal{N}(\mathbf{0}, {\frac{\gamma}{2}}\mathbf{I}),
\end{equation}
где $\hat{\mathbf{X}}$ --- псевдослучайная подвыборка, $\hat{\mathbf{y}}$ --- соответствующие метки, $\hat{m}$ --- размер подвыборки. Длина шага оптимизации $\gamma$ удовлетворяет  {условиям, гарантирующим сходимость алгоритма в стандартных ситуациях~\cite{langevin}}:
\[
	\sum_{\tau=1}^\infty \gamma_\tau = \infty, \quad \sum_{\tau=1}^\infty \gamma_\tau^2 < \infty.
\]

Для оценки энтропии с учетом шума $\boldsymbol{\varepsilon}$ предлагается использовать следующее неравенство~\cite{entropy,var_grad}:
\[
\hat{\mathsf{S}}\bigl(q^\tau(\mathbf{W})\bigr)   \geq \frac{1}{2}u\textnormal{log}\left(\textnormal{exp}\left(\frac{2\mathsf{S}\bigl(q^\tau(\mathbf{W})\bigr)}{u}\right) + \textnormal{exp}\left(\frac{2\mathsf{S}\bigl( \boldsymbol{\varepsilon})}{u}\right)\right),
\]
{где  $\tau$ --- текущий шаг оптимизации,} $\mathsf{S}\bigl( \mathcal{N}({0}, {\frac{\gamma}{2}})\bigr)$ --- энтропия нормального распределения, $\hat{\mathsf{S}}(q^\tau(\mathbf{W}))$ --- энтропия распределения $q^\tau$ с учетом добавленного шума~$\boldsymbol{\varepsilon}$.


В отличие от стохастического градиентного спуска стохастическая динамика Ланжевена сходится к апостериорному распределению параметров $p(\mathbf{W}|\mathfrak{D},\mathbf{h})$~\cite{langevin, langevin_sato}.  График аппроксимации апостериорного распределения с использованием динамики Ланжевена представлен на рис.~\ref{fig:var},\textit{в}. При одинаковом количестве итераций динамика Ланжевена продолжает аппроксимировать апостериорное распределение, в то время как градиентный спуск сходится к моде распределения. {Как видно из графика, алгоритм, основанный на стохастической динамике Ланжевена, способен давать более точную вариационную оценку правдоподобия~\eqref{eq:elbo2}. В то же время алгоритм более требователен к настройке параметров оптимизации~\cite{lang_cond}: \textit{``быстро изменяющаяся кривизна [траекторий параметров модели] делает методы стохастической градиентной динамики Ланжевена по умолчанию неэффективными''.}}
%However, the rapidly changing curvature renders default SGLD methods inefficient

