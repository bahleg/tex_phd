Проблема  выбора структуры модели является фундаментальной в области машинного обучения интеллектуального анализа данных.
Проблема выбора структуры модели глубокого обучения формулируется следующим образом: решается задача классификации или регрессии на заданной или пополняемой выборке $\mathfrak{D}$. Требуется выбрать структуру нейронной сети, доставляющей минимум ошибки на этой функции и максимум качества на некотором внешнем критерии.
 Под моделью глубокого обучения понимается суперпозиция дифференцируемых по параметрам нелинейный функций. Под структурой модели понимается значения структурных параметров модели, т.е. величин, задающих вид итоговой суперпозиции. 

Формализуем описанную выше задачу.
\begin{defin}
\textit{Объектом} назовем пару $(\mathbf{x}, y), \mathbf{x} \in \mathbb{X} =  \mathbb{R}^n, y \in \mathbb{Y}$. В случае задачи классификации $\mathbb{Y}$ является распределением вероятностей принадлежности объекта $\mathbf{x} \in \mathbb{X}$ множеству классов $\{1, \dots, Z\}$: $\mathbb{Y} \subset [0,1]^Z$, где $Z$ --- число классов. В случае задачи регрессии $\mathbb{Y}$ является некоторым подмножеством вещественных чисел ${y} \in \mathbb{Y}  \subseteq \mathbb{R}$.
Объект состоит из двух частей: $\mathbf{x}$  соответствует \textit{признаковому описанию объекта}, $y$ --- \textit{метке объекта}.
\end{defin}

Задана простая выборка \begin{equation}\label{eq:dataset}\mathfrak{D} = \{(\mathbf{x}_i,y_i)\}, i = 1,\dots,m,\end{equation} состоящая из множества объектов $$\mathbf{x}_i \in \mathbf{X} \subset  \mathbb{X}, \quad {y}_i \in \mathbf{y} \subset \mathbb{Y}.$$ 


\begin{defin}
\textit{Моделью} $\mathbf{f}(\mathbf{w}, \mathbf{x})$ назовем дифференцируемую по параметрам $\mathbf{w}$ функцию из множества признаковых описаний объекта во множество меток:
\[
    \mathbf{f}: \mathbb{X} \times \mathbb{W} \to \mathbb{Y},
\] 
где $\mathbb{W}$ --- пространство параметров функции $\mathbf{f}$.
\end{defin}
Специфика задачи  выбора модели \textit{глубокого обучения} заключатеся в том, что модели глубокого обучения могут иметь значительное число параметров, что приводит к неприменимости ряда методов оптимизации и выбора модели. 
Перейдем к формальному описанию параметрического семейства моделей глубокого обучения. 
\begin{defin}
Пусть задан направленный граф $(V,E)$. Пусть для каждого ребра $(j,k) \in E$ определен вектор базовых функций  мощности $K^{j,k}$: $\mathbf{g}^{j,k} = [\mathbf{g}^{j,k}_0, \dots, \mathbf{g}^{j,k}_{K^{j,k}}]$. Пусть также для каждой вершины $v \in V$ определена функция агрегации $\textbf{agg}_v$. Граф $(V, E)$ в совокупности со множестом векторов базовых функций $\{\mathbf{g}^{j,k}, (j,k) \in E\}$ и множеством функций агрегаций $\{ \textbf{agg}_v, {v \in V}\}$ называется \textit{параметрическим семейством моделей} $\mathfrak{F}$, если функция, задаваемая как 
\begin{equation}
\label{eq:modelfam}
    \mathbf{f}_k(\mathbf{x}) = \textbf{agg}_k\left(\{ \langle \boldsymbol{\gamma}^{j,k}, \mathbf{g}^{j,k} \rangle \left(\mathbf{f}_j(\mathbf{x})\right)| j \in \text{Adj}(v_k)\}\right), \quad \mathbf{f}_0(\mathbf{x}) = \mathbf{x}
\end{equation}
является моделью при любых значениях векторов, $\boldsymbol{\gamma}^{j,k} \in [0,1]^{K^{j,k}}$.
\end{defin}
Примером функций агрегации выступают функции суммы и конкатенации векторов.

\begin{defin}
Функции $\mathbf{f}_1, \dots, \mathbf{f}_{|V|}$ из~\eqref{eq:modelfam} назовем называть \textit{слоями или подмоделями} модели $\mathbf{f}$.
\end{defin}


Пример параметрического семейства моделей, которое описывает сверточную нейронную сеть, представлена на Рис.~\ref{fig:scheme_cnn}. Семейство задает множество моделей с двумя операциями свертки с одинаковым размером фильтра $c_0$ и различным числом каналов $c_1$ и $c_2$. Единичная свертка с $c_1$ каналами $\textbf{Conv}(\mathbf{x}, c_1, 1)$ требуется для выравнивания размерностей скрытых слоев. Каждая модель параметрического семейства задается формулой: 
\[
    \mathbf{f} = \textbf{agg}_2\left(\left\{\boldsymbol{\gamma}^{1,2}_0 \mathbf{g}^{1,2}_0 \left(\textbf{agg}_1 \left(\{\boldsymbol{\gamma}^{0,1}_0 \mathbf{g}^{0,1}_0(\mathbf{x}), \boldsymbol{\gamma}^{0,1}_1 \mathbf{g}^{0,1}_1(\mathbf{x})  \} \right)\right)\right\}\right).
\]
Положим, что функции агрегации $ \textbf{agg}_1,  \textbf{agg}_2$ являются операциями суммы. Заметим, что к вершине ``2'' ведет только одно ребро, поэтому операцию суммы можно опустить. 
Итоговая формула модели задается следующим образом:
\[
    \mathbf{f} = \boldsymbol{\gamma}^{1,2}_0 \textbf{softmax} \left(\boldsymbol{\gamma}^{0,1}_0 \textbf{Conv}(\mathbf{x}, c_0, c_1)(\mathbf{x}) + \boldsymbol{\gamma}^{0,1}_1 \textbf{Conv}(\mathbf{x}, 1, c_1) \circ \textbf{Conv}(\mathbf{x}, c_0, c_2)(\mathbf{x}) \right).
\]



\begin{figure}
\begin{tikzpicture}[node distance=cm, auto]
  %\tikzstyle{every state}=[fill=red,draw=none,text=white]

  \node (f0)  at (1,6)                  {$\mathbf{f}_0(\mathbf{x}) = \mathbf{x}$};
  %\node (g11) at (6,3)                    {$\mathbf{g}^{1,1}(\mathbf{x})$};% = \text{Conv}(\mathbf{x}, 3, 32, 1)$};
  %\node (g12)  at (6,9)                   {$\mathbf{g}^{1,2}(\mathbf{x})$};% = \text{Conv}(\mathbf{x}, 4, 32, 1)$};
  \node (f1)  at (9,6)                 {$\mathbf{f}_1(\mathbf{x})$};% = \gamma^{1,1}\mathbf{g}^{1,1}(\mathbf{x}) +  \gamma^{1,2}\mathbf{g}^{1,2}(\mathbf{x})$};
  %\node (g21) at (12,6)                   {$\mathbf{g}^{2,1}(\mathbf{x})$};% = \boldsymbol{\sigma}(\mathbf{w}^{2,1}\mathbf{x})$};
  \node (f2)  at (15,6)                   {$\mathbf{f}_2(\mathbf{x})$};% = \gamma^{2,1}\mathbf{g}^{2,1}(\mathbf{x})$};
  \path[->]  (f0) edge [bend left=50] node {$\mathbf{g}^{0,1}_0(\mathbf{x}) = \textbf{Conv}(\mathbf{x}, c_0, c_1)$}(f1);
  \path[->] (f0)  edge[bend right=50] node[below] {$\mathbf{g}^{0,1}_1(\mathbf{x}) = \textbf{Conv}(\mathbf{x}, 1, c_1) \circ \textbf{Conv}(\mathbf{x}, c_0, c_2)$}(f1);
  \path[->] (f1)  edge node {$\mathbf{g}^{1,2}_0(\mathbf{x}) = \textbf{softmax}(\mathbf{w}^{2,1}\mathbf{x})$}(f2);            
  \draw[->] (f1) to (f2);
 
\end{tikzpicture}
\caption{Пример параметрического семейства моделей глубокого обучения: семейство описывает сверточную нейронную сеть.}
\label{fig:scheme_cnn}

\end{figure}




\begin{defin}
\textit{Параметрами }модели $\mathbf{f}$ из параметрического семейства моделей $\mathfrak{F}$  назовем конкатенацию векторов параметров всех базовых функций $\{\mathbf{g}^{j,k}| {(j,k) \in E} \}, \mathbf{w} \in \mathbb{W}.$ Вектор параметров базовой функции $\mathbf{g}^{j,k}_l$ будем обозначать как $\mathbf{w}^{j,k}_l$.
\end{defin}



\begin{defin}
Структурой $\boldsymbol{\Gamma}$  модели $\mathbf{f}$ из параметрического семейства моделей $\mathfrak{F}$  назовем конкатенацию векторов $\boldsymbol{\gamma}^{j,k}$. Множество всех возможных значений структуры $\boldsymbol{\Gamma}$ будем обозначать как $\amsmathbb{\Gamma}$.
Векторы $\boldsymbol{\gamma}^{j,k}, (j,k) \in E$ назовем \textit{структурными параметрами модели.}
\end{defin}

\begin{defin}
\textit{Параметризацией }множества моделей $M$ назовем параметрическое семейство моделей $\mathfrak{F}$, такое что для каждой модели $\mathbf{f} \in M$ существуют значение структуры модели $\boldsymbol{\Gamma}$ при котором функция $\mathbf{f}$ совпадает с функцией~\eqref{eq:modelfam}.
\end{defin}

\textbf{TODO} Можно доказать, что для любого множества хороших (дифференцируемых?) моделей существует параметризация.

Рассмотрим варианты ограничений, которые накладываются на структурные параметры $\boldsymbol{\gamma}_{j,k}$ параметрического семейства моделей. Цель данных ограничений --- уточнение архитектуры модели глубокого обучения, которую требуется получить. 
\begin{enumerate}
\item Структурные параметры лежат на веришнах булевого куба: $\boldsymbol{\gamma}_{j,k} \in \{0,1\}^{K^{j,k}}$. Структурные параметры  $\boldsymbol{\gamma}_{j,k}$ интерпретируются как параметр включения или выключения компонент вектора базовых функций $\mathbf{g}^{j,k}$ в итоговую модель.
\item Структурные параметры лежат внутри булевого куба: $\boldsymbol{\gamma} \in [0,1]^{K^{j,k}}$. Релаксированная версия предыдущих ограничений, позволяющая проводить градиентную оптимизацию для структурных параметров.
\item Структурные параметры лежат на веришнах симплекса: $\boldsymbol{\gamma}_{j,k} \in \bar{\Delta}^{K^{j,k}-1}$. Каждый вектор структурных параметров $\boldsymbol{\gamma}_{j,k}$ имеет только одну ненулевую компоненту, определяющую какая из базовых функций $\mathbf{g}^{j,k}$ войдет в итоговую модель. Примером параметрического семейства моделей, требующим такое ограничение является семейство полносвязанных нейронных сетей с одним скрытым слоем и двумя значениями коилчества нейронов на скрытом слое. Схема семейства представлена на Рис.~\ref{fig:scheme_mlp}. Данное семейство можно представить как семейство с двумя базовыми функциями вида $\mathbf{g} = \boldsymbol{\sigma}(\mathbf{w}\mathbf{x}),$ где матрицы параметров каждой из функций  $\mathbf{g}^{1,1}, \mathbf{g}^{1,2}$ имеют фиксированное число нулевых столбцов. Количество этих столбцов определяет размерность итогового скрытого пространства (невырожденного?) или числа нейронов на скрытом слое.
\item  Структурные параметры лежат внутри симплекса: $\boldsymbol{\gamma}_{j,k} \in {\Delta}^{K^{j,k}-1}$. Релаксированная версия предыдущих ограничений, позволяющая проводить градиентную оптимизацию для структурных параметров. Значений стуктурных параметров $\boldsymbol{\gamma}_{j,k}$ интерпретируются как вклад каждой компоненты вектора базовых функций $\mathbf{g}^{j,k}$ в итоговую модель. 
\end{enumerate}

Пример, иллюстрирующий представленные выше ограничения, изображен на Рис.~\ref{fig:limit_ex}.
В данной работе рассматривается случай, когда на структурные параметры наложено ограничение 4.  Данные ограничения позволяют решать задачу выбора модели как для семейства моделей типа многослойных полносвязных нейронных сетей, так и для более сложных параметрических семейств~\cite{darts}. 
\begin{figure}
 \begin{minipage}[t]{.5\textwidth}
        \centering
%1 limit
\begin{tikzpicture}[%
x={(2cm,0cm)},
y={(0cm,2cm)},
z={({0.5*cos(45)},{0.5*sin(45)})},
]

\coordinate (A) at (0,0,0); 
\coordinate (B) at (1,0,0) ;
\coordinate (C) at (1,1,0); 
\coordinate (D) at (0,1,0); 
\coordinate (E) at (0,0,1); 
\coordinate (F) at (1,0,1); 
\coordinate (G) at (1,1,1); 
\coordinate (H) at (0,1,1   );

%Ecken
\node[circle,scale=0.5,fill=black,draw=black](Ap) at (0,0,0){};
\node[circle,scale=0.5,fill=black,draw=black](Bp) at (1,0,0){};
\node[circle,scale=0.5,fill=black,draw=black](Cp) at (1,1,0){};
\node[circle,scale=0.5,fill=black,draw=black](Dp) at (0,1,0){};
\node[circle,scale=0.5,fill=black,draw=black](Ep) at (0,0,1){};
\node[circle,scale=0.5,fill=black,draw=black](Fp) at (1,0,1){};
\node[circle,scale=0.5,fill=black,draw=black](Gp) at (1,1,1){};
\node[circle,scale=0.5,fill=black,draw=black](Hp) at (0,1,1){};
\node[left= 1pt of A]{[0,0,0]};
\node[right= 1pt of B]{[1,0,0]};
\node[right= 1pt of C]{[1,1,0]};
\node[left= 1pt of D]{[0,1,0]};
\node[left= 1pt of E]{[0,0,1]};
\node[right= 1pt of F]{[1,0,1]};
\node[right= 1pt of G]{[1,1,1]};
\node[left= 1pt of H]{[0,1,1]};

%Kanten
\draw[] (A)
-- (B)  node[midway, below]{}
-- (C)      node[midway, right]{}
-- (D)  node[midway, above]{}
-- (A)  node[midway, left]{};
\draw[] (B) -- (F) -- (G) -- (C);
\draw[] (G) -- (H) -- (D);
\draw[densely dashed] (A) -- (E) -- (F);
\draw[densely dashed] (E) -- (H);

\end{tikzpicture}
\subcaption{}
\end{minipage}
\hfill
 \begin{minipage}[t]{.5\textwidth}
        \centering

%2 limit
\begin{tikzpicture}[%
x={(2cm,0cm)},
y={(0cm,2cm)},
z={({0.5*cos(45)},{0.5*sin(45)})},
]

\coordinate (A) at (0,0,0); 
\coordinate (B) at (1,0,0) ;
\coordinate (C) at (1,1,0); 
\coordinate (D) at (0,1,0); 
\coordinate (E) at (0,0,1); 
\coordinate (F) at (1,0,1); 
\coordinate (G) at (1,1,1); 
\coordinate (H) at (0,1,1   );

%Ecken
\node[left= 1pt of A]{[0,0,0]};
\node[right= 1pt of B]{[1,0,0]};
\node[right= 1pt of C]{};
\node[left= 1pt of D]{[0,1,0]};
\node[left= 1pt of E]{};
\node[right= 1pt of F]{[1,0,1]};
\node[right= 1pt of G]{[1,1,1]};
\node[left= 1pt of H]{[0,1,1]};

%Kanten
\draw[fill=gray] (A)
-- (B)  node[midway, below]{}
-- (C)      node[midway, right]{}
-- (D)  node[midway, above]{}
-- (A)  node[midway, left]{};
\draw[fill=gray] (B) -- (F) -- (G) -- (C);
\draw[fill=gray] (G) -- (H) -- (D);
\draw[fill=gray] (A) -- (E) -- (F);
\draw[fill=gray] (E) -- (H);
\draw[fill=gray] (D) -- (H) -- (G) -- (C);
\end{tikzpicture}
\subcaption{}
\end{minipage}
\hfill
 \begin{minipage}[t]{.5\textwidth}
        \centering
%3 limit
\begin{tikzpicture}[%
x={(2cm,0cm)},
y={(0cm,2cm)},
z={({0.5*cos(45)},{0.5*sin(45)})},
]

\coordinate (A) at (0,0,0); 
\coordinate (B) at (1,0,0) ;
\coordinate (C) at (1,1,0); 
\coordinate (D) at (0,1,0); 
\coordinate (E) at (0,0,1); 
\coordinate (F) at (1,0,1); 
\coordinate (G) at (1,1,1); 
\coordinate (H) at (0,1,1   );

%Ecken
\node[circle,scale=0.5,fill=black,draw=black](Bp) at (1,0,0){};
\node[circle,scale=0.5,fill=black,draw=black](Dp) at (0,1,0){};
\node[circle,scale=0.5,fill=black,draw=black](Ep) at (0,0,1){};
\node[left= 1pt of A]{};
\node[right= 1pt of B]{[1,0,0]};
\node[right= 1pt of C]{};
\node[left= 1pt of D]{[0,1,0]};
\node[left= 1pt of E]{[0,0,1]};
\node[right= 1pt of F]{};
\node[right= 1pt of G]{};
\node[left= 1pt of H]{};

%Kanten
\draw[] (A)
-- (B)  node[midway, below]{}
-- (C)      node[midway, right]{}
-- (D)  node[midway, above]{}
-- (A)  node[midway, left]{};
\draw[] (B) -- (F) -- (G) -- (C);
\draw[] (G) -- (H) -- (D);
\draw[densely dashed] (A) -- (E) -- (F);
\draw[densely dashed] (E) -- (H);

\end{tikzpicture}
\subcaption{}
\end{minipage}
\hfill
 \begin{minipage}[t]{.5\textwidth}
        \centering
%4 limit
\begin{tikzpicture}[%
x={(2cm,0cm)},
y={(0cm,2cm)},
z={({0.5*cos(45)},{0.5*sin(45)})},
]

\coordinate (A) at (0,0,0); 
\coordinate (B) at (1,0,0) ;
\coordinate (C) at (1,1,0); 
\coordinate (D) at (0,1,0); 
\coordinate (E) at (0,0,1); 
\coordinate (F) at (1,0,1); 
\coordinate (G) at (1,1,1); 
\coordinate (H) at (0,1,1   );

%Ecken
\node[left= 1pt of A]{};
\node[right= 1pt of B]{[1,0,0]};
\node[right= 1pt of C]{};
\node[left= 1pt of D]{[0,1,0]};
\node[left= 1pt of E]{[0,0,1]};
\node[right= 1pt of F]{};
\node[right= 1pt of G]{};
\node[left= 1pt of H]{};

%Kanten
\draw[] (A)
-- (B)  node[midway, below]{}
-- (C)      node[midway, right]{}
-- (D)  node[midway, above]{}
-- (A)  node[midway, left]{};
\draw[] (B) -- (F) -- (G) -- (C);
\draw[] (G) -- (H) -- (D);
\draw[densely dashed] (A) -- (E) -- (F);
\draw[densely dashed] (E) -- (H);
\draw[fill=gray] (B) -- (D) -- (E);


\end{tikzpicture}
\subcaption{}
\end{minipage}

\caption{Примеры ограничений для одного структурного параметра $\boldsymbol{\gamma}, |\boldsymbol{\gamma}| = 3$. \\
а) структурный параметр лежит на вершинах куба, б) структурный параметр лежит внутри куба, в) структурный параметр лежит на верщинах симплекса, г) структурный параметр лежит внутри симплекса.} 
\label{fig:limit_ex}
\end{figure}



\begin{figure}
\begin{tikzpicture}[node distance=cm, auto]
  %\tikzstyle{every state}=[fill=red,draw=none,text=white]

  \node (f0)  at (1,6)                  {$\mathbf{f}_0(\mathbf{x}) = \mathbf{x}$};
  %\node (g11) at (6,3)                    {$\mathbf{g}^{1,1}(\mathbf{x})$};% = \text{Conv}(\mathbf{x}, 3, 32, 1)$};
  %\node (g12)  at (6,9)                   {$\mathbf{g}^{1,2}(\mathbf{x})$};% = \text{Conv}(\mathbf{x}, 4, 32, 1)$};
  \node (f1)  at (9,6)                 {$\mathbf{f}_1(\mathbf{x})$};% = \gamma^{1,1}\mathbf{g}^{1,1}(\mathbf{x}) +  \gamma^{1,2}\mathbf{g}^{1,2}(\mathbf{x})$};
  %\node (g21) at (12,6)                   {$\mathbf{g}^{2,1}(\mathbf{x})$};% = \boldsymbol{\sigma}(\mathbf{w}^{2,1}\mathbf{x})$};
  \node (f2)  at (15,6)                   {$\mathbf{f}_2(\mathbf{x})$};% = \gamma^{2,1}\mathbf{g}^{2,1}(\mathbf{x})$};
  \path[->]  (f0) edge [bend left=50] node {$\mathbf{g}^{0,1}_0(\mathbf{x}) = \boldsymbol{\sigma}(\mathbf{w}^{0,1}_0\mathbf{x})$}(f1);
  \path[->] (f0)  edge[bend right=50] node[below] {$\mathbf{g}^{0,1}_1(\mathbf{x}) = \boldsymbol{\sigma}(\mathbf{w}^{0,1}_1\mathbf{x})$}(f1);
  \path[->] (f1)  edge node {$\mathbf{g}^{1,2}_0(\mathbf{x}) = \textbf{softmax}(\mathbf{w}^{1,2}_0\mathbf{x})$}(f2);       
  \draw[->] (f1) to (f2);
 
\end{tikzpicture}
\label{fig:scheme_mlp}
\caption{Пример параметрического семейства моделей глубокого обучения: семейство описывает многослойную полносвязную нейронную сеть с одним скрытым слоем и нелинейной функцией активации $\boldsymbol{\sigma}$.}

\end{figure}




Для дальнейшей постановки задачи введем понятие вероятностной модели, и связанных с ним определений. Будем полагать, что для параметров модели $\mathbf{w}$ и структуры  $\boldsymbol{\Gamma}$ задано распределение $p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h})$, соответствующее предположениям о распределении структуры и параметров. 

\begin{defin}
\textit{Гиперпараметрами} $\mathbf{h}\in \mathbb{H}$ модели  назовем параметры распределения $p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h})$.
\end{defin}

\begin{defin}
\textit{Априорным распределением} параметров и структуры модели назовем вероятностное распределение, соответствующее предположениям о распределении параметров модели:
\[
    p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}): \mathbb{W} \times \Delta{\boldsymbol{\Gamma}} \times \mathbb{H} \to \mathbb{R}^{+}, 
\]
где $\mathbb{W}$ --- множество значений параметров модели.
\end{defin}


Одной из возможных частных постановок задачи выбора структуры модели является \textit{двусвязный байесовский вывод.} 
На \textit{первом уровне} байесовского вывода  находится апостериорное распределение параметров.

\begin{defin}
\textit{Апостериорным распределением} назовем распределение вида
\begin{equation}
\label{eq:posterior}
    p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{y}, \mathbf{X}, \mathbf{h}) = \frac{p(\mathbf{y}|\mathbf{w}, \boldsymbol{\Gamma}, \mathbf{X}, \mathbf{h})p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h})}{p(\mathbf{y}|\mathbf{X})} \propto p(\mathbf{y}|\mathbf{w},  \boldsymbol{\Gamma}, \mathbf{X}, \mathbf{h}) p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h}).% \mathbb{W} \times \Delta{\boldsymbol{\Gamma}} \times \mathbb{X}  \times \mathbb{H}. 
\end{equation}
\end{defin}

\begin{defin}
\textit{Вероятностной моделью глубокого обучения} назовем совместное распределение вида
\[
    p({y}, \mathbf{w},  \boldsymbol{\Gamma}|\mathbf{x}, \mathbf{h}) = p({y}|\mathbf{x}, \mathbf{w},  \boldsymbol{\Gamma})p( \mathbf{w},  \boldsymbol{\Gamma}|\mathbf{h}): \mathbb{Y} \times \mathbb{W}  \times \Delta{\boldsymbol{\Gamma}} \times \mathbb{R}^{+}.
\]
\end{defin}

\begin{defin}
\textit{Функцией правдоподбия выборки } назовем величину
\[
    p(y|\mathbf{X}, \mathbf{w}, \boldsymbol{\Gamma}): \mathbb{Y} \times \mathbb{X} \times \mathbb{W} \times \mathbb{\Gamma} \to \mathbb{R}^{+}.
\]
\end{defin}
Для каждой модели определена функция правдоподобия  $p(\mathbf{y}|\mathbf{X}, \mathbf{w}, \boldsymbol{\Gamma})$.
 
На \textit{втором уровне} байесовского вывода осуществляется выбор модели на основе правдоподобия модели.
\begin{defin}
\textit{Правдоподобием модели }назовем величину
\begin{equation}
\label{eq:evidence}
p(y|\mathbf{X}, \mathbf{h}) = \int_{\mathbf{w}, \boldsymbol{\Gamma}} p(\mathbf{y}|\mathbf{X}, \mathbf{w}, \boldsymbol{\Gamma})p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h})d\mathbf{w}d\boldsymbol{\Gamma}.
\end{equation}
\end{defin}
Получение значений апостериорного распределения и правдоподобия модели сетей глубокого обучения является вычислительно сложной процедурой. Для получения оценок на данные величины используют методы, такие как аппроксимация Лапласа~\cite{tokmakova} и вариационная нижняя оценка~\cite{nips}.  В данной работе в качестве метода получения оценок правдоподобия модели выступает вариационное распределение.

\begin{defin}
\textit{Вариационным распределением} назовем параметрическое распределение $q(\mathbf{w}, \boldsymbol{\Gamma})$, являющееся приближением  апостериорного распределения параметров и структуры $p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{X}, \mathbf{y}, \mathbf{h}).$ 
\end{defin}

\begin{defin}
\textit{Вариационными параметрами} модели $\boldsymbol{\theta} \in \mathbb{R}^u$ назовем параметры вариационного распределения $q$.
\end{defin} 

\begin{defin}
\label{def:l}
Пусть задано вариационное распределения $q$.
\textit{Функцией потерь} $L( \boldsymbol{\theta}| \mathbf{h}, \mathbf{X}, \mathbf{y})$ для модели $\mathbf{f}$ назовем дифференцируемую функцию, принимаемую за качество модели на обучающей выборки при параметрах модели, получаемых из  распределения $q$.
\end{defin}

В качестве функции $L$ может выступать минус логарифм правдоподобия выборки $\text{log}~p(y|\mathbf{X}, \mathbf{w}, \boldsymbol{\Gamma})$  и логарифм апостериорной вероятности \text{log}~$p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{y}, \mathbf{X}, \mathbf{h}) $ параметров и структуры модели на обучающей выборке.

\begin{defin}
\label{def:q}
Пусть задано вариационное распределения $q$ и функция потерь $L$. 
\textit{Функцией валидации} $Q(\mathbf{h}| \boldsymbol{\theta}, \mathbf{X}, \mathbf{y} )$ для модели $\mathbf{f}$ назовем дифференцируемую функцию, принимаемую за качество модели при векторе $\boldsymbol{\theta}$, заданном неявно.
\end{defin}

 

В данной работе задача выбора структуры модели и параметров модели ставится как двухуровневая задача оптимизации:
\begin{equation}
\label{eq:optim_problem}
	\mathbf{h}^{*} = \argmin_{\mathbf{h} \in \mathbb{H}} Q(\mathbf{h}|  \boldsymbol{\theta}^{*}, \mathbf{X}, \mathbf{y} ),
\end{equation}
где $\boldsymbol{\theta}^{*}$ --- решение задачи оптимизации
\begin{equation}
\label{eq:optim_problem_in}
   \boldsymbol{\theta}^{*} = \argmin_{\boldsymbol{\theta} \in \mathbb{R}^u} L(\boldsymbol{\theta}|  \mathbf{h},  \mathbf{X}, \mathbf{y}).
\end{equation}

\begin{defin}
\textit{Выбором модели} $\mathbf{f}$ назовем решение двухуровневой задачи оптимизации~\eqref{eq:optim_problem}.
\end{defin}

Рассмотрим для примера базовый вариант выбора модели с применением функций $q, L, Q$.
\begin{example}
Будем полагать, что задано разбиение выборки на обучающую $\mathfrak{D}_\text{train}$ и валидационную $\mathfrak{D}_\text{valid}$ части.
Положим в качестве вариационных параметров $\boldsymbol{\theta}$ параметры $\mathbf{w}$ и структуры $\boldsymbol{\Gamma}$ модели:
\[
    \boldsymbol{\theta} = [\mathbf{w}, \boldsymbol{\Gamma}].
\]
Пусть также задано априорное распределение $p(\mathbf{w}, \boldsymbol{\Gamma}|\mathbf{h})$.
Положим в качестве функции $L$ минус логарифм апостериорной вероятности модели:
\[
    L = -\sum_{\mathbf{x},y \in \mathfrak{D}_\text{train}} \text{log}p(y, \mathbf{w}, \boldsymbol{\Gamma}|\mathbf{x}).
\]
Положим в качестве функции $Q$  минус правдоподобие выборки при условии параметров $\mathbf{w}$ и структуры $\boldsymbol{\Gamma}$:
\[
    Q = -\sum_{\mathbf{x},y \in \mathfrak{D}_\text{valid}} \text{log}p(y|\mathbf{x}, \mathbf{w}, \boldsymbol{\Gamma}).
\]
Оптимизация параметров и структуры производится по обучающей выборке. Гиперпараметры $\mathbf{h}$ выступают в качестве регуляризатора, чья оптимизация производится по валидационной выборке. Подобная оптимизация позволяет предотвратить переобучение модели~\cite{hyper}.
\end{example}

Частным случаем задачи выбора структуры глубокой сети является выбор обобщенно-линейных моделей. Отдельные слои полносвязанных нейросетей являются обобщенно-линейными модели. Задачу выбора обобщенно-линейной моделей сводится к задаче выбора признаков, методы решения которой делятся на три группы~\cite{feature_select}:
\begin{enumerate}
\item Фильтрационные методы. Не используют какой-либо информации о модели, а отсекают признаки только на основе статистических показателей, учитывающих взаимосвязь признаков и меток объектов.
\item Оберточные методы  анализируют подмножества признаков. Они выбирают не признаки, а подмножества признаков, что позволяет учесть корреляция признаков.
\item Методы погружения оптимизируют модели и проводият выбор признаков в единой процедуре, являясь комбинацией предыдущих типов отбора признаков.
\end{enumerate} 

\section{Критерии выбора модели глубокого обучения}
В данном разделе рассматриваются различные критерии выбора моделей глубокого обучения, соответствующие функции валидации $Q$.
В данной работе в качестве критерия выбора модели предлагается субоптимальная сложность модели. Под сложностью модели понимается \emph{правдоподобие модели}~\eqref{eq:evidence}, являющееся байесовской интерпретацией \emph{мнимальной длины описания}~\cite{mdl}, т.е. минимальное количество информации, которое требуется передать о модели и о выборке:
\begin{equation}
\label{eq:mdl}
	\textnormal{MDL}(\mathbf{y},\mathbf{f}) = \textnormal{Len}(\mathbf{y}|\mathbf{w}^{*}, \mathbf{f}) + \textnormal{COMP}(\mathbf{f}),
\end{equation}
где  $\textnormal{Len}(\mathbf{y}|\mathbf{w}^{*}, \mathbf{f})$ --- \emph{длина описания} матрицы $\mathbf{y}$ с использованием модели $\mathbf{f}$ и оценки вектора параметров $\mathbf{w}^{*}$, полученных методом наибольшего правдоподобия, а $\textnormal{COMP}(\mathbf{f})$ --- величина, характеризующая \emph{параметрическую сложность} модели, т.е. способность модели описать произвольную выборки из $\mathbb{X}$~\cite{mdl}.

{В общем случае правдоподобие модели является трудновычислимым.} Для получения оценки правдоподобия используются вариационные методы получения оценки правдоподобия~\cite{Bishop}, основанные на аппроксимации неизвестного другим заданным распределением. Под субоптимальной сложностью понимается вариационная оценка правдоподобия модели.
Альтернативной величиной, характеризующей сложность модели, выступает радемахеровская сложность~\eqref{eq:rad}. Данная величина используется как критерий для продолжения итеративного построения модели в~\cite{adanet}.

В работе~\cite{perekrestenko} рассматривается ряд критериев сложности моделей глубокого обучения и их взаимосвязь. В работе~\cite{vladis} в качестве критерия сложности модели выступает показатель нелинейности, характеризуемый степенью полинома Чебышева, аппроксимирующего функцию. В работе~\cite{need_prune} анализируется показатель избыточности параметров сети. Утверждается, что по небольшому набору параметров в глубокой сети с большим количеством избыточных параметров возможно спрогнозировать значения остальных. В работе~\cite{rob} рассматривается показатель робастности моделей, а также его взаимосвязь с топологией выборки и классами функций, в частности рассматривается влияние функции ошибки и ее липшицевой константы на робастность моделей. Схожие идеи были рассмотрены в работе~\cite{intrig}, в которой исследуется устойчивость классификации модели под действием шума. 
В ряде работ~\cite{MacKay,Bishop,tokmakova,zaitsev,strijov_webber, strijov_dsc} в качестве критерия выбора модели  выступает правдоподобие модели. В работах~\cite{tokmakova,zaitsev,strijov_webber, strijov_dsc} рассматривается проблема выбора модели и оценки гиперапараметров в задачах регрессии. Альтернативным критерием выбора модели является минимальная длина описания~\cite{mdl}, являющаяся показателем статистической сложности модели и заданной выборки. В работе~\cite{mdl} рассматриваются различные модификации и интерпретации минимальной длины описания, в том числе связь с правдоподобием модели.

Одним из методов получения приближенного значения правдоподобия модели является вариационный метод получения нижней оценки правдоподобия~\cite{Bishop}.  В работе~\cite{hoffman} рассматривается стохастическая версия вариационного метода.
В~\cite{nips} рассматривается алгоритм получения вариационной нижней оценки правдоподобия  для оптимизации гиперпараметров моделей глубокого обучения. 
В работе~\cite{varmc} рассматривается взаимосвязь градиентных методов получения вариационной нижней оценки интеграла с методом Монте-Карло.
В~\cite{early} рассматривается стохастический градиентный спуск в качестве оператора, порождающего распределение, аппроксимирующее апостериорное распределение параметров модели. В работе отмечается, что стохастический градиентный спуск не оптимизирует вариационную оценку правдоподобия, а приближает ее только до некоторого числа итераций оптимизации. 
Схожий подход рассматривается в работе~\cite{sgd_cont}, где также рассматривается стохастический градиентный спуск в качестве оператора, порождающего апостериорное распределение параметров. В работе~\cite{langevin} предлагается модификация стохастического градиентного спуска, аппроксимирующая апостериорное распределение. 


Альтернативным методом выбора модели является выбор модели на основе скользящего контроля~\cite{cv_ms, tokmakova}. Проблемой такого подхода является высокая вычислительная сложность~\cite{expensive, expensive2}. В работах~\cite{bias,bias2} рассматривается проблема смещения оценок качества модели и гиперпараметров, получаемых при использовании $k$-fold метода скользящего контроля, при котором выборка делится на $k$-частей с обучением на $k-1$ части и валидацией результата на оставшейся части выборки. 


\section{Оптимизация параметров в задаче выбора структуры модели}
Один из подходов к выбору оптимальной модели заключается в итеративном удалении наименее информативных параметров модели. 
В данном разделе собраны методы оптимизации структуры существующей модели. 

\textbf{Алгоритмы прореживания параметров модели.}
В~\cite{obd} предлагается удалять неинформативные параметры модели.
Для этого находится точка оптимума $\boldsymbol{\theta}^{*}$ функции $L$, и производится разложение функции $L$ в ряд Тейлора в окрестности $\boldsymbol{\theta}^{*}$:
\begin{equation}
\label{eq:obd}
    L(\boldsymbol{\theta}^{*} + \Delta\boldsymbol{\theta})  - L(\boldsymbol{\theta}^{*}) = \frac{1}{2} \Delta\boldsymbol{\theta}^{\mathsf{T}}\mathbf{H}\Delta\boldsymbol{\theta} + o(||\Delta\boldsymbol{\theta}||^{3}),
\end{equation}
где $\mathbf{H}$ --- гессиан функции $L$. Связь между параметрами не учитывается, поэтому гессиан матрицы $L$ является диагональным.
Положим в качестве операции удаления параметра замену его значения на ноль. Выбор наиболее неинформативного параметра сводится к задаче условной минимизации~\eqref{eq:obd} при условиях вида
\[
    {\theta}_i + \Delta{\theta}_i = 0, \quad  {\theta}_i \in \boldsymbol{\theta}.
\] 

В результате решения данной задачи минимизации каждому параметру определяется функция выпуклости
\[
    \text{saliency}({\theta}_i) = \frac{{\theta}^2_i}{2H_{i,i}}.
\]
Данная функция характеризует информативность параметра.

В~\cite{obs} было предложено развитие данного метода. В отличие от~\cite{obd} не вводится предположений о диагональности гессиана функции ошибок, поэтому удаление неинформативных параметров модели производится точнее. Для получения оценок гессиана и его обратной матрицы применяется итеративный алгоритм.

\textbf{Алгоритмы компрессии параметров модели.}
В~\cite{weight_quantization, weight_quantization2,nvidia_prune} предлагаются методы компрессии параметров сетей глубокого обучения. Основным отличием задачи прореживания от задачи компрессии выступает эксплуатационное требование: если прореживание используется для получения оптимальной и наиболее устойчивой модели, то компрессия производится для уменьшения потребляемых вычислительных ресурсов при сохранении основных эксплуатационных характеристик исходной модели~\cite{weight_quantization2}.
В~\cite{nvidia_prune}
предлагается итеритавиное использование регуляризации типа DropOut~\cite{dropout} для прореживания модели. 
В~\cite{weight_quantization, weight_quantization2} используются методы снижения вычислительной точности представления парамеров модели на основе кластеризации параметров $\mathbf{w}$ модели: вместо значений параметров предлагается хранить идентификатор кластера, соответствующего параметру, что существенно снижает количество требуемой памяти.
В~\cite{weight_quantization2} предлагается метод компрессии, основанный на кластеризации значений параметров модели и представлении их в сжатом виде на основе кодов Хаффмана.

\textbf{Байесовские методы прореживания параметров модели. }
Байесовский подход к порождению и выбору моделей заключается в использовании вероятностных предположений о распределении параметров и структуры в параметрических семействах моделей. Такой подход позволяет учитывать при выборе моделей не только эксплуатационные критерии качества модели, такие как точность итоговой модели и количество параметров в ней, но и некоторые статистические характеристики модели. 

В работе~\cite{hyper} рассматривается задача оптимизации гиперпараметров.  Авторы предлагают оптимизировать константы $l_2$-регуляризации отдельно для каждого параметра модели, проводится параллель с методами автоматического определения релевантности параметров (англ. automatic relevance determination, ARD)~\cite{MacKay}. 
Идея автоматического определения релевантности заключается в выборе оптимальных начений гиперпараметров $\mathbf{h}$ с дальнейшим удалением неинформативных параметров. Неинформативными параметрами являются те параметры, которые с высокой вероятностью равны нулю относительно априорного или апостериорного распределения.

В работе~\cite{nips} был предложен метод, основанный на получении вариационной нижней оценки правдоподобия модели. В качестве критерия информативности параметра выступает отношение вероятности нахождения параметра в пределах апостериорного распределения к вероятности равенства параметра нулю:
\[
    \left|\frac{\mu_j}{\sigma_j}\right|,  
\]
где $\mu_j, \sigma_j$ --- среднее и дисперсия аппроксимирующего распределения $q$ для параметра $w_j$.

Идея данного метода была развита в~\cite{bayes_compr}, где также используются вариационные методы.  В отличие от~\cite{nips}, в~\cite{bayes_compr} рассматривается ряд априорных распределений параметров, позволяющих прореживать модели более эффективно:
\begin{enumerate}
\item Нормальное распределение с лог-равномерным распределением дисперсии. Для каждого параметра $w \in \mathbf{w}$ задается группа параметров $\boldsymbol{\omega} \in \boldsymbol{\Omega}$, где $\boldsymbol{\Omega}$ --- множество всех групп параметров:
\[
    p(\mathbf{w}, \mathbf{s}) \propto \prod_{\boldsymbol{\omega} \in \boldsymbol{\Omega}} \frac{1}{|\boldsymbol{\omega}|}\prod_{w \in \boldsymbol{\omega}}\mathcal{N}(w|\mathbf{0}, \boldsymbol{\omega}).
\]
\item Априорное распределение задается произведением двух случайных величин ${s}_{\text{general}}, {s}_{jk}$ с половинным распределением Коши $\mathcal{C}^{+}$: одно ответственно за отдельный параметр, другое --- за общее распределение параметров:
\[
    {s}_{\text{general}} \sim \mathcal{C}^{+}(0, \lambda), \quad  {s}_{jk} \sim \mathcal{C}^{+}(0,1), \quad \hat{w}_{jk} \sim \mathcal{N}(0,1), \quad w_{jk} \sim \hat{w}_{jk}s_j  {s}_{\text{general}},
\]
где $\lambda \in \mathbf{h}$ --- параметр распределения.

\end{enumerate}

\section{Оптимизация гиперпараметров модели}
В данном разделе рассматриваются работы, посвященные методам оптимизации гиперпараметров. Методы, используемые для оптимизации гиперпараметров моделей глубокого обучения должны быть эффектинвыми по вычислительным затратам в силу высокой вычислительной сложности оптимизации параметров модели. 
В~\cite{random1,random2} рассматривается задача оптимизации гиперпараметров стохастическими методами. В~\cite{random1} проводится сравнение случайного поиска значений гиперпараметров с переборным алгоритмом. В~\cite{random2} производится сравнение случайного поиска и алгоритмов, основанных на вероятностных моделях.

\textbf{Градиентные методы оптимизации гиперпараметров. } 
\begin{defin} Назовем \textit{оператором оптимизации} алгоритм $T$ выбора вектора параметров $\boldsymbol{\theta}'$  по параметрам предыдущего шага $\boldsymbol{\theta}$:
\begin{equation}
\label{eq:optim_operator}
	\boldsymbol{\theta}' = T(\boldsymbol{\theta} | L, \mathbf{y}, \mathbf{X}, \mathbf{h}, \boldsymbol{\beta}),
\end{equation}
где $\boldsymbol{\beta}$ --- параметры оператора оптимизации или \textit{метапараметры}.
\end{defin}

Пример схожего описания оптимизации модели с использованием оператора оптимизации можно найти в~\cite{early}.

Частным случаем оператора оптимизации является оператор стохастического спуска:
\begin{equation}
\label{eq:sgd_operator}
    T( \boldsymbol{\theta}| L,\mathbf{X},  \mathbf{y},  \mathbf{h}, \boldsymbol{\beta}) = \boldsymbol{\theta} - \beta_{\text{lr}} \nabla L(\boldsymbol{\theta}|   \mathbf{h},  \hat{\mathbf{X}}, \hat{\mathbf{y}}),
\end{equation}
где $\beta_{\text{lr}}$ --- шаг градиентного спуска, $\hat{\mathbf{y}}, \hat{\mathbf{X}}$ --- случайная подвыборка заданной мощности выборки $\mathfrak{D}$.

В случае оптимизации гиперпараметров оператор оптимизации применяется не к вариационным параметрам $\boldsymbol{\theta}$, а к гиперпараметрам $\mathbf{h}$:
\begin{equation}
\label{eq:hyperoptim_operator}
    \mathbf{h} = T(\mathbf{h} | Q,  \mathbf{X},  \mathbf{y},\boldsymbol{\theta}^{*}, \boldsymbol{\beta}),
\end{equation}
где $\boldsymbol{\theta}^{*}$ --- вариационные параметры, полученые в ходе решения задачи оптимизации~\eqref{eq:optim_problem_in}.

В случае, если для решения задачи~\eqref{eq:optim_problem_in} применяется несколько шагов оператора оптимизации~\eqref{eq:optim_operator},
$\boldsymbol{\theta}^{*}$ рассматривается как рекурсивная функция от начального приближения вариационных параметров $\boldsymbol{\theta}^{0}$ и вектора гиперпараметров $\mathbf{h}$:
\begin{equation}
\label{eq:param_trace}
    \boldsymbol{\theta}^{*} = T \circ \dots \circ T(\boldsymbol{\theta}^0 | L, \mathbf{X}, \mathbf{y},  \mathbf{h}, \boldsymbol{\beta}) = \boldsymbol{\theta}^{*}(\boldsymbol{\theta}^{0}, \mathbf{h}).
\end{equation}

Решение задачи оптимизации~\eqref{eq:hyperoptim_operator} при~\eqref{eq:param_trace} является вычислительно сложным, поэтому применяются методы, аппроксимирующие применение градиентных методов при~\eqref{eq:param_trace}.

В~\cite{hyper_bengio} рассматривается оптимизация гиперпараметров градиентными методами для квадратичной функции потерь. В~\cite{hyper} в качестве оператора оптимизации гиперпараметров выступает метод градиентного спуска с моментом. Показано, что использование момента значительно снижает количество вычислительных ресурсов, требуемых для проведения оптимизации. В~\cite{hyper_mad} предлагается аппроксимация градиентного метода, использующая предположение о линейности функции~\eqref{eq:param_trace} от начального приближения $\boldsymbol{\theta}^0$. В~\cite{hyper_hoag} предлагается использовать численные методы для приближенного вычисления оператора оптимизации гиперпараметров. В~\cite{greed_hyper} в качестве аппроксимации~\eqref{eq:param_trace} предлагается рассматривать только последний шаг оптимизации:
\[
    \boldsymbol{\theta}^{*} \approx T(\boldsymbol{\theta}^{\eta-1} | L, \mathbf{y}, \mathbf{X}, \mathbf{h}, \boldsymbol{\beta}),
\]
где $\eta$ --- число шагов оптимизации.


\textbf{Суррогатный выбор моделей. }
Идея суррогатных моделей заключается в аппроксимации модели или параметрического семейства моделей вычислительно менее сложной функцией.

В работе~\cite{bo_gp} предлагается моделировать качество модели $Q$~\eqref{eq:evidence} гауссовым процессом, параметрами которого выступают гиперпараметры исходной модели.

Одна из основных проблем использования гауссового процесса как суррогатной модели --- кубическая сложность оптимизации. В работе~\cite{random_gaus} предлагается использовать случайные подпространства гиперпараметров для ускоренной оптимизации.  В работе~\cite{gp_tree} предлагается кобминация из множества гауссовых моделей и линейной модели, позволяющая модели нелинейные зависимости гиперпараметров, а также существенно сократить сложность оптимизации. 

В работе~\cite{rbf_surrogate} предлагается рассматривать RBF-модель для аппроксимации качества $Q$ исходной модели, что позволяет ускорить процесс оптимизации суррогатной модели. В~\cite{snoek_deep} рассматривается глубокая нейронная сеть в качестве суррогатной функции. Вместо интеграла правдоподобия~\eqref{eq:evidence}, который оценивается в случае использования гауссового процесса в качестве суррогата, используется максимум апостериорной веротяности~\eqref{eq:posterior}.

Одним из параметров гауссовых процессов является функиия ядра гауссового процесса, полностью определяющая процесс в случае нулевого среднего. В работе~\cite{gp_fusion} предлагается функция ядра, определенная на графах:
    \[
    k(v_1,v_2) = r(d(v_1,v_2)),
    \]
где $d$ --- геодезическое расстояние между вершинами графа, $r$ --- некоторая вещественная функция, $v_1, v_2 \in V$.

В работе~\cite{gp_arc} рассматривается задач выбора структуры нейросети. Предлагается метод построения ковариационной функции для сравнения разнородных графов, соответствующих разным моделям нейронных сетей. Ковариационная функция основывается на метрике, заданной на некоторых характеристиках $g(v)$ вершин, возможно не определенных для сравниваемых графов:
\[
    d_v\bigl( (V_1, E_1), (V_2,E_2) \bigr) = \begin{cases}
    0, v \not \in V_1, v \not \in V_2,\\    
    \lambda_1\sqrt{2}\sqrt{1- \text{cos}(\pi\lambda_2\frac{g_1 - g_2}{\text{sup}(g) - \text{inf}(g) })}, v \in V_1, v \in V_2,\\
    \lambda_1 \text{ иначе,}
    \end{cases}
\]
где $\lambda_1, \lambda_2$ --- параметры функции $d_v$.

\section{Порождениие и выбор структуры модели глубокого обучения}
В данном разделе рассматриваются работы, посвященные порождению и модификации структуры моделей. В отличие от работ, описанных в предыдущих разделах, в следующих работах рассматриваемым объектом является не отдельный параметр, а подмодель или группа параметров, входящая в эту подмодель.
 



\textbf{Графовое представление структуры модели. }
Одним из возможных представлений структуры моделей глубокого обучения является графовое представление, в котором в качестве ребер графа выступают нелинейные функции, а в качестве вершин графа --- представление выборки под действием соответствующих нелинейных функций. 
Данный подход к описанию модели является соответсвтует  походам, описанным в~\cite{vokov}, а также в библиотеках типа TensorFlow~\cite{tensorflow}, Theano~\cite{theano}, Pytorch~\cite{pytorch}, в которых модель рассматривается как граф, ребрами которого выступают математические операции, а вершинами --- результат их действия на выборку. 
 В то же время, существуют и другие способы представления модели. В ряде работ, посвященных байесовской оптимизации~\cite{snoek_deep,rbf_surrogate,bo_gp}, модель рассматривается как черный ящик, над которым производится ограниченный набор операций типа ``произвести оптимизацию параметров'' и ``предсказать значение зависимой переменной по независимой переменной и параметрам модели''.
Подход, описанный в данных работах, также коррелирует с  библиотеками машинного обучения, такими как Weka~\cite{weka}, RapidMiner~\cite{rapidminer} или sklearn~\cite{sklearn}, в которых модель машинного обучения рассматривается как черный ящик.

В~\cite{graphs} представлен обзор по графовому описанию моделей глубокого обучения, предлагается метод фомрального описания графовых сетей (англ. Graph Network), являющийся обобщением предложенных ранее графовых описаний моделей.

В работе~\cite{search_space} рассматриваются подходы к порождению моделей глубокого обучения. Предлагается формализация пространства поиска и формальное описание элементов  пространства моделей. Приведем пример описания параметрического семейства моделей, соответствующего схеме из Рис.~\ref{fig:scheme_cnn} при условии, что структурные параметры $\boldsymbol{\gamma}$ имеют только одну ненулевую компоненту:\\
%\begin{center}
\texttt{(Concat\\
\text{\quad}OR(\\
\text{\quad\quad}(Conv2D [$c_0$] [$c_1$] [1],\\
\text{\quad\quad}(Concat(\\
\text{\quad\quad\quad}(Conv2D [$c_0$] [$c_2$] [1],\\
\text{\quad\quad\quad}(Conv2D [1] [$c_1$] [1])),\\
\text{\quad}(Affine [10]),\\
\text{\quad}(SoftMax)).} \\
%\end{center}
TODO: в статье нет сотфмакса


\textbf{Прогнозирование графовых структур. }
В работе~\cite{jaakkola2010learning} предлагается метод прогнозирования графовой структуры на основе линейного программирования. Предлагается свести проблему поиска графовой структуры к комбинаторной проблеме.
В работе~\cite{double_rnn} предлагается метод прогнозирования структур деревьев, основанный на дважды-рекуррентных нейросетях (англ. doubly-reccurent), т.е. на сетях, отдельно прогнозирующих глубину и ширину уровней деревьев.

\par{\textbf{Стохастическое порождение структур. }}
Одним из возможных направлений для порождения структр моделей глубокого обучения выступает стохастическое порождение структур.
Данный тип порождения предполагает, что структуры порождаются случайно в соответствие  вариационным распределением, заданным на структурах $q(\boldsymbol{\Gamma})$. Затем выбирается одна, либо несколько наилучших структур с учетом валидационной функции $Q$ или внешних, возможно недиффиренециремых, критериев качества. Итоговая модель получается путем оптимизации параметров модели при выбранной структуре $\boldsymbol{\Gamma}$. 
Заметим, что в ряде работ, одновременно порождается не только структура модели, но и итоговые параметры.

В работе~\cite{cib} рассматривается порождение моделей, оптимизируемых без учителя. Модель представляется многослойным перцептроном вида:
\[
    \mathbf{f} = \mathbf{f}_\text{|V|} \circ \dots \circ \mathbf{f}_1 (\mathbf{x}), \quad \mathbf{f}_i(\mathbf{x}) = \boldsymbol{\sigma}(\mathbf{w}^i \odot \mathbf{H}^i \mathbf{x}),
\]
где $\mathbf{H}^i$ --- бинарные матрицы, определяющие вклад каждого параметра из $\mathbf{w}^i$ в итоговую модель, знаком $\odot$  обозначается покомпонентное перемножение. 

Порождение моделей производится с использованием композиции процессов индийский буфетов. Процесс индийского буфет заключается в итеративном построении матрицы $\mathbf{H}^i$ с ограниченным, но не заданным наперед количеством столбцов. Интепретируя количество столбцов  матрицы как размер $i$-го слоя предлагается метод, позволяющий выбирать стохастически порождать модели с разилчной размерностью скрытых слоев. 

В работе~\cite{cib_simple} предлагается метод выбора модели сверточной нейронной сети. Используется функция потерь, основанная на аппроксимации априорного распределения процесса индийского буфета для каждой базовой функции $\mathbf{g}_i$, являющейся $i$-м отображением объектов:
\[
    L = \sum_{\mathbf{x}}||\mathbf{x} - \sum_{j=1}^K ||\mathbf{x} - \sum_{j=1}^K \mathbf{w}^j * \mathbf{g}_i(\mathbf{x})||_2^2 + \lambda^2K,
\]
где $K$ --- параметр, отвечающий за количество сверток, $\lambda$ --- параметр алгоритма, знаком $*$ обозначается операция свертки (TODO: проверить).

В работе~\cite{shirakawa2018dynamic} предлагается ввести априорное распределение Бернулли на структурные параметры $\boldsymbol{\gamma}^i$.

В~\cite{optimal_racing} рассматривается задача выбора архитектуры с помощью большого количества параллельных запусков обучения моделей. Предлагаются критерии ранней остановки процедуры оптимизации обучения моделей.

\par{\textbf{Последовательный выбор структуры модели. }}
В работе~\cite{greed} приводятся теоретические оценки построения нейросетей с использованием жадных стратегий,  при которых построение модели производится итеративно последовательным увеличением числа нейронов в сети. В работе~\cite{greed_mlp} предлагается жадная стратегия выбора модели нейросети с использованием релевантных априорных распределений, т.е. параметрических распределений, оптимизация параметров которых позволяет удалить часть параметров из модели. Данный метод был к задаче построения модели метода релевантных векторов~\cite{rvm}. 

В работах\cite{Bengio, hd} рассматривается послойное построение модели с отдельным критерием оптимизации для каждого слоя. В работах~\cite{Kingma, gendis_pictures, gendis_phd} предлагается декомпозиция модели на порождающую и разделяющую, оптимизируемых последовательно. 

В работах~\cite{boost_res,adanet} предлагается наращивание моделей, основанное на бустинге. Рассматривается задача построения нейросетевых моделей специального типа:
\[
    \mathbf{f}(\mathbf{x}) = \mathbf{f}_{|V|} \circ \mathbf{f}_{|V|-1} \circ \dots \mathbf{f}_0(\mathbf{x}), \quad  \mathbf{f}_{i+1}(\mathbf{x}) = \boldsymbol{\sigma}\left(\mathbf{f}_i(\mathbf{x})\right) + \mathbf{f}_i(\mathbf{x}),
\]
приводится параметризация модели, позволяющая рассматривать декомпозировать модель на слабые классификаторы.
В~\cite{adanet} рассматривается задача выбора полносвязной нейронной сети для задачи бинарной классификации, $Z=2$. На каждом шаге построения выбирается одно из двух расширений модели, каждое из которых рассматирвается как слабый классификатор: сделать модель шире или сделать модель глубже. Пример работы AdaNet представлен на Рис.~\ref{fig:scheme_adanet}.
Построение модели заканчивается при условии снижении радемахеровской сложности:
\begin{equation}
\label{eq:rad}
    \mathfrak{R} = \frac{1}{m}\mathsf{E}_{{b}_1,\dots, {b}_{|V|}} \text{sup}_{\mathbf{w}} \sum_{i=1}^m {b}_i \argmax_{c = \{0,1\}}f^{c}(\mathbf{x}_i, \mathbf{w}),
\end{equation}
где ${b}_i$ --- реализация случайной дискретной величины, равновероятно принимающей значений $-1$ и $1$, $f^{c}$ --- $c$-я компонента модели $\mathbf{f}$.

\begin{figure}
\begin{tikzpicture}[node distance=cm, auto]
  %\tikzstyle{every state}=[fill=red,draw=none,text=white]

  \node (f0)  at (1,3)                  {$\mathbf{f}_0 = \mathbf{x}$};
 \node (f1)  at (5,4)                  {$\mathbf{f}_1$};
 \node (f2)  at (5,2)                  {$\mathbf{f}_2$};
 \node (f3)  at (10,3)                  {$\mathbf{f}_3$};
 \node (f4)  at (15,3)                  {$\mathbf{f}_4$};

\path[->]  (f0) edge [bend left=50] node   {$\mathbf{g}^{0,1}_0 =  \boldsymbol{\sigma}(\mathbf{w}^{0,1}_{0}\mathbf{x})$} (f1);


\path[->]  (f0) edge [bend right=50] node[below]   {$\mathbf{g}^{0,2}_0 =  \boldsymbol{\sigma}(\mathbf{w}^{0,2}_{0}\mathbf{x})$} (f2);
\path[->]  (f1) edge [bend left=50] node   {$\mathbf{g}^{1,2}_0 =  \boldsymbol{\sigma}(\mathbf{w}^{1,3}_{0}\mathbf{x})$} (f3);
\path[->]  (f2) edge [bend right=50] node[below]   {$\mathbf{g}^{2,3}_0 =  \boldsymbol{\sigma}(\mathbf{w}^{2,3}_{0}\mathbf{x})$} (f3);
\path[->]  (f2) edge [bend left=25] node[below]   {$\mathbf{g}^{2,3}_1 =  \mathbf{0}$} (f3);

\path[->]  (f3) edge node   {$\mathbf{g}^{0,2}_0 =  \textbf{softmax}$} (f4);


\end{tikzpicture}

\caption{Пример итерации алгоритма AdaNet~\cite{adanet}. Рассматривается две альтернативные модели: модель с углублением сети (соответствует занулению функции $\mathbf{f}_2$ с использованием базовой функции $\mathbf{g}^{2,3}_1$) и модель с расширением сети (соответствует базовой функции $\mathbf{g}^{2,3}_0$).} В качестве функции агрегации для подмодели $\mathbf{f}_3$ выступает конкатенация: $\textbf{agg}_3 = \textbf{concat}$.
\label{fig:scheme_adanet}
\end{figure}

В работе~\cite{search_smbo} рассматривается задача порождения сверточных нейронных сетей. Предлагается проводить последовательный выбор структуры модели по восходящему числу параметров: начиная от сетей с одной подмоделью  и итеративно увеличивая количество подмоделей. В силу высокой вычислительной сложности данного подхода, вместо последовательного порождения моделей, предлагается провести оптимизацию рекуррентной нейронной сети, которая предсказывает качество модели по заданным подмоделям, и на основе данного предсказания выбрать наилучшую модель.
 
 
 
В работе~\cite{layer_probe} предлагается метод анализа структуры сети на основе линейных классификаторов, построенных на промежуточных слоях нейросети.
Схожий метод был предложен в~\cite{branches}, где классификаторы на промежуточных уровнях используются для уменьшения вычислений при выполнении вывода и предсказаний.
Промежуточные классификаторы работают как решающий список.

В работе~\cite{nn_inc} предлагается инкрементальный метод оптимизации нейросети. На первом этапе модель декомпозируется на несколько подмоделей, при которой модель последовательностью слоев $\mathbf{f}_1,\dots,\mathbf{f}_{|V|}$. Проводится последовательная оптимизация моделей вида:
\begin{enumerate}[1)]
\item $\mathbf{f} = \mathbf{f}_{|V|} (\mathbf{x});$
\item $\mathbf{f} = \mathbf{f}_{|V-1|} \circ \mathbf{f}_{|V|} (\mathbf{x});$
\item ...
\item $\mathbf{f} = \mathbf{f}_{1} \circ \dots \circ \mathbf{f}_{|V|} (\mathbf{x}).$
\end{enumerate}




\par{\textbf{Оптимизация структуры модели на основе обучения с подкреплением. }}
В~\cite{reinf} предлагается итеративная схема выбора архитектуры сверточной нейросети с использованием обучения с подкреплением. Распределение структур и параметров $q(\mathbf{w}, \boldsymbol{\Gamma})$ задается рекуррентной нейронной сетью, которая определяет значение параметров модели и наличие ребер с ненулевыми операциями между вершинами графов модели. Параметры рекуррентной нейронной сети оптимизируются на основе значения функции $Q$, получаемого на каждой итерации алгоритма.


\begin{figure}
\begin{tikzpicture}[node distance=cm, auto]
  %\tikzstyle{every state}=[fill=red,draw=none,text=white]

  \node (f0)  at (1,6)                  {$\mathbf{f}_0(\mathbf{x}) = \mathbf{x}$};
 \node (f1)  at (6,6)                  {$\mathbf{f}_1(\mathbf{x})$};
 \node (fn1)  at (11,6)                  {$\mathbf{f}_{|V|-1}(\mathbf{x})$};
 \node (fn)  at (16,6)                  {$\mathbf{f}_{|V|}(\mathbf{x})$};
 \node (fdots)  at (8,6)                  {};
  \path[->]  (f0) edge [bend right=50] node[below] {$\mathbf{g}_0^{0, 1}(\mathbf{x}) = \textbf{Conv}(\mathbf{x}, c_0, c_1)$}(f1);
  \path[->]  (f1) edge [bend right=50] node[below] {\textbf{\dots}}(fn1);
  \path[->]  (f0) edge [bend left=50] node   {$\mathbf{g}_0^{0,|V|-1}(\mathbf{x})=\mathbf{x}$} (fn1);
  \path[->]  (f0) edge [bend left=50] node   {$\mathbf{g}_1^{0,1}(\mathbf{x})=\mathbf{x}$} (f1);
  \path[->]  (f0) edge [bend left=50]  (fdots);
  \path[->]  (f1) edge [bend left=50](fn1);
  \path[->]  (f1) edge [bend left=50] (fdots);
  \path[->]  (fdots) edge [bend left=50]  (fn1);

  \path[->]  (fn1) edge [bend left=50] node   {$\mathbf{g}^{|V|-1,|V|}_0(\mathbf{x})=\textbf{softmax}(\mathbf{x})$} (fn);

  %\path[->] (f0)  edge[bend right=50] node[below] {$\mathbf{g}^{1,2}(\mathbf{x}) = \boldsymbol{\sigma}(\mathbf{x}\mathbf{w}^{1,2})$}(f1);
  %\path[->] (f1)  edge node {$\mathbf{g}^{2,1}(\mathbf{x}) = \textbf{softmax}(\mathbf{x}\mathbf{w}^{2,1})$}(f2);       
  %\draw[->] (f1) to (f2);
 
\end{tikzpicture}


\caption{Пример параметрического семейства моделей глубокого обучения, описываемый в~\cite{reinf}. Каждая подмодель $\mathbf{f}_j$ является линейной комбинацией базовых функций: свертки и результата работы предыдущих подмоделей (англ. skip-connection). }
\label{fig:scheme_mlp}
\end{figure}



В работе~\cite{reinf_predict} предлагается алгоритм построения регрессионной модели для оценки финального качества модели и ранней остановки оптимизации моделей. Он позволяет существенно ускорить поиск моделей, представленный в~\cite{reinf}.
В~\cite{reinf_transfer} рассматривается задача переноса архитектуры нейросети, чья структуры была выбрана по выборке, меньшей мощности. Как и в~\cite{reinf} предлагается метод параметризации сверточной нейронной сети в виде графа. Предложенная параметризация позволяет задать более мощное параметрическое семейство моделей, чем в~\cite{reinf}. Модель представляется в виде последовательности суперпозиций подмоделей, называемых клетками (англ. normal cell и reduction cell). Каждая из этих клеток содержит следующее множество нелинейных операций $\mathbf{g}$, состоящее из тождественной операции $\mathbf{g}(\mathbf{x}) = \mathbf{x}$, а также множество сверток с фиксированным количеством каналов и размером фильтров и функций субдискретизации или пулинга.
Алгоритм выбора структуры модели рекуррентной сетью выглядит следующим образом на шаге $j$:
\begin{enumerate}[1)]
\item выбрать вершину $v'$ из вершин $v_{j-1}$, $v_{j-2}$ из данной клетки или вершину из предыдущих клеток;
\item выбрать вершину $v''$ из вершин $v_{j-1}$, $v_{j-2}$ из данной клетки или вершину из предыдущих клеток;
\item выбрать базовую функцию $\mathbf{g}'$ для применения к вершине $v'$;
\item выбрать базовую функцию $\mathbf{g}''$ для применения к вершине $v''$;
\item выбрать функцию агрегации результатов применения операций $\mathbf{g}',\mathbf{g}''$: сумму или конкатенацию.
\end{enumerate}


В отличие от предыдущих работ, в работе~\cite{reinf_deep2net} предлагается подход к инкрементальному обучению нейросети, основанном на модификации модели, полученной на предыдущем шаге. Рассматривается две операции над нейросетью: расширение и углубление сети.

В работах~\cite{net2net, morph, partition} рассматриваются методы деформации нейросетей. 
В работе~\cite{partition} предлагается метод оптимального разделения нейросети на несколько независимых сетей для уменьшения количества связей и, как следствие, уменьшения сложности оптимизации модели. В работе~\cite{net2net} предлагается метод сохранения результатов оптимизации нейросети при построении новой более глубокой или широкой нейросети. 
В работе~\cite{morph} рассматривается задача расширения сверточной нейросети, нейросеть рассматривается как граф.

В работе~\cite{darts} используется представление модели из~\cite{reinf_transfer}. Вместо обучения с подкреплением испольщуются градиентная оптимизация структуры и параметров, выполненная в единой процедуре.

\section{Метаоптимизация моделей глубокого обучения}
Задача выбора структуры модели тесно связана с раздел машинного обучения под названием \textit{метаобучение} или \textit{метаоптимизация}. Под метаобучением понимаются алгоритмы машинного обучения~\cite{metalearn}, которые:
\begin{enumerate}[1)]
\item оценивают и сравнивают методы оптимизации моделей;
\item оценивают возможные декомпозиции процесса оптимизации моделей;
\item на основе полученных оценок предлагают оптимальные стратегии оптимизации моделей и отвергают неоптимальные. 
\end{enumerate}

В работе~\cite{self_rnn} предлагается подход к адаптивному изменению параметров сети. В качестве оператора оптимизации параметров рассматривается величина:
\[
    T( \boldsymbol{\theta}| L, \mathbf{y}, \mathbf{X}, \mathbf{h}, \boldsymbol{\beta}) = \boldsymbol{\theta} + \mathbf{f}_\text{optim}(\mathbf{f}_\text{mod}( \boldsymbol{\theta})),
\]
где $\mathbf{f}_\text{mod}$ --- функция, определяющая номер параметра из $\boldsymbol{\theta}$, подлежащего оптимизации, а $ \mathbf{f}_\text{optim}$ --- величина изменения параметра. 
В~\cite{self_rnn} также предлагается подмодель $\mathbf{f}_\text{ana}$, определяющая номер параметра, подлежащего дальнейшему анализу. Подход, описанный в данной работе, предполагает оптимизацию и анализ не только самой модели $\mathbf{f}$, но и дополнительных моделей $\mathbf{f}_\text{mod}, \mathbf{f}_\text{ana}, \mathbf{f}_\text{optim}$.

В работе~\cite{meta_sgd} рассматривается оптимизация метапараметров (шага градиентного спуска $\beta_{\text{lr}}$ и начального распределения параметров $\boldsymbol{\theta}^0$). Рассматривается задача оптимизации параметров модели в случае, когда количество примеров невелико. Для этого проводится оптимизация оператора оптимизации, который выглядит следующим образом:
\[
    T(\boldsymbol{\theta}| L, \mathbf{y}, \mathbf{X},  \mathbf{h}, \boldsymbol{\beta})  = \boldsymbol{\theta}^0 - \boldsymbol{\beta}\nabla L(\boldsymbol{\theta}^0| \mathbf{h}, \mathbf{X}, \mathbf{y},),
\]
где векторы $\boldsymbol{\theta}^0$ и $\boldsymbol{\beta}$ являются параметрами оператора $T$. Задача оптимизации параметров оператора $T$ рассматривается как задача многзадачного обучения (англ. multitask learning), когда оператор оптимизируется с учетом нескольких различных выборок и различных функций $L$, определенных отдельно для каждой выборки.


В работе~\cite{l2l} рассматирвается задача восстановления параметров модели по параметрам  другой модели, чьи параметры были получены оптимизацией функции потерь на выборке меньшей мощности. Задачу можно рассматривать как задачу нахождения параметров некоторого оператора оптимизации $T:\boldsymbol{\theta}^0 \to \boldsymbol{\theta},$ где $\boldsymbol{\theta}^0$ --- параметры модели, оптимизированной на небольшой выборке.  Предлагается функция оптимизации:
\[
    T = \argmin ||\hat{\boldsymbol{\theta}} -  \boldsymbol{\theta}^0||_2^2 + \beta_{\lambda} L(\boldsymbol{\theta}|  \mathbf{h},  \hat{\mathbf{X}}, \hat{\mathbf{y}}),
\]
где $\boldsymbol{\theta}$ --- параметры модели, обученной по полной выборке $\mathfrak{D}$, $\hat{\mathfrak{D}}$ --- выборка меньшей мощности, $\beta_{\lambda}$ --- настраиваемый метапараметр.

В работе~\cite{l2l_by_gd_gd} рассматривается оптимизация метапараметров оператора оптимизации с помощью модели долгой краткосрочной памяти LSTM, которая выступает альтернативе аналитических алгоритмов, таких как Adam~\cite{adam} или AdaGrad~\cite{adagrad}. LSTM имеет небольшое число параметров, т.к. для каждого метапараметра используется свой экземпляр модели LSTM с одинаковыми параметрами для каждого экземпляра. Оптимизируемый функционал является суммой значений функции потерь $L$ на нескольких шагах оптимизации:
\[
   Q = \sum_{t=1}^\eta L(\boldsymbol{\theta}^t),
\]
где $\eta$ --- число шагов оптимизации, $\boldsymbol{\theta}^t$ --- оптимизируемые параметры модели на шаге оптимизации $t$.


\section{Выбор структур моделей специального вида}
В данном разделе представлены работы по поиску оптимальных моделей со структурами специального вида.

В работе~\cite{mixed} рассматривается оптимизация моделей нейросетей с бинарной функцией активацией. Задача оптимизации сводится к задаче mixed integer программирования, которая решается методами выпуклого анализа.
В работе~\cite{energynet} предлагается метод построения сети глубокого обучения, структура которой выбирается с использованием обучения без учителя. Критерий оптимальности модели использует оценки энергитических функций и ограниченной машины Больцмана.

В работах~\cite{pathnet, supernet} рассматривается выбор архитектуры сети с использованием \textit{суперсетей}: связанных между собой подмоделей, образующих граф, каждый  путь из нулевой вершины в последнюю которого определяет модель глубокого обучения. Пример графа, описывающего суперсеть представлен на Рис.~\ref{fig:supernet}. В работе~\cite{supernet} рассматриваются стохастические суперсети, позволяющие выбрать структуру нейросети за органиченное время оптимизации. 
Схожий подход был предложен в работе~\cite{pathnet}, где предлагается использовать эволюционные алгоритмы для запоминания оптимальных подмоделей и переноса этих моделей в другие задачи.


\begin{figure}
\centering
\begin{tikzpicture}[node distance=cm, auto]
  %\tikzstyle{every state}=[fill=red,draw=none,text=white]

  \node (f0)  at (1,5)                  {$\mathbf{f}_0$};
  \node (f1)  at (6,5)                  {$\mathbf{f}_1$};
  \node (f2)  at (11,5)                 {$\mathbf{f}_2$};

  \node (f3)  at (1,3)                  {$\mathbf{f}_3$};
  \node (f4)  at (6,3)                  {$\mathbf{f}_4$};
  \node (f5)  at (11,3)                 {$\mathbf{f}_5$};


  \node (f6)  at (1,1)                  {$\mathbf{f}_6$};
  \node (f7)  at (6,1)                  {$\mathbf{f}_7$};
  \node (f8)  at (11,1)                 {$\mathbf{f}_8$};

\path[->]  (f0) edge  (f1);
\path[->]  (f0) edge  (f3);
\path[->]  (f0) edge  (f4);
\path[->]  (f1) edge  (f2);
\path[->]  (f1) edge  (f4);
\path[->]  (f1) edge  (f5);
\path[->]  (f2) edge  (f5);
\path[->]  (f3) edge  (f1);
\path[->]  (f3) edge  (f4);
\path[->]  (f3) edge  (f6);
\path[->]  (f3) edge  (f7);
\path[->]  (f4) edge  (f2);
\path[->]  (f4) edge  (f5);
\path[->]  (f4) edge  (f8);
\path[->]  (f4) edge  (f7);
\path[->]  (f5) edge  (f8);
\path[->]  (f6) edge  (f7);
\path[->]  (f6) edge  (f4);
\path[->]  (f7) edge  (f5);
\path[->]  (f7) edge  (f8);
\end{tikzpicture}
\caption{Пример суперсети. Каждый путь из подмодели $\mathbf{f}_0$ в конечную модель $\mathbf{f}_8$ задает модель глубокого обучения.}
\label{fig:supernet}

\end{figure}

\textbf{Порождающие модели. }
Порождающими моделями называются модели, приближающие совместное распределение объектов и соответствующих им меток $p(\mathbf{X}, \mathbf{y})$. Частным случаем порождающих моделей являются модели, приближающие только распределение векторов объектов $\mathbf{X}$. Подобный случай будем считать частным случаем классификации при пустом множестве меток классов ($Z=0$).

В качестве порождающих моделей в сетях глубокого обучения выступают ограниченные машины Больцмана~\cite{hinton_rbm} и автокодировщики~\cite{founds}. В работе~\cite{contractive} рассматриваются алгоритмы регуляризации автокодировщиков, позволяющих формально рассматривать данные модели как порождающие модели с использованием байесового вывода. В работе~\cite{score} рассматриваются регуляризованные автокодировщики и свойства оценок их правдоподобия. В работе~\cite{vae} предлагается обобщение автокодировщика с использованием вариационного байесовского вывода~\cite{Bishop}. В работе~\cite{train_generative} рассматриваются модификации вариационного автокодировщика и ступенчатых сетей~\cite{ladder} для случая построения многослойных порождающих моделей. 



В ряде работ~\cite{vae_graph,vae_stick,vae_mix,var_boost,layerwise_optimal} рассматривается подход к построению порождающих моделей глубокого обучения, при котором каждая подмодель $\mathbf{f}_i$ приближает распределение некоторой случайной величины $\mathbf{z}_i$, которая влияет на итоговое распределение $p(\mathbf{X}, \mathbf{y}) = \int_{\mathbf{z}_1, \dots, \mathbf{z}_{|V|}} p(\mathbf{X}, \mathbf{y}|\mathbf{z}_1, \dots, \mathbf{z}_{|V|})p(\mathbf{z}_1, \dots, \mathbf{z}_{|V|})d\mathbf{z}_1\dots d\mathbf{z}_{|V|}.$ Подобный подход позволяет использовать вероятностную интерпретацию для каждой отдельной подмодели. 

В работе~\cite{vae_graph} рассматривается обобщение вариационного автокодировщика на случай более общих графических моделей. Предлагается проводить оптимизацию сложных графических моделей в единой процедуре. Для вывода предлагается использовать нейронные сети.
Другая модификация вариационного автокодировщика представлена в работе~\cite{vae_stick}, авторы рассматривают использование процесса сломанной трости в вариационном автокодировщике, тем самым получая модель со стохастической размерностью скрытой переменной. В~\cite{vae_mix} рассматривается смесь автокодировщиков, где смесь моделируется процессом Дирихле.

В работе~\cite{var_boost} предлагается подход к оптимизации неизвестного распределения с помощью вариационного вывода. Предлагается решать задачу оптимизации итеративно, добавляя в модель новые компоненты вариационного распределения, проводится аналогия с бустингом.

В работе~\cite{layerwise_optimal} рассматривается задача построения порождающих моделей с дискретными значениями скрытых переменных $\mathbf{z}_i$, предлагается критерий для послойного обучения порождающих моделей:
\[
    Q = \sum_{\mathbf{x}} \text{log} \sum_{i}p(\mathbf{x}|\mathbf{z}_i)q(\mathbf{z}) \to \max,
\]
где $q$ --- аппроксимирующее распределение для случайной величины $\mathbf{z}$. 

В работе~\cite{vae_ard} рассматривается метод ARD для снижения размерности скрытого пространства вариационных порождающих моделей. Скрытая переменная параметризуется как  произведение некоторой случайной величины $\mathbf{z}$  на вектор, отвечающий за релевантность каждой компоненты скрытой переменной. Схема порождения выборки $\mathbf{X}$ представлена на Рис.~\ref{fig:vae_ard}.

\begin{figure}[H]
\centering
\includegraphics[width=0.25\textwidth]{./plots/notebooks/ae_plate.pdf}
\caption{Схема порождения вектора объектов $\mathbf{X}$, представленная в~\cite{vae_ard}.}
\label{fig:vae_ard}

\end{figure}

В данной работе предлагается метод последовательного порождения моделей глубокого обучения, основывающийся на применении вариационного вывода. Вариационный ввывод позволяет получить оценки правдоподобия модели с небольшими вычислительными затратами, а также проследить потенциальное начало переобучения модели без использования контрольной выборки. Для регуляризации структуры модели предлагается ввести априорное распределение на структуре, позволяющее проводить оптимизацию модели и ее структуры в различных режимах. В качестве метода оптимизации гиперпараметров выступают градиентные методы, что позволяет эффективно производить оптимизацию большого числа гиперпараметров, сопоставимого с числом параметров модели. 

